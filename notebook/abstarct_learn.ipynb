{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from abstractModule import AbstractReLU as AR\n",
    "from abstractModule import AbstractMaxpool2D as AM\n",
    "from abstractWeight import AbstractWeight as AW\n",
    "from abstractNN import AbstractNN as NN\n",
    "from custom_train import CustomTrainer as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path ='dataset'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean =[0.5], std =[0.2]),\n",
    "        #transforms.Resize((56,56))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = datasets.FashionMNIST(root = path,transform = transform, download = True, train = True)\n",
    "dataset_test =datasets.FashionMNIST( root =path,transform=transform ,download = True, train = False)\n",
    "val =0.2\n",
    "len_data_train = len(dataset_train)\n",
    "train_size =int((1-val)*len_data_train)\n",
    "\n",
    "val_size = int(val*len_data_train)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AbstractNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_depth=1,device=torch.device(\"cpu\")):\n",
    "\n",
    "        super(AbstractNN,self).__init__()\n",
    "       \n",
    "      \n",
    "        self.num_depth = num_depth\n",
    "        self.device = device\n",
    "        self.conv1=nn.Conv2d(self.num_depth,16,3,device=self.device)\n",
    "        self.conv2=nn.Conv2d(16,32,3,device=self.device)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "  \n",
    "\n",
    "       \n",
    "\n",
    "        self.fc1=nn.Sequential(nn.Flatten(),nn.Linear(4608,64,device=self.device))\n",
    "        self.fc2=nn.Sequential(nn.Flatten(),nn.Linear(64,10,device=self.device))\n",
    "        self.softMax =nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=torch.relu(x)\n",
    "     \n",
    "        x=self.maxpool(x)\n",
    "     \n",
    "    \n",
    "        x=self.fc1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=torch.relu(x)\n",
    "        x= self.softMax(x)\n",
    "        return x\n",
    "    \n",
    "    def abstract_forward(self,x,\n",
    "                         conv1_eps_weight,\n",
    "                         conv1_eps_bias,\n",
    "                         conv2_eps_weight,\n",
    "                         conv2_eps_bias,\n",
    "                         fc1_eps_weight,\n",
    "                         fc1_eps_bias, \n",
    "                         fc2_eps_weight,\n",
    "                         fc2_eps_bias,\n",
    "                         add_symbol=False,\n",
    "                         device=torch.device(\"cpu\")):\n",
    "        self.device=device\n",
    "        AR.max_symbol = 200_000\n",
    "        AM.max_symbol = np.inf\n",
    "        AR.recycling = 1\n",
    "        AM.recycling =1 \n",
    "        \n",
    "        x_true = x\n",
    "        x_true = x_true[0].unsqueeze(0)\n",
    "        print(\"test1\")\n",
    "        x,x_min,x_max,x_true = NN.abstract_conv2D(self.conv1,x,x_true,conv1_eps_weight , conv1_eps_bias,device=self.device)\n",
    "        print('test2')\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print('test3')\n",
    "        symb_1 = len(x)\n",
    "       \n",
    "        x,x_min,x_max,x_true = NN.abstract_conv2D(self.conv2,x,x_true,conv2_eps_weight,conv2_eps_bias,device=self.device)\n",
    "        print(\"test4\")\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print(\"test5\")\n",
    "        print(len(x))\n",
    "        x,x_min,x_max ,x_true = AM.abstract_maxpool2D(self.maxpool,x,x_true,add_symbol=add_symbol,device=self.device)\n",
    "        print(\"test6\")\n",
    "        print(len(x))\n",
    "        symb_conv2 =len(x)\n",
    "        x,x_min,x_max,x_true = NN.abstract_linear(self.fc1,x,x_true,fc1_eps_weight,fc1_eps_bias,device=self.device)\n",
    "        print('test7')\n",
    "        print(len(x))\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print('test8')\n",
    "        print(len(x))\n",
    "        symb_fc1 = len(x)\n",
    "        x,x_min,x_max,x_true = NN.abstract_linear(self.fc2,x,x_true,fc2_eps_weight,fc2_eps_bias,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        \n",
    "        \n",
    "        return x,x_min,x_max,x_true,symb_1,symb_conv2, symb_fc1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Label:5\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 1, 28, 28])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAaAAAAGdCAYAAABU0qcqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAAAdoElEQVR4nO3dfWxV9R3H8c8ttJcH24ul9GmUriCCitSItHZoxdEU2GJEXcSHJWAMDlbMEHwIRgE3kyomzmiYZFuUmQg4nEA0kYUHW+IsKCghZNrR2tmSPqAs7S0tlErP/iDcrVKE3/HeftvyfiUnofeeT8+X46Efb+/prwHP8zwBANDL4qwHAABcmiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmBhsPcB3dXV1qb6+XomJiQoEAtbjAAAceZ6n1tZWZWZmKi7u/K9z+lwB1dfXKysry3oMAMAPVFdXp9GjR5/3+T5XQImJiZLODJ6UlGQ8DQDAVTgcVlZWVuTr+fnErIDWrFmjF154QY2NjcrNzdUrr7yivLy8C+bOftstKSmJAgKAfuxCb6PE5CaEt956S0uXLtXKlSv16aefKjc3VzNnztTRo0djcTgAQD8UkwJ68cUXtWDBAj3wwAO6+uqrtXbtWg0bNkyvvfZaLA4HAOiHol5Ap06d0v79+1VUVPS/g8TFqaioSBUVFefs39HRoXA43G0DAAx8US+gb775RqdPn1ZaWlq3x9PS0tTY2HjO/qWlpQqFQpGNO+AA4NJg/oOoy5cvV0tLS2Srq6uzHgkA0AuifhdcSkqKBg0apKampm6PNzU1KT09/Zz9g8GggsFgtMcAAPRxUX8FlJCQoClTpmjnzp2Rx7q6urRz504VFBRE+3AAgH4qJj8HtHTpUs2bN0833HCD8vLy9NJLL6mtrU0PPPBALA4HAOiHYlJAc+fO1ddff60VK1aosbFR1113nbZt23bOjQkAgEtXwPM8z3qI/xcOhxUKhdTS0sJKCADQD13s13Hzu+AAAJcmCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmIh6Aa1atUqBQKDbNnHixGgfBgDQzw2OxSe95pprtGPHjv8dZHBMDgMA6Mdi0gyDBw9Wenp6LD41AGCAiMl7QIcPH1ZmZqbGjh2r+++/X7W1tefdt6OjQ+FwuNsGABj4ol5A+fn5WrdunbZt26ZXX31VNTU1uvnmm9Xa2trj/qWlpQqFQpEtKysr2iMBAPqggOd5XiwP0NzcrOzsbL344ot68MEHz3m+o6NDHR0dkY/D4bCysrLU0tKipKSkWI4GAIiBcDisUCh0wa/jMb87YMSIEbryyitVVVXV4/PBYFDBYDDWYwAA+piY/xzQ8ePHVV1drYyMjFgfCgDQj0S9gB599FGVl5fr3//+tz766CPdcccdGjRokO69995oHwoA0I9F/VtwR44c0b333qtjx45p1KhRuummm7Rnzx6NGjUq2ocCAPRjUS+gjRs3RvtTAgAGINaCAwCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYGWw8A4OL87W9/c86MHz/e17EmT57sK9cbPM9zzgQCgRhM0jM/83V1dTln4uL8vX7ozXNxIbwCAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYILFSOGbn0UX/WT8LrrYW7799lvnzI4dO5wz+/fvd85UV1c7ZyRp+/btzpm5c+c6Z0aPHu2c6UuLafbEz3yDBg2KwSR9X9/+lw0AGLAoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYYDFS+OZn0cXeWkjSz6Knkr/5Nm7c6Jx5++23nTMLFy50zgwbNsw5I0lbtmxxzjz33HPOmcLCQufM3Xff7ZzpTe3t7c6ZTz/91Dlz1VVXOWckaeTIkb5yscArIACACQoIAGDCuYB2796t2267TZmZmQoEAue8VPc8TytWrFBGRoaGDh2qoqIiHT58OFrzAgAGCOcCamtrU25urtasWdPj86tXr9bLL7+stWvXau/evRo+fLhmzpypkydP/uBhAQADh/NNCLNnz9bs2bN7fM7zPL300kt66qmndPvtt0uS3njjDaWlpWnLli265557fti0AIABI6rvAdXU1KixsVFFRUWRx0KhkPLz81VRUdFjpqOjQ+FwuNsGABj4olpAjY2NkqS0tLRuj6elpUWe+67S0lKFQqHIlpWVFc2RAAB9lPldcMuXL1dLS0tkq6ursx4JANALolpA6enpkqSmpqZujzc1NUWe+65gMKikpKRuGwBg4ItqAeXk5Cg9PV07d+6MPBYOh7V3714VFBRE81AAgH7O+S6448ePq6qqKvJxTU2NDhw4oOTkZI0ZM0ZLlizRs88+q/HjxysnJ0dPP/20MjMzNWfOnGjODQDo55wLaN++fbr11lsjHy9dulSSNG/ePK1bt06PP/642tra9NBDD6m5uVk33XSTtm3bpiFDhkRvagBAvxfw/K7aGCPhcFihUEgtLS0D6v2grq4u50xcXO/cI3LixAlfuW+//dY5U19f75yZMGGCc6Y3tba2OmceeOAB58zw4cOdM34XnvRzvfpZjHTFihXOGT8rqyxbtsw5I0lffvmlcyYxMdE5c8UVVzhnWlpanDOSNHnyZOeM69fii/06bn4XHADg0kQBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMMFq2APMv/71L+fMSy+95OtYgwYNcs58/PHHzpnf//73zpmf/OQnzhlJOn36tHOmra3NORMMBp0z5eXlzpknn3zSOSNJu3fvds74OXeBQMA588ILLzhn6urqnDOSNHiw82+s0Y033uicycvLc840NDQ4ZyRp2rRpzplhw4Y57c9q2ACAPo0CAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJ95X2eklXV5e6urouev+4uN7rUj/rt/pZdLG2ttY5s2rVKudMfHy8c0aSrrnmGudMfn6+c+ZPf/qTcyYjI8M5I0lpaWnOmfb2dudMc3Ozc2bIkCHOmeLiYueMJD311FPOmcWLFztn/Px3mjJlinPmk08+cc5I/v6th8Nh54yfRU/Hjx/vnJGkoUOH+srFAq+AAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmOizi5HGxcU5LTDqZ9FAl8VO/9+gQYN85Vzt2LHDOXPdddc5Zx577DHnjCSdPn3aOePnnI8YMcI5s2/fPueMJP30pz91zowaNco54+caGjNmjHMmOzvbOSNJv/rVr5wzf/zjH50z119/vXPGzzW0fv1654wk3XLLLc6Zyy+/3Dlzww03OGeuvvpq50xfwysgAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJvrsYqS1tbVKTEy86P0zMzOdj9HZ2emckaT4+HjnTGNjo3Pm66+/ds64nLOzqqqqnDOSlJ6e7pypr693znz00UfOmdbWVueM5G9R27vvvtvXsXqD38VIly1b5pxZtWqVc+bIkSPOmVdffdU542fhXEl69tlnnTOvvfaac2YgLCzqB6+AAAAmKCAAgAnnAtq9e7duu+02ZWZmKhAIaMuWLd2enz9/vgKBQLdt1qxZ0ZoXADBAOBdQW1ubcnNztWbNmvPuM2vWLDU0NES2DRs2/KAhAQADj/NNCLNnz9bs2bO/d59gMOjrDWoAwKUjJu8BlZWVKTU1VRMmTNCiRYt07Nix8+7b0dGhcDjcbQMADHxRL6BZs2bpjTfe0M6dO/X888+rvLxcs2fPPu9tkKWlpQqFQpEtKysr2iMBAPqgqP8c0D333BP587XXXqvJkydr3LhxKisr04wZM87Zf/ny5Vq6dGnk43A4TAkBwCUg5rdhjx07VikpKef9YcdgMKikpKRuGwBg4It5AR05ckTHjh1TRkZGrA8FAOhHnL8Fd/z48W6vZmpqanTgwAElJycrOTlZzzzzjO666y6lp6erurpajz/+uK644grNnDkzqoMDAPo35wLat2+fbr311sjHZ9+/mTdvnl599VUdPHhQf/nLX9Tc3KzMzEwVFxfrd7/7nYLBYPSmBgD0e84FNH369O9dsPHvf//7DxrorHfeeUdDhgy56P337NnjfIy3337bOSP5W7Cyra3NOeNnEc4DBw44Z7799lvnjHRm1QtXn3zyiXPmz3/+s3OmsLDQOSNJa9eudc5c6OfiepKQkOCc8bPAalycv++y33jjjc6Zn//8586ZyspK58ymTZucM3l5ec4ZScrJyXHO+LkeysvLnTN+FkWW5Ot99kmTJvk61oWwFhwAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwETA87O0cwyFw2GFQiE1Nzc7rdrq5/cNXX/99c4ZSSooKHDOZGdnO2euu+4654wffi+Bzs5O58zgwe6/Bf7LL790zlx22WXOGcnfufCzsrWf1dH9HCcQCDhnJGno0KHOGT//bYcNG+acOXnypHPmq6++cs5IUmJionMmOTnZOePn39Lw4cOdM5K/VexdVwXv6OjQ888/r5aWlu/9Os4rIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACb67GKkF1rELhrq6up85V577bVeOdbp06edM//5z3+cM/n5+c4ZSRo5cqRzxs/ik6mpqc4Zv9rb250zx44dc874WYy0o6PDOeNn4U7J36Ksfq4HP/P5OU58fLxzRvK3KOuJEyecM37+Tq+88opzRpLef/9958y+ffuc9j9+/LimT5/OYqQAgL6JAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACYoIACAiT67GGlzc7PTYqSBQCCGU/UfFRUVzhm/l8CRI0ecM5988olzJjMz0zmTnJzsnJGkvXv3Omeuvvpq50xeXp5z5ujRo84Zv4tw+vn3lJ2d7Zz5/PPPnTOHDh1yzkydOtU5I0mHDx92zvhZePimm25yzvi5HiSpuLjYOZOVleW0/8UuKs0rIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJiggAAAJiggAIAJCggAYIICAgCYoIAAACb67GKkF1rEDgDQN7EYKQCgT6OAAAAmnAqotLRUU6dOVWJiolJTUzVnzhxVVlZ22+fkyZMqKSnRyJEjddlll+muu+5SU1NTVIcGAPR/TgVUXl6ukpIS7dmzR9u3b1dnZ6eKi4vV1tYW2eeRRx7Ru+++q02bNqm8vFz19fW68847oz44AKB/+0E3IXz99ddKTU1VeXm5CgsL1dLSolGjRmn9+vX6xS9+IUn64osvdNVVV6miokI33njjBT8nNyEAQP/WKzchtLS0SPrfrz/ev3+/Ojs7VVRUFNln4sSJGjNmzHl/VXRHR4fC4XC3DQAw8PkuoK6uLi1ZskTTpk3TpEmTJEmNjY1KSEjQiBEjuu2blpamxsbGHj9PaWmpQqFQZHP93eMAgP7JdwGVlJTo0KFD2rhx4w8aYPny5WppaYlsdXV1P+jzAQD6h8F+QosXL9Z7772n3bt3a/To0ZHH09PTderUKTU3N3d7FdTU1KT09PQeP1cwGFQwGPQzBgCgH3N6BeR5nhYvXqzNmzdr165dysnJ6fb8lClTFB8fr507d0Yeq6ysVG1trQoKCqIzMQBgQHB6BVRSUqL169dr69atSkxMjLyvEwqFNHToUIVCIT344INaunSpkpOTlZSUpIcfflgFBQUXdQccAODS4XQbdiAQ6PHx119/XfPnz5d05gdRly1bpg0bNqijo0MzZ87UH/7wh/N+C+67uA0bAPq3i/06zmKkAICoYjFSAECfRgEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABNOBVRaWqqpU6cqMTFRqampmjNnjiorK7vtM336dAUCgW7bwoULozo0AKD/cyqg8vJylZSUaM+ePdq+fbs6OztVXFystra2bvstWLBADQ0NkW316tVRHRoA0P8Ndtl527Zt3T5et26dUlNTtX//fhUWFkYeHzZsmNLT06MzIQBgQPpB7wG1tLRIkpKTk7s9/uabbyolJUWTJk3S8uXL1d7eft7P0dHRoXA43G0DAAx8Tq+A/l9XV5eWLFmiadOmadKkSZHH77vvPmVnZyszM1MHDx7UE088ocrKSr3zzjs9fp7S0lI988wzfscAAPRTAc/zPD/BRYsW6f3339eHH36o0aNHn3e/Xbt2acaMGaqqqtK4cePOeb6jo0MdHR2Rj8PhsLKystTS0qKkpCQ/owEADIXDYYVCoQt+Hff1Cmjx4sV67733tHv37u8tH0nKz8+XpPMWUDAYVDAY9DMGAKAfcyogz/P08MMPa/PmzSorK1NOTs4FMwcOHJAkZWRk+BoQADAwORVQSUmJ1q9fr61btyoxMVGNjY2SpFAopKFDh6q6ulrr16/Xz372M40cOVIHDx7UI488osLCQk2ePDkmfwEAQP/k9B5QIBDo8fHXX39d8+fPV11dnX75y1/q0KFDamtrU1ZWlu644w499dRTF/1+zsV+7xAA0DfF5D2gC3VVVlaWysvLXT4lAOASxVpwAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATFBAAwAQFBAAwQQEBAExQQAAAExQQAMAEBQQAMEEBAQBMUEAAABMUEADABAUEADBBAQEATFBAAAATg60H+C7P8yRJ4XDYeBIAgB9nv36f/Xp+Pn2ugFpbWyVJWVlZxpMAAH6I1tZWhUKh8z4f8C5UUb2sq6tL9fX1SkxMVCAQ6PZcOBxWVlaW6urqlJSUZDShPc7DGZyHMzgPZ3AezugL58HzPLW2tiozM1Nxced/p6fPvQKKi4vT6NGjv3efpKSkS/oCO4vzcAbn4QzOwxmchzOsz8P3vfI5i5sQAAAmKCAAgIl+VUDBYFArV65UMBi0HsUU5+EMzsMZnIczOA9n9Kfz0OduQgAAXBr61SsgAMDAQQEBAExQQAAAExQQAMBEvymgNWvW6Mc//rGGDBmi/Px8ffzxx9Yj9bpVq1YpEAh02yZOnGg9Vszt3r1bt912mzIzMxUIBLRly5Zuz3uepxUrVigjI0NDhw5VUVGRDh8+bDNsDF3oPMyfP/+c62PWrFk2w8ZIaWmppk6dqsTERKWmpmrOnDmqrKzsts/JkydVUlKikSNH6rLLLtNdd92lpqYmo4lj42LOw/Tp08+5HhYuXGg0cc/6RQG99dZbWrp0qVauXKlPP/1Uubm5mjlzpo4ePWo9Wq+75ppr1NDQENk+/PBD65Firq2tTbm5uVqzZk2Pz69evVovv/yy1q5dq71792r48OGaOXOmTp482cuTxtaFzoMkzZo1q9v1sWHDhl6cMPbKy8tVUlKiPXv2aPv27ers7FRxcbHa2toi+zzyyCN69913tWnTJpWXl6u+vl533nmn4dTRdzHnQZIWLFjQ7XpYvXq10cTn4fUDeXl5XklJSeTj06dPe5mZmV5paanhVL1v5cqVXm5urvUYpiR5mzdvjnzc1dXlpaeney+88ELksebmZi8YDHobNmwwmLB3fPc8eJ7nzZs3z7v99ttN5rFy9OhRT5JXXl7ued6Z//bx8fHepk2bIvt8/vnnniSvoqLCasyY++558DzPu+WWW7zf/OY3dkNdhD7/CujUqVPav3+/ioqKIo/FxcWpqKhIFRUVhpPZOHz4sDIzMzV27Fjdf//9qq2ttR7JVE1NjRobG7tdH6FQSPn5+Zfk9VFWVqbU1FRNmDBBixYt0rFjx6xHiqmWlhZJUnJysiRp//796uzs7HY9TJw4UWPGjBnQ18N3z8NZb775plJSUjRp0iQtX75c7e3tFuOdV59bjPS7vvnmG50+fVppaWndHk9LS9MXX3xhNJWN/Px8rVu3ThMmTFBDQ4OeeeYZ3XzzzTp06JASExOtxzPR2NgoST1eH2efu1TMmjVLd955p3JyclRdXa0nn3xSs2fPVkVFhQYNGmQ9XtR1dXVpyZIlmjZtmiZNmiTpzPWQkJCgESNGdNt3IF8PPZ0HSbrvvvuUnZ2tzMxMHTx4UE888YQqKyv1zjvvGE7bXZ8vIPzP7NmzI3+ePHmy8vPzlZ2drb/+9a968MEHDSdDX3DPPfdE/nzttddq8uTJGjdunMrKyjRjxgzDyWKjpKREhw4duiTeB/0+5zsPDz30UOTP1157rTIyMjRjxgxVV1dr3LhxvT1mj/r8t+BSUlI0aNCgc+5iaWpqUnp6utFUfcOIESN05ZVXqqqqynoUM2evAa6Pc40dO1YpKSkD8vpYvHix3nvvPX3wwQfdfn1Lenq6Tp06pebm5m77D9Tr4XznoSf5+fmS1Keuhz5fQAkJCZoyZYp27twZeayrq0s7d+5UQUGB4WT2jh8/rurqamVkZFiPYiYnJ0fp6endro9wOKy9e/de8tfHkSNHdOzYsQF1fXiep8WLF2vz5s3atWuXcnJyuj0/ZcoUxcfHd7seKisrVVtbO6Cuhwudh54cOHBAkvrW9WB9F8TF2LhxoxcMBr1169Z5//znP72HHnrIGzFihNfY2Gg9Wq9atmyZV1ZW5tXU1Hj/+Mc/vKKiIi8lJcU7evSo9Wgx1dra6n322WfeZ5995knyXnzxRe+zzz7zvvrqK8/zPO+5557zRowY4W3dutU7ePCgd/vtt3s5OTneiRMnjCePru87D62trd6jjz7qVVRUeDU1Nd6OHTu866+/3hs/frx38uRJ69GjZtGiRV4oFPLKysq8hoaGyNbe3h7ZZ+HChd6YMWO8Xbt2efv27fMKCgq8goICw6mj70Lnoaqqyvvtb3/r7du3z6upqfG2bt3qjR071issLDSevLt+UUCe53mvvPKKN2bMGC8hIcHLy8vz9uzZYz1Sr5s7d66XkZHhJSQkeD/60Y+8uXPnelVVVdZjxdwHH3zgSTpnmzdvnud5Z27Ffvrpp720tDQvGAx6M2bM8CorK22HjoHvOw/t7e1ecXGxN2rUKC8+Pt7Lzs72FixYMOD+J62nv78k7/XXX4/sc+LECe/Xv/61d/nll3vDhg3z7rjjDq+hocFu6Bi40Hmora31CgsLveTkZC8YDHpXXHGF99hjj3ktLS22g38Hv44BAGCiz78HBAAYmCggAIAJCggAYIICAgCYoIAAACYoIACACQoIAGCCAgIAmKCAAAAmKCAAgAkKCABgggICAJj4L/TS1dpa4LsZAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img, label = dataset_train[1310]\n",
    "plt.imshow(1-img[0],cmap='gray')\n",
    "print(f\"Label:{label}\")\n",
    "img.shape\n",
    "from abstract import abstractTensor as AT\n",
    "\n",
    "x=AT(img,alpha =torch.tensor([]))\n",
    "x=x.abstract_tensor()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.0001\n",
    "conv1_eps_weight = torch.tensor([])\n",
    "conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "conv1_eps_weight.values = scale*(1)*torch.ones_like(conv1_eps_weight.indices)\n",
    "conv1_eps_bias = torch.tensor([])\n",
    "conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "conv1_eps_bias.values = scale*(1)*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "conv2_eps_weight = torch.tensor([])\n",
    "conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "conv2_eps_weight.values = scale*(1)*torch.ones_like(conv2_eps_weight.indices)\n",
    "conv2_eps_bias = torch.tensor([])\n",
    "conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "conv2_eps_bias.values = scale*(1)*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "fc1_eps_weight = torch.tensor([])\n",
    "fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "fc1_eps_weight.values = scale*(1)*torch.ones_like(fc1_eps_weight.indices)\n",
    "fc1_eps_bias = torch.tensor([])\n",
    "fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "fc1_eps_bias.values = scale*(1)*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "fc2_eps_weight = torch.tensor([])\n",
    "fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "fc2_eps_weight.values = scale*(1)*torch.ones_like(fc2_eps_weight.indices)\n",
    "fc2_eps_bias = torch.tensor([])\n",
    "fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "fc2_eps_bias.values = scale*(1)*torch.ones_like(fc2_eps_bias.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "   result,x_min,x_max,x_true,_,_,_=model.abstract_forward(x,\n",
    "                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                    \n",
    "                                                    add_symbol=True)\n",
    "\n",
    "print(f\"y_min       =  {x_min}\")\n",
    "print(f\"y_max       =  {x_max}\")\n",
    "print(f\"center Ztp  =  {result[0]}\")\n",
    "print(f\"y_true      =  {x_true[:]}\")\n",
    "print(f\"y_max-x_min =  {x_max-x_min}\")\n",
    "print(f\"Trash symbol=  {result[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_dominance(result,x_min,x_max,x_true):\n",
    "       y_min       =  np.array(x_min)\n",
    "       y_max       =  np.array(x_max)\n",
    "       center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "       y_true      =  np.expand_dims(np.array(x_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       x_scale = np.arange(len(y_min))\n",
    "       D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "       print(D.shape)\n",
    "       print(center_Ztp.shape)\n",
    "       print(y_true.shape)\n",
    "       # plot:\n",
    "\n",
    "       fig,ax = plt.subplots(1, 1, figsize=(8,4), tight_layout=True)\n",
    "       ax.eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "       ax.eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "       ax.eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "       ax.set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "              ylim=(np.min(D), np.max(D)))\n",
    "       plt.ylabel(\"Value of the abstract domain\")\n",
    "       plt.title(\"Dominance interval for the 10 classes of Fashion MNIST .\\n Abstract domain based on 100_000 lower weights of the first layer of the first fully connected layer of the model\")\n",
    "       plt.legend()\n",
    "       plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from abstract import abstractTensor as AT\n",
    "\n",
    "for i in tqdm(range(50)):\n",
    "    scale = 0.0001\n",
    "    conv1_eps_weight = torch.tensor([])\n",
    "    conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "    conv1_eps_weight.values = scale*(1)*torch.ones_like(conv1_eps_weight.indices)\n",
    "    conv1_eps_bias = torch.tensor([])\n",
    "    conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "    conv1_eps_bias.values = scale*(1)*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "    conv2_eps_weight = torch.tensor([])\n",
    "    conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "    conv2_eps_weight.values = scale*(1)*torch.ones_like(conv2_eps_weight.indices)\n",
    "    conv2_eps_bias = torch.tensor([])\n",
    "    conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "    conv2_eps_bias.values = scale*(1)*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "    fc1_eps_weight = torch.tensor([])\n",
    "    fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "    fc1_eps_weight.values = scale*(1)*torch.ones_like(fc1_eps_weight.indices)\n",
    "    fc1_eps_bias = torch.tensor([])\n",
    "    fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "    fc1_eps_bias.values = scale*(1)*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "    fc2_eps_weight = torch.tensor([])\n",
    "    fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "    fc2_eps_weight.values = scale*(1)*torch.ones_like(fc2_eps_weight.indices)\n",
    "    fc2_eps_bias = torch.tensor([])\n",
    "    fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "    fc2_eps_bias.values = scale*(1)*torch.ones_like(fc2_eps_bias.indices)\n",
    "   \n",
    "\n",
    "    img, label = dataset_train[np.random.randint(0,1000)]\n",
    "\n",
    "   \n",
    "   \n",
    "    \n",
    "\n",
    "    x=AT(img,alpha =torch.tensor([]))\n",
    "    x=x.abstract_tensor()\n",
    "    x.shape\n",
    "\n",
    "    \n",
    "  \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        \n",
    "        result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1=model.abstract_forward(x,\n",
    "                                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "\n",
    "                                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                                    add_symbol=False)\n",
    "    plot_dominance(result,x_min,x_max,x_true)\n",
    "    print(f\"Label = {label}\")\n",
    "\n",
    "    concatenated_tensors = []\n",
    "    concatenated_bias_tensors =[]\n",
    "    for j in range(10):\n",
    "        concat_part = result[1:145, j].view(16, 1, 3, 3)\n",
    "        concatenated_tensors.append(concat_part)\n",
    "    concatenated_tensors =torch.stack(concatenated_tensors)\n",
    "    for j in range(10):\n",
    "        concat_part_bias = result[145:161, j]\n",
    "        concatenated_bias_tensors.append(concat_part_bias)\n",
    "    concatenated_bias_tensors = torch.stack(concatenated_bias_tensors)\n",
    "\n",
    "    concatenated_tensors_2 = []\n",
    "    concatenated_bias_tensors_2 =[]\n",
    "    for j in range(10):\n",
    "        concat_part = result[len_symb:len_symb+4608, j].view(32, 16, 3, 3)\n",
    "        concatenated_tensors_2.append(concat_part)\n",
    "    concatenated_tensors_2 =torch.stack(concatenated_tensors_2)\n",
    "    for j in range(10):\n",
    "        concat_part_bias = result[len_symb+4608:len_symb+4640, j]\n",
    "        concatenated_bias_tensors_2.append(concat_part_bias)\n",
    "    concatenated_bias_tensors_2 = torch.stack(concatenated_bias_tensors_2)\n",
    "\n",
    "    concatenated_tensors_fc1 = []\n",
    "    concatenated_bias_tensors_fc1 =[]\n",
    "    for j in range(10):\n",
    "        concat_part = result[len_symb_c2:len_symb_c2+294912, j].view(64,4608)\n",
    "        concatenated_tensors_fc1.append(concat_part)\n",
    "    concatenated_tensors_fc1 =torch.stack(concatenated_tensors_fc1)\n",
    "    for j in range(10):\n",
    "        concat_part_bias = result[len_symb_c2+294912:len_symb_c2+294976, j]\n",
    "        concatenated_bias_tensors_fc1.append(concat_part_bias)\n",
    "    concatenated_bias_tensors_fc1= torch.stack(concatenated_bias_tensors_fc1)\n",
    "\n",
    "    concatenated_tensors_fc2 = []\n",
    "    concatenated_bias_tensors_fc2 =[]\n",
    "    for j in range(10):\n",
    "        concat_part = result[len_symb_fc1:len_symb_fc1+640, j].view(10,64)\n",
    "        concatenated_tensors_fc2.append(concat_part)\n",
    "    concatenated_tensors_fc2 =torch.stack(concatenated_tensors_fc2)\n",
    "    for j in range(10):\n",
    "        concat_part_bias = result[len_symb_fc1+640:len_symb_fc1+650, j]\n",
    "        concatenated_bias_tensors_fc2.append(concat_part_bias)\n",
    "    concatenated_bias_tensors_fc2= torch.stack(concatenated_bias_tensors_fc2)\n",
    "    k_min = 1\n",
    "    k_plus = 10\n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    model.conv1.weight.data =model.conv1.weight.data+k_plus*(concatenated_tensors[label]-k_min*(torch.sum(concatenated_tensors[:label],dim =0)+torch.sum(concatenated_tensors[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.conv1.bias.data = model.conv1.bias.data + k_plus*(concatenated_bias_tensors[label]-k_min*(torch.sum(concatenated_bias_tensors[:label],dim = 0)+torch.sum(concatenated_bias_tensors[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.conv2.weight.data =model.conv2.weight.data+k_plus*(concatenated_tensors_2[label]-k_min*(torch.sum(concatenated_tensors_2[:label],dim =0)+torch.sum(concatenated_tensors_2[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.conv2.bias.data = model.conv2.bias.data + k_plus*(concatenated_bias_tensors_2[label]-k_min*(torch.sum(concatenated_bias_tensors_2[:label],dim = 0)+torch.sum(concatenated_bias_tensors_2[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.fc1[1].weight.data =model.fc1[1].weight.data+k_plus*(concatenated_tensors_fc1[label]-k_min*(torch.sum(concatenated_tensors_fc1[:label],dim =0)+torch.sum(concatenated_tensors_fc1[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(concatenated_bias_tensors_fc1[label]-k_min*(torch.sum(concatenated_bias_tensors_fc1[:label],dim = 0)+torch.sum(concatenated_bias_tensors_fc1[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.fc2[1].weight.data =model.fc2[1].weight.data+k_plus*(concatenated_tensors_fc2[label]-k_min*(torch.sum(concatenated_tensors_fc2[:label],dim =0)+torch.sum(concatenated_tensors_fc2[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(concatenated_bias_tensors_fc2[label]-k_min*(torch.sum(concatenated_bias_tensors_fc2[:label],dim = 0)+torch.sum(concatenated_bias_tensors_fc2[label+1:],dim = 0)))#/torch.no\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.fc2[1].weight.data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_eps_bias.values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv1_eps_weight.values\n",
    "model.conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = np.arange(0,10,1)\n",
    "test[1:5]\n",
    "test[5:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from custom_train import CustomTrainer as CT\n",
    "eval =CT(model, device = device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), f'abstractLearn.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('dataset/FMNIST.pth'))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
