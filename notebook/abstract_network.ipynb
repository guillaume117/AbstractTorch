{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Abstract Network\n",
    "This notebook aims to evaluate an entire abstracted network. \n",
    "At the end, some pruning trial will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "from abstractModule import AbstractReLU as AR\n",
    "from abstractModule import AbstractMaxpool2D as AM\n",
    "from abstractWeight import AbstractWeight as AW\n",
    "from abstractNN import AbstractNN as NN\n",
    "from custom_train import CustomTrainer as CT\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "import numpy as np\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path ='dataset'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean =[0.5], std =[0.2]),\n",
    "        #transforms.Resize((56,56))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = datasets.FashionMNIST(root = path,transform = transform, download = True, train = True)\n",
    "dataset_test =datasets.FashionMNIST( root =path,transform=transform ,download = True, train = False)\n",
    "val =0.2\n",
    "len_data_train = len(dataset_train)\n",
    "train_size =int((1-val)*len_data_train)\n",
    "\n",
    "val_size = int(val*len_data_train)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The network\n",
    "We are still working on our simple CNN network for benchmark.\n",
    "The forward abstract method is implemented with abstractNN method. \n",
    "abstractNN.abstract_linear takes in input a nn.Sequential(nn.Flatten, nn.Linear), an abstract tensor, and two tupple of index/values for abstraction of weight and bias. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AbstractNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_depth=1,device=torch.device(\"cpu\")):\n",
    "\n",
    "        super(AbstractNN,self).__init__()\n",
    "       \n",
    "      \n",
    "        self.num_depth = num_depth\n",
    "        self.device = device\n",
    "        self.conv1=nn.Conv2d(self.num_depth,16,3,device=self.device)\n",
    "        self.conv2=nn.Conv2d(16,32,3,device=self.device)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "  \n",
    "\n",
    "       \n",
    "\n",
    "        self.fc1=nn.Sequential(nn.Flatten(),nn.Linear(4608,64,device=self.device))\n",
    "        self.fc2=nn.Sequential(nn.Flatten(),nn.Linear(64,10,device=self.device))\n",
    "        self.softMax =nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=torch.relu(x)\n",
    "     \n",
    "        x=self.maxpool(x)\n",
    "     \n",
    "    \n",
    "        x=self.fc1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=torch.relu(x)\n",
    "        x= self.softMax(x)\n",
    "        return x\n",
    "    \n",
    "    def abstract_forward(self,x,\n",
    "                         conv1_eps_weight,\n",
    "                         conv1_eps_bias,\n",
    "                         conv2_eps_weight,\n",
    "                         conv2_eps_bias,\n",
    "                         fc1_eps_weight,\n",
    "                         fc1_eps_bias, \n",
    "                         fc2_eps_weight,\n",
    "                         fc2_eps_bias,\n",
    "                         add_symbol=False,\n",
    "                         device=torch.device(\"cpu\")):\n",
    "        self.device=device\n",
    "        AR.max_symbol = np.inf\n",
    "        AM.max_symbol = np.inf\n",
    "        AR.recycling = 1\n",
    "        AM.recycling =1 \n",
    "        \n",
    "        x_true = x\n",
    "        x_true = x_true[0].unsqueeze(0)\n",
    "      \n",
    "        x,x_min,x_max,x_true = NN.abstract_conv2D(self.conv1,x,x_true,conv1_eps_weight , conv1_eps_bias,device=self.device)\n",
    "     \n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "   \n",
    "        symb_1 = len(x)\n",
    "       \n",
    "        x,x_min,x_max,x_true = NN.abstract_conv2D(self.conv2,x,x_true,conv2_eps_weight,conv2_eps_bias,device=self.device)\n",
    "       \n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "       \n",
    "      \n",
    "        x,x_min,x_max ,x_true = AM.abstract_maxpool2D(self.maxpool,x,x_true,add_symbol=add_symbol,device=self.device)\n",
    "    \n",
    "      \n",
    "        symb_conv2 =len(x)\n",
    "        x,x_min,x_max,x_true = NN.abstract_linear(self.fc1,x,x_true,fc1_eps_weight,fc1_eps_bias,device=self.device)\n",
    "     \n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "  \n",
    "  \n",
    "        symb_fc1 = len(x)\n",
    "        x,x_min,x_max,x_true = NN.abstract_linear(self.fc2,x,x_true,fc2_eps_weight,fc2_eps_bias,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        \n",
    "        \n",
    "        return x,x_min,x_max,x_true,symb_1,symb_conv2, symb_fc1\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img, label = dataset_train[1310]\n",
    "plt.imshow(1-img[0],cmap='gray')\n",
    "print(f\"Label:{label}\")\n",
    "img.shape\n",
    "from abstract import abstractTensor as AT\n",
    "\n",
    "x=AT(img,alpha =torch.tensor([]))\n",
    "x=x.abstract_tensor()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here you can read an example how to generate tupples for weight and bias abstraction\n",
    "Some methods need to be implement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale = 0.0001\n",
    "conv1_eps_weight = torch.tensor([])\n",
    "conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "conv1_eps_weight.values = scale*(1)*torch.ones_like(conv1_eps_weight.indices)\n",
    "conv1_eps_bias = torch.tensor([])\n",
    "conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "conv1_eps_bias.values = scale*(1)*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "conv2_eps_weight = torch.tensor([])\n",
    "conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "conv2_eps_weight.values = scale*(1)*torch.ones_like(conv2_eps_weight.indices)\n",
    "conv2_eps_bias = torch.tensor([])\n",
    "conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "conv2_eps_bias.values = scale*(1)*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "fc1_eps_weight = torch.tensor([])\n",
    "fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "fc1_eps_weight.values = scale*(1)*torch.ones_like(fc1_eps_weight.indices)\n",
    "fc1_eps_bias = torch.tensor([])\n",
    "fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "fc1_eps_bias.values = scale*(1)*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "fc2_eps_weight = torch.tensor([])\n",
    "fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "fc2_eps_weight.values = scale*(1)*torch.ones_like(fc2_eps_weight.indices)\n",
    "fc2_eps_bias = torch.tensor([])\n",
    "fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "fc2_eps_bias.values = scale*(1)*torch.ones_like(fc2_eps_bias.indices)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "\n",
    "   result,x_min,x_max,x_true,_,_,_=model_assesment.abstract_forward(x,\n",
    "                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                    \n",
    "                                                    add_symbol=True)\n",
    "\n",
    "print(f\"y_min       =  {x_min}\")\n",
    "print(f\"y_max       =  {x_max}\")\n",
    "print(f\"center Ztp  =  {result[0]}\")\n",
    "print(f\"y_true      =  {x_true[:]}\")\n",
    "print(f\"y_max-x_min =  {x_max-x_min}\")\n",
    "print(f\"Trash symbol=  {result[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "def plot_dominance(result,x_min,x_max,x_true):\n",
    "       y_min       =  np.array(x_min)\n",
    "       y_max       =  np.array(x_max)\n",
    "       center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "       y_true      =  np.expand_dims(np.array(x_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "       x_scale = np.arange(len(y_min))\n",
    "       D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "   \n",
    "       # plot:\n",
    "\n",
    "       fig,ax = plt.subplots(1, 1, figsize=(8,4), tight_layout=True)\n",
    "       ax.eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "       ax.eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "       ax.eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "       ax.set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "              ylim=(np.min(D)-1, np.max(D)+1))\n",
    "       plt.ylabel(\"Value of the abstract domain\")\n",
    "       plt.title(\"Dominance interval for the 10 classes of Fashion MNIST .\\n Abstract domain based on 100_000 lower weights of the first layer of the first fully connected layer of the model\")\n",
    "       plt.legend()\n",
    "       plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#model.load_state_dict(torch.load('dataset/FMNIST.pth'))\n",
    "from custom_train import CustomTrainer as CT\n",
    "eval =CT(model, device = device)\n",
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()\n",
    "model.load_state_dict(torch.load('240523_abstractLearn_47.pth'))\n",
    "from custom_train import CustomTrainer as CT\n",
    "eval =CT(model, device = device)\n",
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "torch.save(best_model_wts, f'model_first_step.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()\n",
    "torch.save(model.state_dict(), f'modelseeds.pth')\n",
    "from custom_train import CustomTrainer as CT\n",
    "eval =CT(model, device = device)\n",
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('modelseeds.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model = AbstractNN()\n",
    "model.load_state_dict(torch.load('model_first_step3627.pth'))\n",
    "#model.load_state_dict(torch.load('modelseeds.pth'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "num_epochs =10\n",
    "learning_rate=0.01\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.99))\n",
    "scheduler =  optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.98)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "TD= T(model=model,\n",
    "            device=device\n",
    "                            )\n",
    "model = TD.train_model(train_dataset,val_dataset,\n",
    "                        criterion=criterion,  \n",
    "                        batch_size= batch_size,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        num_epochs=num_epochs,\n",
    "                        learning_rate=learning_rate,\n",
    "                        resname='trainfromabstract',verbose=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "5"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.random.randint(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "    param.data = torch.zeros_like(param.data)\n",
    "\n",
    "torch.sum(model.conv1.weight.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 9.53 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 9/1200 [04:51<11:04:17, 33.47s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 10.10 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 11.96 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 16.32 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 22.24 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 27.41 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 30.96 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 33.31 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.56 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.56 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.29 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 33.38 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 10/1200 [05:28<11:24:10, 34.50s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 34.37 %\n",
      "new seed generated with accuracy  tensor(34.3700)\n",
      "New exploration domain with scale  = 0.005 ; noise ! = 0.15\n",
      "EPOCH ACCURACY = 34.37 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 19/1200 [10:17<10:29:21, 31.97s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.55 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.36 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.50 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.21 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 33.87 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 20/1200 [11:02<11:45:56, 35.90s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 34.86 %\n",
      "new seed generated with accuracy  tensor(34.8600)\n",
      "New exploration domain with scale  = 0.005 ; noise ! = 0.15\n",
      "EPOCH ACCURACY = 34.86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▏         | 28/1200 [15:02<10:36:02, 32.56s/it]No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAxYAAAGGCAYAAADmRxfNAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjguNCwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8fJSN1AAAACXBIWXMAAA9hAAAPYQGoP6dpAABwB0lEQVR4nO3dd1gUV9sG8HtBellAqoqAYsGuWEHFhliJJWjURDTG+NpLNNFoBDVqNInd2JKgMVFjj12xRYNETWIviApWsNNEaXu+P/wYGVlwFxYX9P5d1166Z87OPHt2d5hn5pwzCiGEABERERERUSEY6DsAIiIiIiIq+ZhYEBERERFRoTGxICIiIiKiQmNiQUREREREhcbEgoiIiIiICo2JBRERERERFRoTCyIiIiIiKjQmFkREREREVGhMLIiIiIiIqNCYWBAVI7GxsVAoFFi5cqW+Q3ljWrRogRYtWug7jDdCoVAgNDT0tfXu3buH999/H6VLl4ZCocC8efOKPDZ1+vXrB0tLS71sW580/ZzeBSdPnoSPjw8sLCygUChw+vRpfYeE0NBQKBQKPHz48LV13d3d0a9fv6IPiogAMLGgd9DKlSuhUCikh6mpKcqUKYOAgAAsWLAAycnJ+g6RNLRmzRq9HXQXpdGjR2Pv3r2YMGECVq9ejXbt2hXZtlJTUxEaGorDhw8X2TZeJy4uDuPHj0fLli1hZWUFhUKRbzzHjh1D06ZNYW5uDmdnZ4wYMQIpKSlvLuB3REZGBoKCgvD48WPMnTsXq1evhpubm9q6hw8flu1Xcz4++OCDNxy5/uX8O/PXX3/lWi6EgKurKxQKBTp16iRblv2677//Ps/1/vPPP1JZXonW9u3b4efnB0dHR5ibm6NChQro0aMH9uzZA+DFSZ28PrOcDybZpI1S+g6ASF+mTp0KDw8PZGRkID4+HocPH8aoUaMwZ84cbNu2DbVq1XrjMbm5ueHZs2cwMjJ649vWl3379hX4tWvWrMH58+cxatQo3QVUDBw8eBDvvfcexo4dW+TbSk1NxZQpUwBAb1eOoqKiMGvWLFSqVAk1a9ZEZGRknnVPnz6N1q1bw8vLC3PmzMHt27fx3XffITo6Grt3736DUb/9rl27hhs3bmDFihX45JNPNHrNiBEj0KBBA1mZu7t7EUSnmaioKBgY6O8cqqmpKdasWYOmTZvKyv/880/cvn0bJiYmeb7222+/xeDBg2Fubq71dr/77juMGzcOfn5+mDBhAszNzXH16lXs378f69atQ7t27TBx4kTZ53ry5EksWLAAX375Jby8vKRyffwtpJKLiQW9s9q3b4/69etLzydMmICDBw+iU6dOCAwMxKVLl2BmZvZGY8q+gvIuMTY21ncIMiqVCunp6Xr9HO7fvw8bGxudre/58+cwNjbW6wFWfry9vfHo0SPY2dlh48aNCAoKyrPul19+CVtbWxw+fBjW1tYAXhy4Dhw4EPv27UPbtm3fVNhvvfv37wOAVt/FZs2a4f333y+iiLSX34H7m9ChQwds2LABCxYsQKlSLw+51qxZA29v7zy7c9WpUwenT5/G0qVLMWbMGK22mZmZiWnTpsHf31/tiZvsz9Xf319WbmpqigULFsDf3/+d6Z5Kulc8/8oQ6UmrVq3w1Vdf4caNG/j1119lyw4ePIhmzZrBwsICNjY2eO+993Dp0iVZnexL0leuXMGHH34IpVIJBwcHfPXVVxBC4NatW3jvvfdgbW0NZ2fnXJe61Y2xyO7nfufOHXTp0gWWlpZwcHDA2LFjkZWVJXv9d999Bx8fH5QuXRpmZmbw9vbGxo0bc71PhUKBYcOGYevWrahRowZMTExQvXp16RJ5Tnfu3MGAAQNQpkwZmJiYwMPDA4MHD0Z6erpUJyEhAaNGjYKrqytMTEzg6emJWbNmQaVSvbbNXx1jkd2lYv369Zg+fTrKlSsHU1NTtG7dGlevXpW9bufOnbhx44Z0yT7nmdG0tDSEhITA09MTJiYmcHV1xeeff460tDS1bfHbb7+hevXqMDExwfbt22FnZ4f+/fvnijcpKQmmpqbS1YT09HRMnjwZ3t7eUCqVsLCwQLNmzXDo0KHXvvdXZXdzEEJg8eLF0vvKdv36dQQFBcHOzg7m5uZo3Lgxdu7cKVtHdvutW7cOkyZNQtmyZWFubo6kpKRc24uNjYWDgwMAYMqUKXl2fdDku6dSqTBv3jxUr14dpqamcHJywqBBg/DkyZPXvm8rKyvY2dm9tl5SUhLCw8Px4YcfSkkFAPTt2xeWlpZYv379a9fx/PlzhIaGonLlyjA1NYWLiwu6deuGa9eu5fmaGzduYMiQIahSpQrMzMxQunRpBAUFITY2VlYvIyMDU6ZMQaVKlWBqaorSpUujadOmCA8Pl+rEx8ejf//+KFeuHExMTODi4oL33nsv17p2794t7W+srKzQsWNHXLhwQVZH03Wp87r9Wb9+/eDn5wcACAoKgkKhKNTB5uPHjzF27FjUrFkTlpaWsLa2Rvv27XHmzJlcdRcuXIjq1avD3Nwctra2qF+/PtasWZOrXkJCAvr16wcbGxsolUr0798fqampsjrqxlho8zt63X7odXr16oVHjx7JvgPp6enYuHEjevfunefrfH190apVK8yePRvPnj3TeHsA8PDhQyQlJcHX11ftckdHR63WR6QNXrEgesVHH32EL7/8Evv27cPAgQMBAPv370f79u1RoUIFhIaG4tmzZ1i4cCF8fX3x33//5brU37NnT3h5eeGbb77Bzp078fXXX8POzg7Lli1Dq1atMGvWLPz2228YO3YsGjRogObNm+cbU1ZWFgICAtCoUSN899132L9/P77//ntUrFgRgwcPlurNnz8fgYGB6NOnD9LT07Fu3ToEBQVhx44d6Nixo2ydf/31FzZv3owhQ4bAysoKCxYsQPfu3XHz5k2ULl0aAHD37l00bNgQCQkJ+PTTT1G1alXcuXMHGzduRGpqKoyNjZGamgo/Pz/cuXMHgwYNQvny5XHs2DFMmDABcXFxBR4D8c0338DAwABjx45FYmIiZs+ejT59+uD48eMAgIkTJyIxMRG3b9/G3LlzAUAaaKxSqRAYGIi//voLn376Kby8vHDu3DnMnTsXV65cwdatW2XbOnjwINavX49hw4bB3t4elSpVQteuXbF582YsW7ZMdlVl69atSEtLk/qNJyUl4ccff0SvXr0wcOBAJCcn46effkJAQABOnDiBOnXqaPyemzdvjtWrV+Ojjz6Cv78/+vbtKy27d+8efHx8kJqaihEjRqB06dJYtWoVAgMDsXHjRnTt2lW2rmnTpsHY2Bhjx45FWlqa2itDDg4OWLJkCQYPHoyuXbuiW7duAORdHzT97g0aNAgrV65E//79MWLECMTExGDRokU4deoUIiIidNK979y5c8jMzJRdaQReXPWqU6cOTp06le/rs7Ky0KlTJxw4cAAffPABRo4cieTkZISHh+P8+fOoWLGi2tedPHkSx44dwwcffIBy5cohNjYWS5YsQYsWLXDx4kWpq0poaChmzpyJTz75BA0bNkRSUhL++ecf/Pfff9LZ4e7du+PChQsYPnw43N3dcf/+fYSHh+PmzZvSfmT16tUIDg5GQEAAZs2ahdTUVCxZsgRNmzbFqVOnpHqarEsdTfZngwYNQtmyZTFjxgype5OTk9NrP6Pk5ORcZ+Ht7Oxw/fp1bN26FUFBQfDw8MC9e/ewbNky+Pn54eLFiyhTpgwAYMWKFRgxYgTef/99jBw5Es+fP8fZs2dx/PjxXAfiPXr0gIeHB2bOnIn//vsPP/74IxwdHTFr1qw849P2d/S6/dDruLu7o0mTJli7di3at28P4EXSmJiYiA8++AALFizI87WhoaFo3rw5lixZotVVC0dHR5iZmWH79u0YPny4Rkk7kc4IondMWFiYACBOnjyZZx2lUinq1q0rPa9Tp45wdHQUjx49ksrOnDkjDAwMRN++faWykJAQAUB8+umnUllmZqYoV66cUCgU4ptvvpHKnzx5IszMzERwcLBUFhMTIwCIsLAwqSw4OFgAEFOnTpXFWLduXeHt7S0rS01NlT1PT08XNWrUEK1atZKVAxDGxsbi6tWrsvcDQCxcuFAq69u3rzAwMFDbViqVSgghxLRp04SFhYW4cuWKbPn48eOFoaGhuHnzZq7X5uTn5yf8/Pyk54cOHRIAhJeXl0hLS5PK58+fLwCIc+fOSWUdO3YUbm5uuda5evVqYWBgII4ePSorX7p0qQAgIiIiZG1hYGAgLly4IKu7d+9eAUBs375dVt6hQwdRoUIF6XlmZqYsTiFefLZOTk7i448/lpUDECEhIeob4pV6Q4cOlZWNGjVKAJC9p+TkZOHh4SHc3d1FVlaWEOJl+1WoUCHX90GdBw8e5BmXpt+9o0ePCgDit99+k9Xbs2eP2vL8bNiwQQAQhw4dynPZkSNHci0LCgoSzs7O+a77559/FgDEnDlzci3L/j4LkftzUteOkZGRAoD45ZdfpLLatWuLjh075rn9J0+eCADi22+/zbNOcnKysLGxEQMHDpSVx8fHC6VSKZVrsq68aLo/y/4ubdiw4bXrzK6r7hETEyOeP38ufUezxcTECBMTE9n367333hPVq1fPd1vZ+9lXf19du3YVpUuXlpW5ubnJ9rHa/o402Q+pk/PvzKJFi4SVlZX0PQoKChItW7aU4nv1O5Pz99+yZUvh7OwsvVbd36/s9njw4IFUNnnyZAFAWFhYiPbt24vp06eLf//9N9+Y8/vtEWmKXaGI1LC0tJRmh4qLi8Pp06fRr18/2ZmfWrVqwd/fH7t27cr1+pwD4gwNDVG/fn0IITBgwACp3MbGBlWqVMH169c1iul///uf7HmzZs1yvTbnmJAnT54gMTERzZo1w3///ZdrfW3atJGdoa1Vqxasra2ldapUKmzduhWdO3fOdYYYgNRFZ8OGDWjWrBlsbW3x8OFD6dGmTRtkZWXhyJEjGr2/V/Xv3192lr1Zs2YAoFF7bdiwAV5eXqhataosplatWgFArm5Kfn5+qFatmqysVatWsLe3x++//y6VPXnyBOHh4ejZs6dUZmhoKMWpUqnw+PFj6ay6unYvqF27dqFhw4ayQaCWlpb49NNPERsbi4sXL8rqBwcH62yM0Ou+exs2bIBSqYS/v7+svb29vWFpaVmgbmHqZHcJUddv3tTU9LVdRjZt2gR7e3sMHz4817KcXc5elbMdMzIy8OjRI3h6esLGxkb2GdvY2ODChQuIjo7Ocz3GxsY4fPhwnl3EwsPDkZCQgF69esna0tDQEI0aNZLaUpN1qVOQ/Zk2Jk+ejPDwcNnD2dkZJiYm0hifrKwsPHr0CJaWlqhSpUquNrx9+zZOnjz52m2p+14+evRIbbe/bNr+jgqzH8rWo0cPPHv2DDt27EBycjJ27NiRbzeonEJDQxEfH4+lS5dqvD3gRdfGNWvWoG7duti7dy8mTpwIb29v1KtXL1cXXiJdYmJBpEZKSgqsrKwAvOhfDQBVqlTJVc/LywsPHz7E06dPZeXly5eXPVcqlTA1NYW9vX2uck0OCkxNTaW+8NlsbW1zvXbHjh1o3LgxTE1NYWdnJ3V1SUxMzLXOV2N8dZ0PHjxAUlISatSokW9s0dHR2LNnDxwcHGSPNm3aAHg5UFBbr8Zna2sLABq1V3R0NC5cuJArpsqVK6uNycPDI9c6SpUqhe7du+OPP/6QxmVs3rwZGRkZssQCAFatWoVatWpJ/eodHBywc+dOte1eUDdu3MjzO5i9PCd176kgNPnuRUdHIzExEY6OjrnaPCUlpcDfgVdlH+C/Ok4GeDF24nWJ1LVr11ClShXZIFpNPHv2DJMnT5bGENnb28PBwQEJCQmyz3jq1KlISEhA5cqVUbNmTYwbNw5nz56VlpuYmGDWrFnYvXs3nJyc0Lx5c8yePRvx8fFSneykpFWrVrnact++fVJbarIudQqyP9NGzZo10aZNG9nD1NQUKpUKc+fORaVKlWRtePbsWVkbfvHFF7C0tETDhg1RqVIlDB06FBEREWq3VZB9hLa/o8Lsh7Jl7w/XrFmDzZs3IysrS+MB7s2bN0fLli0LNNaiV69eOHr0KJ48eYJ9+/ahd+/eOHXqFDp37oznz59rtS4iTXGMBdErbt++jcTERHh6ehZ4HYaGhhqVAS/mMy/I+l519OhRBAYGonnz5vjhhx/g4uICIyMjhIWFqR34WJh4clKpVPD398fnn3+udnn2wby2ChOfSqVCzZo1MWfOHLXLXV1dZc/zOiD94IMPsGzZMuzevRtdunTB+vXrUbVqVdSuXVuq8+uvv6Jfv37o0qULxo0bB0dHRxgaGmLmzJn5Dgguarq6WqHJd0+lUsHR0RG//fab2uWvJiYF5eLiAuDFWfdXxcXFSf30dW348OEICwvDqFGj0KRJEyiVSun+DDknKGjevDmuXbuGP/74A/v27cOPP/6IuXPnYunSpdJVzFGjRqFz587YunUr9u7di6+++gozZ87EwYMHUbduXWl9q1evhrOzc65YciZFr1tXcTJjxgx89dVX+PjjjzFt2jTY2dnBwMAAo0aNkrWhl5cXoqKisGPHDuzZswebNm3CDz/8gMmTJ0vTImfT1T4sP7raRu/evTFw4EDEx8ejffv2Ws20FRISghYtWmDZsmUFmi3O2toa/v7+8Pf3h5GREVatWoXjx49Lg/OJdImJBdErVq9eDQAICAgAAOmGUFFRUbnqXr58Gfb29rCwsHhzAeZh06ZNMDU1xd69e2VdRcLCwgq0PgcHB1hbW+P8+fP51qtYsSJSUlKkKxRvUl7dVypWrIgzZ86gdevW+XZxeZ3mzZvDxcUFv//+O5o2bYqDBw9i4sSJsjobN25EhQoVsHnzZtm2QkJCCrxdddzc3PL8DmYvL4jCtE+2ihUrYv/+/fD19S3SKZpr1KiBUqVK4Z9//kGPHj2k8vT0dJw+fVpWllecx48fR0ZGhlaDyTdu3Ijg4GDZLG7Pnz9HQkJCrrrZs4n1798fKSkpaN68OUJDQ2XdIytWrIjPPvsMn332GaKjo1GnTh18//33+PXXX6XuiY6Ojhr9pvJblzr62p9t3LgRLVu2xE8//SQrT0hIyHUl18LCAj179kTPnj2Rnp6Obt26Yfr06ZgwYUKhp4Euqt/R63Tt2hWDBg3C33//LeteqQk/Pz+0aNECs2bNwuTJkwsVR/369bFq1Sq1yTmRLrArFFEOBw8exLRp0+Dh4YE+ffoAeHGWtE6dOli1apXsQOL8+fPYt28fOnTooKdo5QwNDaFQKGTTgMbGxuaaAUlTBgYG6NKlC7Zv3y67y2u27DN2PXr0QGRkJPbu3ZurTkJCAjIzMwu0fU1YWFio7W7Uo0cP3LlzBytWrMi17NmzZxp39TAwMMD777+P7du3Y/Xq1cjMzMzVDSr7jGbOM5jHjx/P9yZvBdGhQwecOHFCtt6nT59i+fLlcHd3zzVGRFPZMxqpO0jWVI8ePZCVlYVp06blWpaZmVmodeekVCrRpk0b/Prrr9IYKODFyYCUlJR8738BvJhF6eHDh1i0aFGuZfmdgTY0NMy1fOHChbmm3H306JHsuaWlJTw9PaWuW6mpqbm6oFSsWBFWVlZSnYCAAFhbW2PGjBnIyMjIFcuDBw80Xpc6+tqfqWvDDRs24M6dO7KyV9vQ2NgY1apVgxBCbXtoq6h+R69jaWmJJUuWIDQ0FJ07d9b69dljLZYvX/7auqmpqXnuf7JvIqmuO1hBPHz4EJcvX841zS+9u3jFgt5Zu3fvxuXLl5GZmYl79+7h4MGDCA8Ph5ubG7Zt2yY7M/btt9+iffv2aNKkCQYMGCBNz6hUKnPN+a8vHTt2xJw5c9CuXTv07t0b9+/fx+LFi+Hp6Snr562NGTNmYN++ffDz85OmbY2Li8OGDRvw119/wcbGBuPGjcO2bdvQqVMn9OvXD97e3nj69CnOnTuHjRs3IjY2NtcZSV3x9vbG77//jjFjxqBBgwawtLRE586d8dFHH2H9+vX43//+h0OHDsHX1xdZWVm4fPky1q9fj71796odkK5Oz549sXDhQoSEhKBmzZqyO9ICQKdOnbB582Z07doVHTt2RExMDJYuXYpq1aohJSVFZ+91/Pjx0pSVI0aMgJ2dHVatWoWYmBhs2rSpwDe/MzMzQ7Vq1fD777+jcuXKsLOzQ40aNV47tiYnPz8/DBo0CDNnzsTp06fRtm1bGBkZITo6Ghs2bMD8+fNf26f866+/BgDpXg2rV6/GX3/9BQCYNGmSVG/69Onw8fGRvpO3b9/G999/j7Zt26Jdu3b5bqNv37745ZdfMGbMGJw4cQLNmjXD06dPsX//fgwZMgTvvfee2td16tQJq1evhlKpRLVq1RAZGYn9+/dL0zJnq1atGlq0aAFvb2/Y2dnhn3/+wcaNGzFs2DAAwJUrV9C6dWv06NED1apVQ6lSpbBlyxbcu3dPmr7Y2toaS5YswUcffYR69erhgw8+gIODA27evImdO3fC19cXixYt0mhdedHH/qxTp06YOnUq+vfvDx8fH5w7dw6//fYbKlSoIKvXtm1bODs7w9fXF05OTrh06RIWLVqEjh07SuPeCqOofkeaCA4OLvBr/fz84Ofnhz///PO1dVNTU+Hj44PGjRujXbt2cHV1RUJCArZu3YqjR4+iS5cuOusqt2jRIkyZMgWHDh3iTfXoBf1MRkWkP9nT9WU/jI2NhbOzs/D39xfz588XSUlJal+3f/9+4evrK8zMzIS1tbXo3LmzuHjxoqyOumn/hHgxbaeFhUWudfr5+cmmVsxrull1r83eVk4//fSTqFSpkjAxMRFVq1YVYWFhautBzXSmQuSemlEIIW7cuCH69u0rHBwchImJiahQoYIYOnSobArG5ORkMWHCBOHp6SmMjY2Fvb298PHxEd99951IT0/PtZ1X20DddLOvTnGprm1SUlJE7969hY2NjQAgm3o2PT1dzJo1S1SvXl2YmJgIW1tb4e3tLaZMmSISExNf2xbZVCqVcHV1FQDE119/rXb5jBkzhJubmzAxMRF169YVO3bsEMHBwbmmwkUhppsVQohr166J999/X9jY2AhTU1PRsGFDsWPHDlkdbaYIzXbs2DHh7e0tjI2NZTFq890TQojly5cLb29vYWZmJqysrETNmjXF559/Lu7evfvaGHL+Jl99vOro0aPCx8dHmJqaCgcHBzF06NA8f7evSk1NFRMnThQeHh7CyMhIODs7i/fff19cu3ZNFkvOz+nJkyeif//+wt7eXlhaWoqAgABx+fLlXL+Xr7/+WjRs2FDY2NgIMzMzUbVqVTF9+nTpN/Dw4UMxdOhQUbVqVWFhYSGUSqVo1KiRWL9+fa44Dx06JAICAoRSqRSmpqaiYsWKol+/fuKff/7Rel3qaLI/K8h0s3nVff78ufjss8+Ei4uLMDMzE76+viIyMjLX73/ZsmWiefPmonTp0sLExERUrFhRjBs3TvabzWs/m71vj4mJkcrU7dMK8ztStx9SR5NpzbPjy2+6WXUxvbreV9sjIyNDrFixQnTp0kXaL5mbm4u6deuKb7/9Ntf02NkKMt1s9rY5RS1lUwihw1FORERERET0TuIYCyIiIiIiKjQmFkREREREVGhMLIiIiIiIqNCYWBARERERUaExsSAiIiIiokJjYkFERERERIXGxIKIqAgpFArZTcdWrlwJhUKB2NjYNxpHv379YGlp+Ua3WVK9+pm9y7K/r//884++QyGiEoCJBRFRDrGxsVAoFGofjRs31nd4xcLFixcRGhr6xpOjnNasWYN58+bpbftF4dixYwgNDUVCQoK+QyEiKpBS+g6AiKg46tWrFzp06CArc3Bw0FM0xcvFixcxZcoUtGjRAu7u7nqJYc2aNTh//jxGjRqll+0XhWPHjmHKlCno168fbGxs9B0OEZHWmFgQEalRr149fPjhh/oOo8QTQuD58+cwMzPTdyikhefPn8PY2FjfYRBRCcOuUEREWmrRogVatGiRq7xfv35an8EPDg6Gvb09MjIyci1r27YtqlSpku/rjx49iqCgIJQvXx4mJiZwdXXF6NGj8ezZM7X1r1+/joCAAFhYWKBMmTKYOnUqhBCyOuvWrYO3tzesrKxgbW2NmjVrYv78+QBe9LkPCgoCALRs2VLqJnb48GEAgLu7Ozp16oS9e/eifv36MDMzw7JlywAAYWFhaNWqFRwdHWFiYoJq1aphyZIlauPcvXs3/Pz8pBgaNGiANWvWAHjR/jt37sSNGzek7eds97S0NISEhMDT01Nqk88//xxpaWmybaSlpWH06NFwcHCAlZUVAgMDcfv27XzbO9vhw4ehUCiwfv16TJ8+HeXKlYOpqSlat26Nq1ev5qp//PhxtGvXDkqlEubm5vDz80NERIS0PDQ0FOPGjQMAeHh4SO8rNjYW3bp1Q7169WTr69y5MxQKBbZt2ybbhkKhwO7du6Wy69evIygoCHZ2djA3N0fjxo2xc+dOte9l3bp1mDRpEsqWLQtzc3MkJSWpfe9PnjxBw4YNUa5cOURFRWnUXkT0buAVCyIiNVJTU/Hw4UNZmVKphJGRkU6389FHH+GXX37B3r170alTJ6k8Pj4eBw8eREhISL6v37BhA1JTUzF48GCULl0aJ06cwMKFC3H79m1s2LBBVjcrKwvt2rVD48aNMXv2bOzZswchISHIzMzE1KlTAQDh4eHo1asXWrdujVmzZgEALl26hIiICIwcORLNmzfHiBEjsGDBAnz55Zfw8vICAOlfAIiKikKvXr0waNAgDBw4UEqOlixZgurVqyMwMBClSpXC9u3bMWTIEKhUKgwdOlR6/cqVK/Hxxx+jevXqmDBhAmxsbHDq1Cns2bMHvXv3xsSJE5GYmIjbt29j7ty5ACANTFepVAgMDMRff/2FTz/9FF5eXjh37hzmzp2LK1euYOvWrdJ2PvnkE/z666/o3bs3fHx8cPDgQXTs2FGrz++bb76BgYEBxo4di8TERMyePRt9+vTB8ePHpToHDx5E+/bt4e3tjZCQEBgYGEhJ1tGjR9GwYUN069YNV65cwdq1azF37lzY29sDeNH9rlmzZvjjjz+QlJQEa2trCCEQEREBAwMDHD16FIGBgQBeJJkGBgbw9fUFANy7dw8+Pj5ITU3FiBEjULp0aaxatQqBgYHYuHEjunbtKnsv06ZNg7GxMcaOHYu0tDS1VywePnwIf39/PH78GH/++ScqVqyoVXsR0VtOEBGRJCYmRgBQ+zh06JAQQgg/Pz/h5+eX67XBwcHCzc1NVgZAhISESM/DwsIEABETEyOEECIrK0uUK1dO9OzZU/a6OXPmCIVCIa5fv55vvKmpqbnKZs6cKRQKhbhx44YsNgBi+PDhUplKpRIdO3YUxsbG4sGDB0IIIUaOHCmsra1FZmZmntvcsGGDrD1ycnNzEwDEnj17NIo1ICBAVKhQQXqekJAgrKysRKNGjcSzZ89kdVUqlfT/jh075mprIYRYvXq1MDAwEEePHpWVL126VAAQERERQgghTp8+LQCIIUOGyOr17t0712emzqFDhwQA4eXlJdLS0qTy+fPnCwDi3LlzUsyVKlUSAQEBsvhTU1OFh4eH8Pf3l8q+/fZb2Xcj28mTJwUAsWvXLiGEEGfPnhUARFBQkGjUqJFULzAwUNStW1d6PmrUKAFA1hbJycnCw8NDuLu7i6ysLNl7qVChQq7PKPv7evLkSREXFyeqV68uKlSoIGJjY/NtHyJ6N7ErFBGRGp9++inCw8Nlj9q1a+t8OwYGBujTpw+2bduG5ORkqfy3336Dj48PPDw88n19zrELT58+xcOHD+Hj4wMhBE6dOpWr/rBhw6T/KxQKDBs2DOnp6di/fz8AwMbGBk+fPkV4eHiB35OHhwcCAgLyjTUxMREPHz6En58frl+/jsTERAAvrpgkJydj/PjxMDU1lb1eoVC8dtsbNmyAl5cXqlatiocPH0qPVq1aAQAOHToEANi1axcAYMSIEbLXazsYvH///rIz+82aNQPwogsSAJw+fRrR0dHo3bs3Hj16JMXz9OlTtG7dGkeOHIFKpcp3G3Xr1oWlpSWOHDkC4MWViXLlyqFv377477//kJqaCiEE/vrrL2n72e+xYcOGaNq0qVRmaWmJTz/9FLGxsbh48aJsO8HBwXmOhbl9+zb8/PyQkZGBI0eOwM3NTYtWIqJ3BbtCERGpUalSJbRp0+aNbKtv376YNWsWtmzZgr59+yIqKgr//vsvli5d+trX3rx5E5MnT8a2bdvw5MkT2bLsg/VsBgYGqFChgqyscuXKACBNHTtkyBCsX78e7du3R9myZdG2bVv06NED7dq10/j95JUMRUREICQkBJGRkUhNTc0Vq1KpxLVr1wAANWrU0Hh7OUVHR+PSpUt5zuB1//59AMCNGzdgYGCQqyvP68a0vKp8+fKy57a2tgAgfRbR0dEAXhy05yUxMVF6nTqGhoZo0qQJjh49CuBFYtGsWTM0bdoUWVlZ+Pvvv+Hk5ITHjx/LEosbN26gUaNGudaX3W3txo0bsnbOL4n96KOPUKpUKVy6dAnOzs551iOidxsTCyIiLSkUilwDnoEXYxgKolq1avD29savv/6Kvn374tdff4WxsTF69OiR7+uysrKk/u5ffPEFqlatCgsLC9y5cwf9+vV77ZlwdRwdHXH69Gns3bsXu3fvxu7duxEWFoa+ffti1apVGq1D3Vnva9euoXXr1qhatSrmzJkDV1dXGBsbY9euXZg7d26BYlVHpVKhZs2amDNnjtrlrq6uOtlONkNDQ7Xl2d+P7Pf17bffok6dOmrranLjwqZNm2L69Ol4/vw5jh49iokTJ8LGxgY1atTA0aNH4eTkBACyxEJb+c3c1a1bN/zyyy+YP38+Zs6cWeBtENHbjYkFEZGWbG1tpa4uOd24caPA6+zbty/GjBmDuLg4rFmzBh07dsz3LDYAnDt3DleuXMGqVavQt29fqTyvbkwqlQrXr1+XrlIAwJUrVwBANquSsbExOnfujM6dO0OlUmHIkCFYtmwZvvrqK3h6emrUJelV27dvR1paGrZt2yY7y5/dNSlb9hWE8+fPw9PTM8/15RVDxYoVcebMGbRu3TrfON3c3KBSqXDt2jXZVQpdz3KU/X6sra1fewUsv3ibNWuG9PR0rF27Fnfu3JESiObNm0uJReXKlaUEA3jxHtW9n8uXL0vLNTV8+HB4enpi8uTJUCqVGD9+vMavJaJ3B8dYEBFpqWLFirh8+TIePHgglZ05c0Y2fai2evXqBYVCgZEjR+L69esa3UMj+2x5zqsnQghpalh1Fi1aJKu7aNEiGBkZoXXr1gCAR48eyeobGBigVq1aACBN12phYQEAWt0hWl2siYmJCAsLk9Vr27YtrKysMHPmTDx//ly2LOdrLSwscnX1AoAePXrgzp07WLFiRa5lz549w9OnTwEA7du3BwAsWLBAVkfXd/P29vZGxYoV8d133yElJSXX8pzfofzatVGjRjAyMsKsWbNgZ2eH6tWrA3iRcPz999/4888/c12t6NChA06cOIHIyEip7OnTp1i+fDnc3d1RrVo1rd7LV199hbFjx2LChAl5ThNMRO82XrEgItLSxx9/jDlz5iAgIAADBgzA/fv3sXTpUlSvXj3Puf9fx8HBAe3atcOGDRtgY2Oj0bSnVatWRcWKFTF27FjcuXMH1tbW2LRpU66xFtlMTU2xZ88eBAcHo1GjRti9ezd27tyJL7/8UhqT8Mknn+Dx48do1aoVypUrhxs3bmDhwoWoU6eO1De/Tp06MDQ0xKxZs5CYmAgTExPp/hR5adu2rXQlZNCgQUhJScGKFSvg6OiIuLg4qZ61tTXmzp2LTz75BA0aNEDv3r1ha2uLM2fOIDU1VeqO5e3tjd9//x1jxoxBgwYNYGlpic6dO+Ojjz7C+vXr8b///Q+HDh2Cr68vsrKycPnyZaxfv166v0adOnXQq1cv/PDDD0hMTISPjw8OHDig9h4UhWFgYIAff/wR7du3R/Xq1dG/f3+ULVsWd+7cwaFDh2BtbY3t27dL7wkAJk6ciA8++ABGRkbo3LkzLCwsYG5uDm9vb/z999/SPSyAF1csnj59iqdPn+ZKLMaPH4+1a9eiffv2GDFiBOzs7LBq1SrExMRg06ZNMDDQ/tzit99+i8TERAwdOhRWVla8iSQRyeltPioiomIoe7rZb7/9Nt96v/76q6hQoYIwNjYWderUEXv37i3QdLM5rV+/XgAQn376qcbxXrx4UbRp00ZYWloKe3t7MXDgQHHmzBkBQISFhUn1goODhYWFhbh27Zpo27atMDc3F05OTiIkJESadlQIITZu3Cjatm0rHB0dhbGxsShfvrwYNGiQiIuLk213xYoVokKFCsLQ0FA29aybm5vo2LGj2li3bdsmatWqJUxNTYW7u7uYNWuW+Pnnn9W2x7Zt24SPj48wMzMT1tbWomHDhmLt2rXS8pSUFNG7d29hY2MjAMjaPT09XcyaNUtUr15dmJiYCFtbW+Ht7S2mTJkiEhMTpXrPnj0TI0aMEKVLlxYWFhaic+fO4tatW1pNN7thwwZZefb3J2fbCyHEqVOnRLdu3UTp0qWFiYmJcHNzEz169BAHDhyQ1Zs2bZooW7asMDAwyNUu48aNEwDErFmzZK/x9PQUAMS1a9dyxXnt2jXx/vvvCxsbG2FqaioaNmwoduzYodF7EUI+3Wy2rKws0atXL1GqVCmxdevWfNuJiN4tCiHUjEAkIqI37o8//kCXLl1w5MiRQg3CJSIi0gcmFkRExUSnTp1w6dIlXL16tUADpImIiPSJYyyIiPRs3bp1OHv2LHbu3In58+czqSAiohKJVyyIiPRMoVDA0tISPXv2xNKlS1GqFM/5EBFRycO/XkREesbzO0RE9DbgfSyIiIiIiKjQmFgQEREREVGhsStUDiqVCnfv3oWVlRUHTxIRERHRO08IgeTkZJQpU+a1N9ZkYpHD3bt34erqqu8wiIiIiIiKlVu3bqFcuXL51mFikYOVlRWAFw1nbW2t52iIiIiIiPQrKSkJrq6u0nFyfphY5JDd/cna2pqJBREVK3FxLx665OLy4kFERPQ6mgwTYGJBRFQCLFsGTJmi23WGhAChobpdJxERvbuYWBARlQCDBgGBgbpdJ69WEBGRLjGxICIqAdhtiYiIijsmFkREREREb7GsrCxkZGSoXWZkZARDQ0OdbIeJBRERERHRW0gIgfj4eCQkJORbz8bGBs7OzoW+jxsTCyIiIqK3FGeUe7dlJxWOjo4wNzfPlTgIIZCamor79+8DAFwK+cEysSAiIiJ6S3FGuXdXVlaWlFSULl06z3pmZmYAgPv378PR0bFQ3aKYWBAREdFbRddn6UvyGXrOKPfuyh5TYW5u/tq62XUyMjKYWBARERFl0/VZ+pJ8hr4kJ0WkG5qMmyjs2IpsTCyIiIjoraLrs/Q8MCfSDBMLIiIieqvwLD2RfjCxeMPY75OIiIiI3kZMLN4w9vskIiIiojdFCKGTOppgYvGGsd8n5YdXtIiIiEgXjIyMAACpqanSlLJ5SU1Nlb2moJhYvGE80KP88IoWERER6YKhoSFsbGykm9+97gZ5NjY2hZpqFihhicWdO3fwxRdfYPfu3UhNTYWnpyfCwsJQv359AC8aJyQkBCtWrEBCQgJ8fX2xZMkSVKpUSc+RE2mGV7SIiIhIV5ydnQFASi7yYmNjI9UtjBKTWDx58gS+vr5o2bIldu/eDQcHB0RHR8PW1laqM3v2bCxYsACrVq2Ch4cHvvrqKwQEBODixYswNTXVY/REmuEVLSIiItIVhUIBFxcXODo6SjfMe5WRkVGhr1RI2xO6Gq1RxMaPH4+IiAgcPXpU7XIhBMqUKYPPPvsMY8eOBQAkJibCyckJK1euxAcffPDabSQlJUGpVCIxMRHW1tY6jZ+IiIiIqKTR5vjY4A3FVGjbtm1D/fr1ERQUBEdHR9StWxcrVqyQlsfExCA+Ph5t2rSRypRKJRo1aoTIyEh9hExERERE9M4oMV2hrl+/jiVLlmDMmDH48ssvcfLkSYwYMQLGxsYIDg5GfHw8AMDJyUn2OicnJ2nZq9LS0pCWliY9T0pKKro3QEREOsMZ1IiIip8Sk1ioVCrUr18fM2bMAADUrVsX58+fx9KlSxEcHFygdc6cORNTdDkFDxERvRGcQY2IqPgpMYmFi4sLqlWrJivz8vLCpk2bALwc9X7v3j245DjtdO/ePdSpU0ftOidMmIAxY8ZIz5OSkuDq6qrjyImISNc4gxoRUfFTYhILX19fREVFycquXLkCNzc3AICHhwecnZ1x4MABKZFISkrC8ePHMXjwYLXrNDExgYmJSZHGTUREuseuS0RExU+JSSxGjx4NHx8fzJgxAz169MCJEyewfPlyLF++HMCL6bRGjRqFr7/+GpUqVZKmmy1Tpgy6dOmi3+CJiIiIiN5yJSaxaNCgAbZs2YIJEyZg6tSp8PDwwLx589CnTx+pzueff46nT5/i008/RUJCApo2bYo9e/bwHhZEREREREWsxNzH4k3gfSyIiIiIiF56K+9jQURERERExRcTCyIiIiIiKjQmFkREREREVGhMLIiIiIiIqNCYWBARERERUaExsSAiIiIiokJjYkFERERERIXGxIKIiIiIiAqNiQURERERERUaEwsiIiIiIio0JhZERERERFRoTCyIiIiIiKjQSuk7ACIideLiXjx0ycXlxYOIiIh0j4kFERVLy5YBU6bodp0hIUBoqG7XSURERC8wsSCiYmnQICAwULfr5NUKIiKiosPEgoiKJXZbIiIiKlk4eJuIiIiIiAqNiQURERERERUaEwsiIiIiIio0JhZERERERFRoTCyIiIiIiKjQmFgQEREREVGhMbEgIiIiIqJCY2JBRERERESFVqAb5EVHR+PQoUO4f/8+VCqVbNnkyZN1EhgREREREZUcWicWK1aswODBg2Fvbw9nZ2coFAppmUKhYGJBWomLe/HQFd6tmYiIiEg/tE4svv76a0yfPh1ffPFFUcRD75hly4ApU3S3vpAQIDRUd+sjIiIiIs1onVg8efIEQUFBRRELvYMGDQICA3W3Pl6tICIiItIPrROLoKAg7Nu3D//73/+KIh56x7DrEhEREb0p7IJdtLROLDw9PfHVV1/h77//Rs2aNWFkZCRbPmLECJ0FR0RERESkK+yCXbQUQgihzQs8PDzyXplCgevXrxc6KH1JSkqCUqlEYmIirK2t9R0OEREREekQr1hoT5vjY62vWMTExBQ4MCIiIiIifXkXEgF94g3yiIiIiIio0DS6YjFmzBhMmzYNFhYWGDNmTL5158yZo5PAiIiIiIio5NAosTh16hQyMjKk/+cl583yiIiIiIjo3aH14O23GQdvExERERG9pM3xMcdYEBERERFRoWk9KxQA/PPPP1i/fj1u3ryJ9PR02bLNmzfrJDAiIiIiIio5tL5isW7dOvj4+ODSpUvYsmULMjIycOHCBRw8eBBKpbIoYiQiIiIiomJO68RixowZmDt3LrZv3w5jY2PMnz8fly9fRo8ePVC+fPmiiJGIiIiIiIo5rROLa9euoWPHjgAAY2NjPH36FAqFAqNHj8by5ct1HiARERERERV/WicWtra2SE5OBgCULVsW58+fBwAkJCQgNTVVt9EREREREVGJoPXg7ebNmyM8PBw1a9ZEUFAQRo4ciYMHDyI8PBytW7cuihiJiIiIiKiY0zqxWLRoEZ4/fw4AmDhxIoyMjHDs2DF0794dkyZN0nmARERERERU/PEGeTnwBnlERERERC9pc3xcoPtYAMD9+/dx//59qFQqWXmtWrUKukoiIiIqgLi4Fw9dcXF58SAi0obWicW///6L4OBgXLp0Ca9e7FAoFMjKytJZcERERPR6y5YBU6bobn0hIUBoqO7WR0TvBq0Ti48//hiVK1fGTz/9BCcnJygUiqKIi4iIiDQ0aBAQGKi79fFqBREVhNaJxfXr17Fp0yZ4enoWRTxERESkJXZdIqLiQOv7WLRu3RpnzpwpiliIiIiIiKiE0jqx+PHHH/Hzzz9jypQp2LRpE7Zt2yZ7vCnffPMNFAoFRo0aJZU9f/4cQ4cORenSpWFpaYnu3bvj3r17bywmIiIiIqJ3ldZdoSIjIxEREYHdu3fnWvamBm+fPHkSy5YtyzUD1ejRo7Fz505s2LABSqUSw4YNQ7du3RAREVHkMRERERERvcu0vmIxfPhwfPjhh4iLi4NKpZI93kRSkZKSgj59+mDFihWwtbWVyhMTE/HTTz9hzpw5aNWqFby9vREWFoZjx47h77//LvK4iIiIiIjeZVonFo8ePcLo0aPh5ORUFPG81tChQ9GxY0e0adNGVv7vv/8iIyNDVl61alWUL18ekZGRateVlpaGpKQk2YOIiIiIiLSndWLRrVs3HDp0qChiea1169bhv//+w8yZM3Mti4+Ph7GxMWxsbGTlTk5OiI+PV7u+mTNnQqlUSg9XV9eiCJuIiIiI6K2n9RiLypUrY8KECfjrr79Qs2ZNGBkZyZaPGDFCZ8HldOvWLYwcORLh4eEwNTXVyTonTJiAMWPGSM+TkpKYXBARERERFYBCvHr77Nfw8PDIe2UKBa5fv17ooNTZunUrunbtCkNDQ6ksKysLCoUCBgYG2Lt3L9q0aYMnT57Irlq4ublh1KhRGD169Gu3kZSUBKVSicTERFhbWxfF2yAiIiIiKjG0OT7W+opFTExMgQMrjNatW+PcuXOysv79+6Nq1ar44osv4OrqCiMjIxw4cADdu3cHAERFReHmzZto0qSJPkImIiIiInpnaJ1Y5JR9sUOhUOgkmPxYWVmhRo0asjILCwuULl1aKh8wYADGjBkDOzs7WFtbY/jw4WjSpAkaN25c5PEREREREb3LtB68DQC//PILatasCTMzM5iZmaFWrVpYvXq1rmPT2ty5c9GpUyd0794dzZs3h7OzMzZv3qzvsIiIiIiI3npaj7GYM2cOvvrqKwwbNgy+vr4AgL/++guLFy/G119/rdFYhuKKYyyIiIiIiF7S5vi4QIO3p0yZgr59+8rKV61ahdDQUL2NwdAFJhZERERERC9pc3ysdVeouLg4+Pj45Cr38fFBXFyctqsjIiIiIqK3gNaJhaenJ9avX5+r/Pfff0elSpV0EhQREREREZUsWs8KNWXKFPTs2RNHjhyRxlhERETgwIEDahMOIiIiIiJ6+2l9xaJ79+44fvw47O3tsXXrVmzduhX29vY4ceIEunbtWhQxEhERERFRMaf14O23GQdvExERERG9pPM7byclJWm8cR6QExERERG9ezRKLGxsbDS+u3ZWVlahAiIiIiIiopJHo8Ti0KFD0v9jY2Mxfvx49OvXD02aNAEAREZGYtWqVZg5c2bRRElERERERMWa1mMsWrdujU8++QS9evWSla9ZswbLly/H4cOHdRnfG8UxFkRERERELxXpDfIiIyNRv379XOX169fHiRMntF0dERERERG9BbROLFxdXbFixYpc5T/++CNcXV11EhQREREREZUsWt8gb+7cuejevTt2796NRo0aAQBOnDiB6OhobNq0SecBEhERERFR8af1FYsOHTogOjoagYGBePz4MR4/fozOnTvjypUr6NChQ1HESERERERExRxvkJcDB28TEREREb1UpIO3iYiIiIiIXsXEgoiIiIiICo2JBRERERERFRoTCyIiIiIiKjStE4tWrVohISEhV3lSUhJatWqli5iIiIiIiKiE0TqxOHz4MNLT03OVP3/+HEePHtVJUEREREREVLJofIO8s2fPSv+/ePEi4uPjpedZWVnYs2cPypYtq9voiIiIiIioRNA4sahTpw4UCgUUCoXaLk9mZmZYuHChToMjIiIiIqKSQePEIiYmBkIIVKhQASdOnICDg4O0zNjYGI6OjjA0NCySIImIiIiIqHjTOLFwc3MDAKhUqiILhoiIiIiISiatB2/PnDkTP//8c67yn3/+GbNmzdJJUEREREREVLJonVgsW7YMVatWzVVevXp1LF26VCdBERERERFRyaJ1YhEfHw8XF5dc5Q4ODoiLi9NJUEREREREVLJonVi4uroiIiIiV3lERATKlCmjk6CIiIiIiKhk0XjwdraBAwdi1KhRyMjIkKadPXDgAD7//HN89tlnOg+QiIiIiIiKP60Ti3HjxuHRo0cYMmSIdAduU1NTfPHFF5gwYYLOAyQiIiIiouJPIYQQBXlhSkoKLl26BDMzM1SqVAkmJia6ju2NS0pKglKpRGJiIqytrfUdDhERERGRXmlzfKz1FYtslpaWaNCgQUFfTkREREREb5ECJRb//PMP1q9fj5s3b0rdobJt3rxZJ4EREREREVHJofWsUOvWrYOPjw8uXbqELVu2ICMjAxcuXMDBgwehVCqLIkYiIiIiIirmtE4sZsyYgblz52L79u0wNjbG/PnzcfnyZfTo0QPly5cvihiJiIiIiKiY0zqxuHbtGjp27AgAMDY2xtOnT6FQKDB69GgsX75c5wESEREREVHxp3ViYWtri+TkZABA2bJlcf78eQBAQkICUlNTdRsdERERERGVCFoP3m7evDnCw8NRs2ZNBAUFYeTIkTh48CDCw8PRunXrooiRiIiIiIiKOa0Ti0WLFuH58+cAgIkTJ8LIyAjHjh1D9+7dMWnSJJ0HSERERERExZ9WiUVmZiZ27NiBgIAAAICBgQHGjx9fJIEREREREVHJodUYi1KlSuF///ufdMWCiIiIiIgIKMDg7YYNG+L06dNFEAoREREREZVUWo+xGDJkCMaMGYNbt27B29sbFhYWsuW1atXSWXBERERERFQyKIQQQpsXGBjkvsihUCgghIBCoUBWVpbOgnvTkpKSoFQqkZiYCGtra32HQ0RERESkV9ocH2t9xSImJqbAgRERERER0dtJ68Tixo0b8PHxQalS8pdmZmbi2LFjcHNz01lwRERERERUMmg9eLtly5Z4/PhxrvLExES0bNlSJ0EREREREVHJonVikT2W4lWPHj3KNZCbiIiIiIjeDRp3herWrRuAFwO1+/XrBxMTE2lZVlYWzp49Cx8fH91H+P9mzpyJzZs34/LlyzAzM4OPjw9mzZqFKlWqSHWeP3+Ozz77DOvWrUNaWhoCAgLwww8/wMnJqcjiIiIiIiIiLa5YKJVKKJVKCCFgZWUlPVcqlXB2dsann36KX3/9tcgC/fPPPzF06FD8/fffCA8PR0ZGBtq2bYunT59KdUaPHo3t27djw4YN+PPPP3H37l0pISIiIiIioqKj9XSzU6ZMwdixY/Xe7enBgwdwdHTEn3/+iebNmyMxMREODg5Ys2YN3n//fQDA5cuX4eXlhcjISDRu3Pi16+R0s0REREREL2lzfKz1GIvPP/9cNsbixo0bmDdvHvbt26d9pIWQmJgIALCzswMA/Pvvv8jIyECbNm2kOlWrVkX58uURGRn5RmMjIiIiInrXaJ1YvPfee/jll18AAAkJCWjYsCG+//57vPfee1iyZInOA1RHpVJh1KhR8PX1RY0aNQAA8fHxMDY2ho2Njayuk5MT4uPj1a4nLS0NSUlJsgcREREREWlP68Tiv//+Q7NmzQAAGzduhLOzM27cuIFffvkFCxYs0HmA6gwdOhTnz5/HunXrCrWemTNnysaKuLq66ihCIiIiIqJ3i9aJRWpqKqysrAAA+/btQ7du3WBgYIDGjRvjxo0bOg/wVcOGDcOOHTtw6NAhlCtXTip3dnZGeno6EhISZPXv3bsHZ2dnteuaMGECEhMTpcetW7eKMnQiIiIioreW1omFp6cntm7dilu3bmHv3r1o27YtAOD+/ftFOuBZCIFhw4Zhy5YtOHjwIDw8PGTLvb29YWRkhAMHDkhlUVFRuHnzJpo0aaJ2nSYmJrC2tpY9iIiIiIhIexrfxyLb5MmT0bt3b4wePRqtWrWSDtr37duHunXr6jzAbEOHDsWaNWvwxx9/wMrKSho3oVQqYWZmBqVSiQEDBmDMmDGws7ODtbU1hg8fjiZNmmg0IxQRERERERWc1tPNAi8GSsfFxaF27dowMHhx0ePEiROwtrZG1apVdR4kALV3+waAsLAw9OvXD8DLG+StXbtWdoO8vLpCvYrTzRIRERERvaTN8XGBEots2WMS3pZBz0wsiIiIiIhe0ub4WOuuUJmZmZgyZQoWLFiAlJQUAIClpSWGDx+OkJAQGBkZFSxqIiIiIiqYuLgXD31ycXnxoHeW1onF8OHDsXnzZsyePVsaXxEZGYnQ0FA8evTojd3LgojorVMcDgwAHhwQlUTLlgFTpug3hpAQIDRUvzGQXmndFUqpVGLdunVo3769rHzXrl3o1auXdEfskohdoYhIr0JD9X9gAPDggKgkKg4nJnhS4q1UpF2hTExM4O7unqvcw8MDxsbG2q6OiIiyDRoEBAbqOwoeGBCVRDyop2JA68Ri2LBhmDZtGsLCwmBiYgIASEtLw/Tp0zFs2DCdB0j01ikOZ5UA/hEqjviZyPG3Qvnh94Oo2NEosejWrZvs+f79+1GuXDnUrl0bAHDmzBmkp6ejdevWuo+wJOLOjvJTHPrBAuzuQsUffyuUH34/iIodjcZY9O/fX+MVhoWFFSogfdLZGAv2k6b8MPEk0gx/K5Qffj+I3og3dh+Lt43OEgvu7IiIiIjoLVCkg7dJAzygJyIiIqJ3TIESi40bN2L9+vW4efMm0tPTZcv+++8/nQRGREREREQlh4G2L1iwYAH69+8PJycnnDp1Cg0bNkTp0qVx/fr1XPe2ICIiIiKid4PWicUPP/yA5cuXY+HChTA2Nsbnn3+O8PBwjBgxokTfHI+IiIiIiApO68Ti5s2b8PHxAQCYmZkhOTkZAPDRRx9h7dq1uo2OiIiIiIhKBK0TC2dnZzx+/BgAUL58efz9998AgJiYGHCCKSIiIiKid5PWiUWrVq2wbds2AC/ubzF69Gj4+/ujZ8+e6Nq1q84DJCIiIiKi4k/r+1ioVCqoVCqUKvViQql169bh2LFjqFSpEgYNGgRjY+MiCfRN0Nl9LIhIc7zvCxERUbHFG+QVEBMLIj3gneqJiIiKLd4gj4hKjkGDgMBAfUfBqxVERESFxMSCih67ulB++LkQERG9FZhYUNFbtoxdXYiIiIjeckwsqOixqwsRERHRW69AiUVmZiYOHz6Ma9euoXfv3rCyssLdu3dhbW0NS0tLXcdIJR27uhARERG99bROLG7cuIF27drh5s2bSEtLg7+/P6ysrDBr1iykpaVh6dKlRREnEREREREVY1rfIG/kyJGoX78+njx5AjMzM6m8a9euOHDggE6DIyIiIiKikkHrKxZHjx7FsWPHct0Iz93dHXfu3NFZYEREREREWuNslHqjdWKhUqmQlZWVq/z27duwsrLSSVBERET0Ch4sEWmGs1HqjdaJRdu2bTFv3jwsX74cAKBQKJCSkoKQkBB06NBB5wESEREReLBEpCnORqk3CiGE0OYFt2/fRkBAAIQQiI6ORv369REdHQ17e3scOXIEjo6ORRVrkdPmluVERERvFK9YEJEeaHN8rHViAbyYbnbdunU4e/YsUlJSUK9ePfTp00c2mLskYmJBRERERPSSNsfHBbqPRalSpfDhhx8WKDgiIiIiInr7FCixiI6OxqFDh3D//n2oVCrZssmTJ+skMCIiIiIiKjm0TixWrFiBwYMHw97eHs7OzlAoFNIyhULBxIKIiIiI6B2kdWLx9ddfY/r06fjiiy+KIh4iIiIiIiqBtL7z9pMnTxAUFFQUsRARERERUQmldWIRFBSEffv2FUUsRERERERUQmnUFWrBggXS/z09PfHVV1/h77//Rs2aNWFkZCSrO2LECN1GSERERERExZ5G97Hw8PDQbGUKBa5fv17ooPSF97EgIiIiInpJ5/exiImJ0UlgRERERET0dtJ6jMXUqVORmpqaq/zZs2eYOnWqToIiIiIiIqKSRaOuUDkZGhoiLi4Ojo6OsvJHjx7B0dERWVlZOg3wTWJXKCIiIiKil7Q5Ptb6ioUQQnZTvGxnzpyBnZ2dtqsjIiIiIqK3gMY3yLO1tYVCoYBCoUDlypVlyUVWVhZSUlLwv//9r0iCJCIiIiKi4k3jxGLevHkQQuDjjz/GlClToFQqpWXGxsZwd3dHkyZNiiRIIiIiIiIq3jROLIKDgwG8mHrW19cXpUpp/FIiIiIiInrLaZ0d+Pn5FUUcRERERERUgmk9eJuIiIiIiOhVTCyIiIiIiKjQNEoszp49C5VKVdSxEBERERFRCaVRYlG3bl08fPgQAFChQgU8evSoSIMiIiIiIqKSRaPEwsbGBjExMQCA2NhYXr0gIiIiIiIZjRKL7t27w8/PDx4eHlAoFKhfvz4qVKig9lEcLF68GO7u7jA1NUWjRo1w4sQJfYdERERERPRW02i62eXLl6Nbt264evUqRowYgYEDB8LKyqqoYyuQ33//HWPGjMHSpUvRqFEjzJs3DwEBAYiKioKjo6O+wyMiIiIieisphBBCmxf0798fCxYsKLaJRaNGjdCgQQMsWrQIAKBSqeDq6orhw4dj/Pjx+b42KSkJSqUSiYmJsLa2fhPhEhEREREVW9ocH2s93WxYWJiUVNy+fRu3b98uWJRFID09Hf/++y/atGkjlRkYGKBNmzaIjIzUY2RERERERG83rRMLlUqFqVOnQqlUws3NDW5ubrCxscG0adP0Pqj74cOHyMrKgpOTk6zcyckJ8fHxueqnpaUhKSlJ9iAiIiIiIu1pNMYip4kTJ+Knn37CN998A19fXwDAX3/9hdDQUDx//hzTp0/XeZBFZebMmZgyZUqRb2ftubVYe35tkW8HAHrV6IVeNXu9kW0VFNvjJbaFHNtDju0hx/Z4iW0hx/aQY3vIsT3eHK3HWJQpUwZLly5FYGCgrPyPP/7AkCFDcOfOHZ0GqI309HSYm5tj48aN6NKli1QeHByMhIQE/PHHH7L6aWlpSEtLk54nJSXB1dWVYyyIiIiIiFDEYyweP36MqlWr5iqvWrUqHj9+rO3qdMrY2Bje3t44cOCAVKZSqXDgwAE0adIkV30TExNYW1vLHkREREREpD2tE4vatWtLMy7ltGjRItSuXVsnQRXGmDFjsGLFCqxatQqXLl3C4MGD8fTpU/Tv31/foRERERERvbW0HmMxe/ZsdOzYEfv375euAkRGRuLWrVvYtWuXzgPUVs+ePfHgwQNMnjwZ8fHxqFOnDvbs2ZNrQDcREREREemO1mMsAODu3btYvHgxLl++DADw8vLCkCFDUKZMGZ0H+CbxPhZERERERC9pc3xcoMTibcXEgoiIiIjopSIdvE1ERERERPQqJhZERERERFRoTCyIiIiIiKjQmFgQEREREVGhFSixyMzMxP79+7Fs2TIkJycDeDFTVEpKik6DIyIiIiKikkHr+1jcuHED7dq1w82bN5GWlgZ/f39YWVlh1qxZSEtLw9KlS4siTiIiIiIiKsa0vmIxcuRI1K9fH0+ePIGZmZlU3rVrVxw4cECnwRERERERUcmg9RWLo0eP4tixYzA2NpaVu7u7486dOzoLjIiIiIiISg6tr1ioVCpkZWXlKr99+zasrKx0EhQREREREZUsWicWbdu2xbx586TnCoUCKSkpCAkJQYcOHXQZGxERERERlRAKIYTQ5gW3b99GQEAAhBCIjo5G/fr1ER0dDXt7exw5cgSOjo5FFWuR0+aW5UREREREbzttjo+1TiyAF9PNrlu3DmfPnkVKSgrq1auHPn36yAZzl0RMLIiIiIiIXtLm+FjrwdsAUKpUKXz44YcFCo6IiIiIiN4+WicWv/zyS77L+/btW+BgiIiIiIioZNK6K5Stra3seUZGBlJTU2FsbAxzc3M8fvxYpwG+SewKRURERET0kjbHx1rPCvXkyRPZIyUlBVFRUWjatCnWrl1b4KCJiIiIiKjk0jqxUKdSpUr45ptvMHLkSF2sjoiIiIiIShidJBbAiwHdd+/e1dXqiIiIiIioBNF68Pa2bdtkz4UQiIuLw6JFi+Dr66uzwIiIiIiIqOTQOrHo0qWL7LlCoYCDgwNatWqF77//XldxERERERFRCaJ1YqFSqYoiDiIiIiIiKsF0NsaCiIiIiIjeXRpdsRgzZozGK5wzZ06BgyEiIiIiopJJo8Ti1KlTGq1MoVAUKhgiIiIiIiqZNEosDh06VNRxEBERERFRCcYxFkREREREVGhazwoFAP/88w/Wr1+PmzdvIj09XbZs8+bNOgmMiIiIiIhKDq2vWKxbtw4+Pj64dOkStmzZgoyMDFy4cAEHDx6EUqksihiJiIiIiKiY0zqxmDFjBubOnYvt27fD2NgY8+fPx+XLl9GjRw+UL1++KGIkIiIiIqJiTuvE4tq1a+jYsSMAwNjYGE+fPoVCocDo0aOxfPlynQdIRERERETFn9aJha2tLZKTkwEAZcuWxfnz5wEACQkJSE1N1W10RERERERUImg9eLt58+YIDw9HzZo1ERQUhJEjR+LgwYMIDw9H69atiyJGIiIiIiIq5jROLM6fP48aNWpg0aJFeP78OQBg4sSJMDIywrFjx9C9e3dMmjSpyAIlIiIiIqLiSyGEEJpUNDAwQIMGDfDJJ5/ggw8+gJWVVVHH9sYlJSVBqVQiMTER1tbW+g6HiIiIiEivtDk+1niMxZ9//onq1avjs88+g4uLC4KDg3H06NFCB0tERERERCWfxolFs2bN8PPPPyMuLg4LFy5EbGws/Pz8ULlyZcyaNQvx8fFFGScRERERERVjWs8KZWFhgf79++PPP//ElStXEBQUhMWLF6N8+fIIDAwsihiJiIiIiKiY03iMRV6ePn2K3377DRMmTEBCQgKysrJ0FdsbxzEWREREREQvaXN8rPV0s9mOHDmCn3/+GZs2bYKBgQF69OiBAQMGFHR1RERERERUgmmVWNy9excrV67EypUrcfXqVfj4+GDBggXo0aMHLCwsiipGIiIiIiIq5jROLNq3b4/9+/fD3t4effv2xccff4wqVaoUZWxERERERFRCaJxYGBkZYePGjejUqRMMDQ2LMiYiIiIiIiphNE4stm3bVpRxEBERERFRCab1dLNERERERESvYmJBRERERESFxsSCiIiIiIgKjYkFEREREREVGhMLIiIiIiIqNCYWRERERERUaCUisYiNjcWAAQPg4eEBMzMzVKxYESEhIUhPT5fVO3v2LJo1awZTU1O4urpi9uzZeoqYiIiIiOjdovF9LPTp8uXLUKlUWLZsGTw9PXH+/HkMHDgQT58+xXfffQcASEpKQtu2bdGmTRssXboU586dw8cffwwbGxt8+umnen4HRERERERvN4UQQug7iIL49ttvsWTJEly/fh0AsGTJEkycOBHx8fEwNjYGAIwfPx5bt27F5cuXNVpnUlISlEolEhMTYW1tXWSxExERERGVBNocH5eIrlDqJCYmws7OTnoeGRmJ5s2bS0kFAAQEBCAqKgpPnjxRu460tDQkJSXJHkREREREpL0SmVhcvXoVCxcuxKBBg6Sy+Ph4ODk5yeplP4+Pj1e7npkzZ0KpVEoPV1fXoguaiIiIiOgtptfEYvz48VAoFPk+Xu3GdOfOHbRr1w5BQUEYOHBgobY/YcIEJCYmSo9bt24Van1ERERERO8qvQ7e/uyzz9CvX79861SoUEH6/927d9GyZUv4+Phg+fLlsnrOzs64d++erCz7ubOzs9p1m5iYwMTEpACRExERERFRTnpNLBwcHODg4KBR3Tt37qBly5bw9vZGWFgYDAzkF1uaNGmCiRMnIiMjA0ZGRgCA8PBwVKlSBba2tjqPnYiIiIiIXioRYyzu3LmDFi1aoHz58vjuu+/w4MEDxMfHy8ZO9O7dG8bGxhgwYAAuXLiA33//HfPnz8eYMWP0GDkRERER0buhRNzHIjw8HFevXsXVq1dRrlw52bLs2XKVSiX27duHoUOHwtvbG/b29pg8eTLvYUFERERE9AaU2PtYFAXex4KIiIiI6KV34j4WRERERERUfDCxICIiIiKiQmNiQUREREREhcbEgoiIiIiICo2JBRERERERFVqJmG6WiIiI8hYX9+KhKy4uLx5ERNpgYkFERFTCLVsGTJmiu/WFhAChobpbHxG9G5hYEBERlXCDBgGBgbpbH69WEFFBMLEgIiIq4dh1iYiKAw7eJiIiIiKiQmNiQUREREREhcauUKRXnMmEiIiI6O3AxIL0ijOZEBEREb0dmFiQXnEmEyIiIqK3AxML0it2XSKigmA3SiKi4oeJBRERlTjsRklEVPwwsSCiYknXZ6QBnpV+m7AbJRFR8cPEgoiKJV2fkQZ4VvptwiSRiKj4YWJBRMWSrs9IAzwQJSJ613F8VtFiYkFUjHCH91JJjp2IiIonjs8qWkwsiIoR7vCIiEiXOF5NjuOzihYTC6JihDs8IiLSJY5XkyvJSVFJwMSCqBjhDo+IiHSJ49XoTWJi8YaxDz0RERG9KTxOoDeJicUbxj70RERERPQ2YmLxhrEPPRERERG9jZhYvGG8JElEREREbyMmFkREJQCnjCTSHMczEukHEwsiohKAU0YSaY7jGYn0QyGEEPoOorhISkqCUqlEYmIirK2t9R0OEZGEVyyINMcrFkS6o83xMa9Y5JCdYyUlJek5EiIiOQsLwNNT9+vl7o7eRkXxe+Fvhd5V2cfFmlyLYGKRQ3JyMgDA1dVVz5EQERERERUfycnJUCqV+dZhV6gcVCoV7t69CysrKygUCr3FkZSUBFdXV9y6dYtdssD2eBXbQ47t8RLbQo7tIcf2kGN7vMS2kGN7yAkhkJycjDJlysDAwCDfurxikYOBgQHKlSun7zAk1tbW/ELnwPaQY3vIsT1eYlvIsT3k2B5ybI+X2BZybI+XXnelIlv+aQcREREREZEGmFgQEREREVGhMbEohkxMTBASEgITExN9h1IssD3k2B5ybI+X2BZybA85tocc2+MltoUc26PgOHibiIiIiIgKjVcsiIiIiIio0JhYEBERERFRoTGxICIiIiKiQmNiUYRCQ0NRp06dfOu0aNECo0aNeiPxUPHm7u6OefPmSc8VCgW2bt2qt3iI3gX9+vVDly5dNK4fGxsLhUKB06dPF1lMhfW6fcfhw4ehUCiQkJDwxmIqaUrC50zqve7Ya+XKlbCxsSnUNrTdb7xLmFjkoFAo8n2EhobqfJubN2/GtGnT8q3zuh3clClT8OGHHwLQz8GoPtqtOOrXr5/0no2NjeHp6YmpU6ciMzNT36EVSznby8jICE5OTvD398fPP/8MlUql7/CKhfj4eAwfPhwVKlSAiYkJXF1d0blzZxw4cEBn23g1odWXBw8eYPDgwShfvjxMTEzg7OyMgIAARERE6Du0YqewbeXj44O4uLjX3vBKXwdP/C4UTs59q0KhQOnSpdGuXTucPXtW36FpJDIyEoaGhujYsaO+Q9G7knjymXfeziEuLk76/++//47JkycjKipKKrO0tNT5Nu3s7PJdnp6e/tp1/PHHHxg/fryuQtKaNu0mhEBWVhZKlSp+X7309HQYGxsXah3t2rVDWFgY0tLSsGvXLgwdOhRGRkaYMGGCjqJ8s3TRJvnJbq+srCzcu3cPe/bswciRI7Fx40Zs27ZN7fckIyMDRkZGRRZTcREbGwtfX1/Y2Njg22+/Rc2aNZGRkYG9e/di6NChuHz5sr5D1Knu3bsjPT0dq1atQoUKFXDv3j0cOHAAjx490ndoxU5h28rY2BjOzs55Ls/KyoJCodBVuFp7W78Lb3Lflb1vBV6coJg0aRI6deqEmzdvvpHtF8ZPP/2E4cOH46effsLdu3dRpkwZfYdE2hCkVlhYmFAqla+td+jQIdGgQQNhbm4ulEql8PHxEbGxsUIIIUJCQkTt2rXFL7/8Itzc3IS1tbXo2bOnSEpKkl7v5+cnRo4cKT13c3MTU6dOFR999JGwsrISwcHBAoDs4efnJ9W/efOmMDY2FomJicLNzU1Wz83NTar3ww8/iAoVKggjIyNRuXJl8csvv8jeBwDxww8/iHbt2glTU1Ph4eEhNmzYUOh2O3TokAAgdu3aJerVqyeMjIzEoUOHxPPnz8Xw4cOFg4ODMDExEb6+vuLEiRN5rkcIIbZs2SJyfmVPnz4tWrRoISwtLYWVlZWoV6+eOHnypLT86NGjomnTpsLU1FSUK1dODB8+XKSkpOTb1oURHBws3nvvPVmZv7+/aNy4ca7PWQgh3nvvPdk23dzcxNy5c6XnAMSWLVuk52fPnhUtW7YUpqamws7OTgwcOFAkJycLIYTYu3evMDExEU+ePJFtY8SIEaJly5bS8zfdJvlR115CCHHgwAEBQKxYsUII8fK72blzZ2Fubi5CQkKEEEJs3bpV1K1bV5iYmAgPDw8RGhoqMjIyhBBCqFQqERISIlxdXYWxsbFwcXERw4cPl7axePFi4enpKUxMTISjo6Po3r17kb3Pgmrfvr0oW7as7PPJlv0537hxQwQGBgoLCwthZWUlgoKCRHx8vFTv6tWrIjAwUDg6OgoLCwtRv359ER4eLi338/PLtX/RhydPnggA4vDhw3nW+f7770WNGjWEubm5KFeunBg8eLD0/Rfi5T5jz549omrVqsLCwkIEBASIu3fvSnUyMzPF6NGjhVKpFHZ2dmLcuHGib9++su/h7t27ha+vr1SnY8eO4urVq9LymJgYAUCcOnVKp22gKU3aKvv306VLF2FmZiY8PT3FH3/8IS3P3i9nf4+y2+6PP/4QXl5ewtDQUO3fnkOHDhXxu9PN+xNCiHPnzol27doJCwsL4ejoKD788EPx4MEDabm2n3NmZqbo37+/qFKlirhx44YQIv99UHac6vZdRU3dvvXo0aMCgLh//74QQojPP/9cVKpUSZiZmQkPDw8xadIkkZ6eLnvNtGnThIODg7C0tBQDBgwQX3zxhahdu3aRxp6cnCwsLS3F5cuXRc+ePcX06dNly7O/u/v37xfe3t7CzMxMNGnSRFy+fFmqk33sle3q1avCw8NDDB06VKhUKrXHF6/7LF+V3cahoaHC3t5eWFlZiUGDBom0tDSpzuuOc4QQ4vDhw6JBgwbC2NhYODs7iy+++ELarrrfYExMjJYt+uYxsciDJolFRkaGUCqVYuzYseLq1avi4sWLYuXKldJOJyQkRFhaWopu3bqJc+fOiSNHjghnZ2fx5ZdfSutQl1hYW1uL7777Tly9elVcvXpVnDhxQvohxcXFiUePHkn1Fy1aJNq2bSuEEOL+/fsCgAgLCxNxcXHSDmTz5s3CyMhILF68WERFRYnvv/9eGBoaioMHD0rrASBKly4tVqxYIaKiosSkSZOEoaGhuHjxYqHaLXsnUKtWLbFv3z5x9epV8ejRIzFixAhRpkwZsWvXLnHhwgURHBwsbG1tpfemSWJRvXp18eGHH4pLly6JK1euiPXr14vTp08LIV7sSCwsLMTcuXPFlStXREREhKhbt67o169fvm1dGOp25oGBgaJevXqFTixSUlKEi4uL9F06cOCA8PDwkF6fmZkpnJycxI8//ii9/tUyfbRJfvJKLIQQonbt2qJ9+/ZCiBft4OjoKH7++Wdx7do1cePGDXHkyBFhbW0tVq5cKa5duyb27dsn3N3dRWhoqBBCiA0bNghra2uxa9cucePGDXH8+HGxfPlyIYQQJ0+eFIaGhmLNmjUiNjZW/Pfff2L+/PlF9j4L4tGjR0KhUIgZM2bkWScrK0vUqVNHNG3aVPzzzz/i77//Ft7e3rITD6dPnxZLly4V586dE1euXBGTJk0Spqam0j7q0aNHoly5cmLq1KkiLi5OxMXFFfVbUysjI0NYWlqKUaNGiefPn6utM3fuXHHw4EERExMjDhw4IKpUqSIGDx4sLQ8LCxNGRkaiTZs24uTJk+Lff/8VXl5eonfv3lKdWbNmCVtbW7Fp0yZx8eJFMWDAAGFlZSX7Hm7cuFFs2rRJREdHi1OnTonOnTuLmjVriqysLCGE/hMLTdoKgChXrpxYs2aNiI6OFiNGjBCWlpbS/lVdYmFkZCR8fHxERESEuHz5skhMTBQ9evQQ7dq1k74bOQ+aivP7e/LkiXBwcBATJkwQly5dEv/995/w9/eXnWTR5nN+/vy56Nq1q6hbt670d/V1+6DsOF/dd70Jr+5bk5OTxaBBg4Snp6f0/qZNmyYiIiJETEyM2LZtm3BychKzZs2SXvPrr78KU1NT8fPPP4uoqCgxZcoUYW1tXeSJxU8//STq168vhBBi+/btomLFikKlUknLs7+7jRo1EocPHxYXLlwQzZo1Ez4+PlKdnInFmTNnhLOzs5g4caK0/NXjC00+y1cFBwcLS0tL0bNnT3H+/HmxY8cO4eDgIDu+e91xzu3bt4W5ubkYMmSIuHTpktiyZYuwt7eXEtCEhATRpEkTMXDgQOk3mJmZWeC2fVOYWORBk8Ti0aNH+Z5ZCQkJEebm5rIrFOPGjRONGjWSnqtLLLp06SJbT35/yPz9/cWiRYuk56+e5RZCCB8fHzFw4EBZWVBQkOjQoYPsdf/73/9kdRo1aiT7w62JvBKLrVu3SmUpKSnCyMhI/Pbbb1JZenq6KFOmjJg9e7ba9QiRO7GwsrISK1euVBvHgAEDxKeffiorO3r0qDAwMBDPnj0TQqhv68LIuTNXqVQiPDxcmJiYiLFjxxY6sVi+fLmwtbWVnb3euXOnMDAwkM5Qjxw5UrRq1Upa/upVDH20SX7ySyx69uwpvLy8hBAv2mHUqFGy5a1bt8510L169Wrh4uIihHhxdrty5cq5zsAJIcSmTZuEtbW17HdZ3Bw/flwAEJs3b86zzr59+4ShoaG4efOmVHbhwgUBINdZsZyqV68uFi5cKD1/9XunLxs3bhS2trbC1NRU+Pj4iAkTJogzZ87kWX/Dhg2idOnS0vOwsDABQJYML168WDg5OUnPXVxcpH2MEC8OYsuVK5fn91AIIR48eCAAiHPnzgkh9J9YCPH6tgIgJk2aJD1PSUkRAMTu3buFEOoTCwDSiZls+f1Gi1Jh39+0adOkE27Zbt26JQCIqKgotdvM63M+evSoaN26tWjatKlISEiQ6r9uH5Qd56v7rjchODhYGBoaCgsLC2FhYSEACBcXF/Hvv//m+Zpvv/1WeHt7S88bNWokhg4dKqvj6+tb5ImFj4+PmDdvnhDixe/T3t5edqUs5xWLbDt37hQApL9j2YlFRESEsLW1Fd99951sG68eX2jyWb4qODhY2NnZiadPn0plS5YsEZaWliIrK0uj45wvv/xSVKlSRZY4LV68WFqHELmPEUsCDt7W0M2bN2FpaSk9ZsyYATs7O/Tr1w8BAQHo3Lkz5s+fLxtvALwYGGllZSU9d3Fxwf379/PdVv369TWKKSkpCX/++ScCAwPzrXfp0iX4+vrKynx9fXHp0iVZWZMmTXI9f7VOQeV8T9euXUNGRoYsJiMjIzRs2FCr7Y0ZMwaffPIJ2rRpg2+++QbXrl2Tlp05cwYrV66UfWYBAQFQqVSIiYlRG5cu7NixA5aWljA1NUX79u3Rs2dPnQxev3TpEmrXrg0LCwupzNfXFyqVShrP0qdPHxw+fBh3794FAPz222/o2LGjNPuFvtqkIIQQsj7er8Z05swZTJ06VfZeBg4ciLi4OKSmpiIoKAjPnj1DhQoVMHDgQGzZskUaRO/v7w83NzdUqFABH330EX777Tekpqa+0ff3OkKI19a5dOkSXF1d4erqKpVVq1YNNjY20u8oJSUFY8eOhZeXF2xsbGBpaYlLly4Vy37W3bt3x927d7Ft2za0a9cOhw8fRr169bBy5UoAwP79+9G6dWuULVsWVlZW+Oijj/Do0SPZZ2dubo6KFStKz3PubxMTExEXF4dGjRpJy0uVKpXruxUdHY1evXqhQoUKsLa2hru7OwAUqzZ7XVsBQK1ataT/W1hYwNraOt+/PcbGxrLX6FNh39+ZM2dw6NAh2f6hatWqACD9ndD0c+7VqxeePn2Kffv2yQa7v24flE1f+9OWLVvi9OnTOH36NE6cOIGAgAC0b98eN27cAPBiPKSvry+cnZ1haWmJSZMmyd57VFQUGjZsKFvnq891LSoqCidOnECvXr0AvPh99uzZEz/99FOuujk/fxcXFwCQfb9v3rwJf39/TJ48GZ999lm+29X0s3xV7dq1YW5uLj1v0qQJUlJScOvWLY2Ocy5duoQmTZrI/tb5+voiJSUFt2/fzjfm4oyJhYbKlCkj/UhPnz6N//3vfwCAsLAwREZGwsfHB7///jsqV66Mv//+W3rdqwO1FArFa2e8yXnwmJ/du3ejWrVqsgOL4krT95TNwMAg18FVRkaG7HloaCguXLiAjh074uDBg6hWrRq2bNkC4MUB1aBBg2Sf2ZkzZxAdHS078NA2rtfJ3plHR0fj2bNnWLVqFSwsLDR6P4XVoEEDVKxYEevWrcOzZ8+wZcsW9OnTR1qurzYpiEuXLsHDw0N6/mpMKSkpmDJliuy9nDt3DtHR0TA1NYWrqyuioqLwww8/wMzMDEOGDEHz5s2RkZEBKysr/Pfff1i7di1cXFwwefJk1K5du1hNvVmpUiUoFIpCD9AeO3YstmzZghkzZuDo0aM4ffo0atasqdGkEPpgamoKf39/fPXVVzh27Bj69euHkJAQxMbGolOnTqhVqxY2bdqEf//9F4sXLwYgn+BC3f5WkyQtp86dO+Px48dYsWIFjh8/juPHj+faTnGQV1tl0/Zvj5mZmV4HbL+qMO8vJSUFnTt3lu0fsvfLzZs3B6D559yhQwecPXsWkZGRsvLX7YOy6Wt/amFhAU9PT3h6eqJBgwb48ccf8fTpU6xYsQKRkZHo06cPOnTogB07duDUqVOYOHGi3r/jP/30EzIzM1GmTBmUKlUKpUqVwpIlS7Bp0yYkJibK6ub8/LO/tzm/3w4ODmjYsCHWrl2LpKSkfLer6WdJmmFioaFSpUpJP1JPT0/ZbE5169bFhAkTcOzYMdSoUQNr1qzR6bazZ+XJysqSlf/xxx947733ZGVGRka56nl5eeWapi8iIgLVqlWTleVMiLKfe3l5FSp2dSpWrAhjY2NZTBkZGTh58qQUk4ODA5KTk/H06VOpjrrpditXrozRo0dj37596NatmzQLRr169XDx4kXZZ5b9KMpZjrJ35uXLl5fNaOTg4CC7mpWVlYXz589rvF4vLy+cOXNG1h4REREwMDBAlSpVpLI+ffrgt99+w/bt22FgYCCbrk9fbaKtgwcP4ty5c+jevXuederVq4eoqCi178XA4MVuzczMDJ07d8aCBQtw+PBhREZG4ty5cwBe/J7btGmD2bNn4+zZs4iNjcXBgwffyPvThJ2dHQICArB48WLZZ54tISEBXl5euHXrFm7duiWVX7x4EQkJCdLvKCIiAv369UPXrl1Rs2ZNODs7IzY2VrYuY2PjXPuM4qJatWp4+vQp/v33X6hUKnz//fdo3LgxKleuLF2Z05RSqYSLi4t0AAkAmZmZ+Pfff6Xnjx49QlRUFCZNmoTWrVvDy8sLT5480dn7KUrZbaVLxem7oc37q1evHi5cuAB3d/dc+wcLCwutPufBgwfjm2++QWBgIP7880/ZNl63DypOFAoFDAwM8OzZMxw7dgxubm6YOHEi6tevj0qVKklXMrJVqVIFJ0+elJW9+lyXMjMz8csvv+D777/PdfKrTJkyWLt2rVbrMzMzw44dO2BqaoqAgAAkJyfnWbegn+WZM2fw7Nkz6fnff/8NS0tLuLq6anSc4+XlhcjISNnJj4iICFhZWaFcuXIAitdvUFPFb87PEiQmJgbLly9HYGAgypQpg6ioKERHR6Nv37463Y6joyPMzMywZ88elCtXDqamprCwsMDu3bsxduxYWV13d3ccOHAAvr6+MDExga2tLcaNG4cePXqgbt26aNOmDbZv347Nmzdj//79stdu2LAB9evXR9OmTfHbb7/hxIkTai9BFpaFhQUGDx6McePGwc7ODuXLl8fs2bORmpqKAQMGAAAaNWoEc3NzfPnllxgxYgSOHz8uuwz+7NkzjBs3Du+//z48PDxw+/ZtnDx5UjoY/eKLL9C4cWMMGzYMn3zyCSwsLHDx4kWEh4dj0aJFOn9Pr9OqVSuMGTMGO3fuRMWKFTFnzhytzpD36dMHISEhCA4ORmhoKB48eIDhw4fjo48+gpOTk6xeaGgopk+fjvfffx8mJibSsuLWJgCQlpaG+Ph42XSzM2fORKdOnfL9HU2ePBmdOnVC+fLl8f7778PAwABnzpzB+fPn8fXXX2PlypXIysqSvke//vorzMzM4Obmhh07duD69eto3rw5bG1tsWvXLqhUKlmCVhwsXrwYvr6+aNiwIaZOnYpatWohMzMT4eHhWLJkCS5evIiaNWuiT58+mDdvHjIzMzFkyBD4+flJ3S8qVaqEzZs3o3PnzlAoFPjqq69ynbV2d3fHkSNH8MEHH8DExAT29vZv/L0+evQIQUFB+Pjjj1GrVi1YWVnhn3/+wezZs/Hee+/B09MTGRkZWLhwITp37oyIiAgsXbpU6+2MHDkS33zzDSpVqoSqVavm+h3a2tqidOnSWL58OVxcXHDz5k29TuWtzuvaSpfc3d2xd+9eREVFoXTp0lAqlUU+Xaou3t/QoUOxYsUK9OrVC59//jns7Oxw9epVrFu3Dj/++KPWn/Pw4cORlZWFTp06Yffu3WjatOlr90H6lr1vBYAnT55g0aJF0pWcpKQk3Lx5E+vWrUODBg2wc+dO6Wp/tuHDh2PgwIGoX7++1CPj7NmzqFChQpHEu2PHDjx58gQDBgzIdX+V7t2746effpJ6imjKwsICO3fuRPv27dG+fXvs2bNH7W0DCvpZpqenY8CAAZg0aRJiY2MREhKCYcOGwcDAQKPjnCFDhmDevHkYPnw4hg0bhqioKISEhGDMmDFSQuPu7o7jx48jNjYWlpaWsLOzK5aJq4w+B3gUZ5oM3o6PjxddunQRLi4uwtjYWLi5uYnJkydLg25enfJMiBczm+ScBlbd4G11AylXrFghXF1dhYGBgfDz8xP79+8X5cqVy1Vv27ZtwtPTU5QqVUrr6WYXL14s/P39hYmJiXB3dxe///57vu9fnbwGb786DeqzZ8/E8OHDhb29fZ7TsG3ZskV4enoKMzMz0alTJ7F8+XJp8HZaWpr44IMPpOlEy5QpI4YNGyYN3hJCiBMnTgh/f39haWkpLCwsRK1atWRT1+l60Gp+Ax3T09PF4MGDhZ2dnXB0dBQzZ87U6XSzOTVs2FAAkM36le1Nt0l+ck6lV6pUKeHg4CDatGkjfv75Z+k3JIT6CQmEEGLPnj3Cx8dHmJmZCWtra9GwYUNp5qctW7aIRo0aCWtra2FhYSEaN24sDfY7evSo8PPzE7a2tsLMzEzUqlWrQN/1N+Hu3bti6NChws3NTRgbG4uyZcuKwMBAaTDj66abjYmJES1bthRmZmbC1dVVLFq0KNc+JzIyUtSqVUuYmJjobbrZ58+fi/Hjx4t69eoJpVIpzM3NRZUqVcSkSZNEamqqEEKIOXPmCBcXF2FmZiYCAgLEL7/8onbK1JxenfAhIyNDjBw5UlhbWwsbGxsxZsyYXNPNhoeHCy8vL2FiYiJq1aolDh8+LPsO6nvwtiZtpe43o1QqRVhYmBAi7+lmX3X//n1pf4E3NN2sLt6fEEJcuXJFdO3aVdjY2AgzMzNRtWpVMWrUKGmgbEE+5++//15YWVmJiIgIIUT++6C84nwTXp2m1MrKSjRo0EBs3LhRqjNu3DhRunRpaWajuXPn5voOTJ06Vdjb2wtLS0vx8ccfixEjRojGjRtLy7O/R7qYArVTp06yCWVyyp7M4syZM2qPKU6dOiWL49Vjr+TkZOHj4yOaN28uUlJS1H7fX/dZvir77/3kyZOldhw4cKBsJjNNjnPym25WCCGioqJE48aNhZmZWYmZblYhhJYdUKlYGDFiBDIzM/HDDz/oZH0KhQJbtmzhLeqJiIgoF39/fzg7O2P16tUAXowxnTFjBi5evPhO3LSUNMOuUCVUjRo1cs3iRERERFRYqampWLp0KQICAmBoaIi1a9di//79CA8Pl+rs2rULM2bMYFJBMrxiQQB4xYKIiIheePbsGTp37oxTp07h+fPnqFKlCiZNmoRu3brpOzQq5phYEBERERFRoRXzoeVERERERFQSMLEgIiIiIqJCY2JBRERERESFxsSCiIiIiIgKjYkFEREREREVGhMLIiIiIiIqNCYWRERERERUaEwsiIiIiIio0JhYEBERERFRof0fDNutZq21MmEAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  2%|▎         | 30/1200 [15:48<9:00:50, 27.74s/it] /home/guillaume/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 34.86 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 39/1200 [20:28<10:30:41, 32.59s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.86 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.72 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.84 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.92 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.73 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.63 %\n",
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 34.46 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  3%|▎         | 40/1200 [21:16<11:56:34, 37.06s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 35.03 %\n",
      "new seed generated with accuracy  tensor(35.0300)\n",
      "New exploration domain with scale  = 0.005 ; noise ! = 0.15\n",
      "EPOCH ACCURACY = 35.03 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▎         | 42/1200 [22:22<11:02:48, 34.34s/it]No artists with labels found to put in legend.  Note that artists whose label start with an underscore are ignored when legend() is called with no argument.\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 800x400 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  4%|▍         | 46/1200 [24:23<10:11:58, 31.82s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[33], line 148\u001b[0m\n\u001b[1;32m    140\u001b[0m x\u001b[38;5;241m=\u001b[39mx\u001b[38;5;241m.\u001b[39mabstract_tensor()\n\u001b[1;32m    145\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 148\u001b[0m     result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1\u001b[38;5;241m=\u001b[39m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabstract_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    149\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mconv1_eps_bias\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv1_eps_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    150\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mconv1_eps_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconv1_eps_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    151\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mconv2_eps_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconv2_eps_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    152\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mconv2_eps_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mconv2_eps_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    153\u001b[0m \n\u001b[1;32m    154\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mfc1_eps_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfc1_eps_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    155\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mfc1_eps_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfc1_eps_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[1;32m    156\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mfc2_eps_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfc2_eps_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    157\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43mfc2_eps_bias\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfc2_eps_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    158\u001b[0m \n\u001b[1;32m    159\u001b[0m \u001b[43m                                                                \u001b[49m\u001b[43madd_symbol\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m    161\u001b[0m result_list\u001b[38;5;241m.\u001b[39mappend([result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1])    \n\u001b[1;32m    162\u001b[0m true_predict_list\u001b[38;5;241m.\u001b[39mappend(torch\u001b[38;5;241m.\u001b[39margmax(x_true))\n",
      "Cell \u001b[0;32mIn[5], line 71\u001b[0m, in \u001b[0;36mAbstractNN.abstract_forward\u001b[0;34m(self, x, conv1_eps_weight, conv1_eps_bias, conv2_eps_weight, conv2_eps_bias, fc1_eps_weight, fc1_eps_bias, fc2_eps_weight, fc2_eps_bias, add_symbol, device)\u001b[0m\n\u001b[1;32m     67\u001b[0m x,x_min,x_max ,x_true \u001b[38;5;241m=\u001b[39m AM\u001b[38;5;241m.\u001b[39mabstract_maxpool2D(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmaxpool,x,x_true,add_symbol\u001b[38;5;241m=\u001b[39madd_symbol,device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     70\u001b[0m symb_conv2 \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(x)\n\u001b[0;32m---> 71\u001b[0m x,x_min,x_max,x_true \u001b[38;5;241m=\u001b[39m \u001b[43mNN\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mabstract_linear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfc1\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43mx_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfc1_eps_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfc1_eps_bias\u001b[49m\u001b[43m,\u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     73\u001b[0m x,x_min,x_max,x_true \u001b[38;5;241m=\u001b[39m AR\u001b[38;5;241m.\u001b[39mabstract_relu(x,x_min,x_max,x_true,add_symbol\u001b[38;5;241m=\u001b[39madd_symbol,device \u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     76\u001b[0m symb_fc1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(x)\n",
      "File \u001b[0;32m~/Documents/ProtoTensor/notebook/../src/abstractNN.py:109\u001b[0m, in \u001b[0;36mAbstractNN.abstract_linear\u001b[0;34m(lin, x, x_true, lin_weights_alphas_index_and_values, lin_bias_alphas_index_and_values, device)\u001b[0m\n\u001b[1;32m    106\u001b[0m add_eps \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mempty(\u001b[38;5;28mlen\u001b[39m(index),lin_weight_shape[\u001b[38;5;241m0\u001b[39m])\n\u001b[1;32m    107\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m indice \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mlen\u001b[39m(index)):\n\u001b[0;32m--> 109\u001b[0m     weight_epsilon\u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin_weight_shape\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mflatten()\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    110\u001b[0m     weight_epsilon[index[indice]] \u001b[38;5;241m=\u001b[39mvalues[indice]\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m    111\u001b[0m     weight_epsilon \u001b[38;5;241m=\u001b[39m weight_epsilon\u001b[38;5;241m.\u001b[39mreshape(lin_weight_shape)\u001b[38;5;241m.\u001b[39mto(device)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from abstract import abstractTensor as AT\n",
    "from common import make_indice_and_values_tupple,plot_dominance\n",
    "abstract_network = True\n",
    "random__image_select = True\n",
    "model = AbstractNN()\n",
    "#model.load_state_dict(torch.load('model_first_step3627.pth'))\n",
    "#model.load_state_dict(torch.load('modelseeds.pth'))\n",
    "\"\"\"\n",
    "Xperience over sparse initialisation\n",
    "for param in model.parameters():\n",
    "    param.data = torch.zeros_like(param.data)\n",
    "for param in model.parameters():\n",
    "    shape = param.data.shape\n",
    "    tensor = torch.zeros(shape)\n",
    "    num_elements = tensor.numel()\n",
    "    num_random_elements = int(0.5 * num_elements)\n",
    "    random_indices = np.random.choice(num_elements, num_random_elements, replace=False)\n",
    "    random_values = 1/num_class*torch.randn(num_random_elements)\n",
    "    multi_dim_indices = np.unravel_index(random_indices, tensor.shape)\n",
    "    tensor[multi_dim_indices] = random_values\n",
    "    param.data =tensor\n",
    "    \"\"\"\n",
    "\n",
    "xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"]\n",
    "\n",
    "img_list = []\n",
    "img_relevance_list =[]\n",
    "conv1_weight_relevance_list = []\n",
    "conv1_bias_relevance_list = []\n",
    "conv2_weight_relevance_list = []\n",
    "conv2_bias_relevance_list = []\n",
    "\n",
    "fc1_weight_relevance_list = []\n",
    "fc1_bias_relevance_list = []\n",
    "fc2_weight_relevance_list = []\n",
    "fc2_bias_relevance_list  = []\n",
    "result_list =[]\n",
    "num_label_list = []\n",
    "name_label_list = []\n",
    "true_predict_list = []\n",
    "\n",
    "best_model_wts = None\n",
    "best_acc    = 0\n",
    "best_epoch_indice = 0\n",
    "\n",
    "scale_init      = 0.005\n",
    "noise_init      = 0.15\n",
    "noise           = noise_init\n",
    "scale           = scale_init\n",
    "epoch_acc       = 0\n",
    "       \n",
    "num_class       = 10\n",
    "batch_size      = 10\n",
    "for i in tqdm(range(1200)):\n",
    "    if i%num_class==0:\n",
    "        eval =CT(model, device = device)\n",
    "        eval.evaluate_model(dataset_test)\n",
    " \n",
    "   \n",
    "   \n",
    "    if abstract_network : \n",
    "    \n",
    "        conv1_eps_weight = torch.tensor([])\n",
    "        conv1__weight_span = torch.max(model.conv1.weight.data.flatten())-torch.min(model.conv1.weight.data.flatten())\n",
    "        conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "        conv1_eps_weight.values = scale*torch.ones_like(conv1_eps_weight.indices)\n",
    "        conv1_eps_bias = torch.tensor([])\n",
    "        conv1__bias_span = torch.max(model.conv1.bias.data.flatten())-torch.min(model.conv1.bias.data.flatten())\n",
    "        conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "        conv1_eps_bias.values = scale*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        conv2_eps_weight = torch.tensor([])\n",
    "        conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "        conv2_weight_span = torch.max(model.conv2.weight.data.flatten())-torch.min(model.conv2.weight.data.flatten())\n",
    "        conv2_eps_weight.values = scale*torch.ones_like(conv2_eps_weight.indices)\n",
    "        conv2_eps_bias = torch.tensor([])\n",
    "        conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "        conv2_bias_span = torch.max(model.conv2.bias.data.flatten())-torch.min(model.conv2.bias.data.flatten())\n",
    "        conv2_eps_bias.values = scale*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "        fc1_eps_weight = torch.tensor([])\n",
    "        fc1_weight_span = torch.max(model.fc1[1].weight.data.flatten())-torch.min(model.fc1[1].weight.data.flatten())\n",
    "        fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "        fc1_eps_weight.values = scale*torch.ones_like(fc1_eps_weight.indices)\n",
    "        fc1_eps_bias = torch.tensor([])\n",
    "        fc1_bias_span = torch.max(model.fc1[1].bias.data.flatten())-torch.min(model.fc1[1].bias.data.flatten())\n",
    "        fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "        fc1_eps_bias.values = scale*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        fc2_eps_weight = torch.tensor([])\n",
    "        fc2_weight_span = torch.max(model.fc2[1].weight.data.flatten())-torch.min(model.fc2[1].weight.data.flatten())\n",
    "        fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "        fc2_eps_weight.values = scale*fc2_weight_span*torch.ones_like(fc2_eps_weight.indices)\n",
    "        fc2_eps_bias = torch.tensor([])\n",
    "        fc2_bias_span = torch.max(model.fc2[1].bias.data.flatten())-torch.min(model.fc2[1].bias.data.flatten())\n",
    "        fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "        fc2_eps_bias.values = scale*torch.ones_like(fc2_eps_bias.indices)\n",
    "\n",
    "\n",
    "    else :\n",
    "        conv1_eps_weight = make_indice_and_values_tupple()\n",
    "        conv1_eps_bias = make_indice_and_values_tupple()\n",
    "       \n",
    "\n",
    "        conv2_eps_weight = make_indice_and_values_tupple()\n",
    "        conv2_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_bias = make_indice_and_values_tupple()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label = None\n",
    "\n",
    "    if random__image_select:\n",
    "        while label!= i%num_class: \n",
    "            index = np.random.randint(0,5000)\n",
    "            \n",
    "            img, label = dataset_train[index]\n",
    "    else :\n",
    "        index = 0\n",
    "        while label!= i%num_class:\n",
    "            img, label = dataset_train[index]\n",
    "            index+=1\n",
    "\n",
    "    name_label_list.append(xticklabels[label])\n",
    "    num_label_list.append(label)\n",
    "   \n",
    "    img_list.append(img)\n",
    "    \n",
    "    x=AT(img,alpha =noise*torch.ones(28*28))\n",
    "    x=x.abstract_tensor()\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        \n",
    "        result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1=model.abstract_forward(x,\n",
    "                                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "\n",
    "                                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                                    add_symbol=True)\n",
    "        \n",
    "    result_list.append([result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1])    \n",
    "    true_predict_list.append(torch.argmax(x_true))\n",
    "    random_plot = np.random.randint(num_class)\n",
    "    if random_plot==1:\n",
    "        plot_dominance(result,x_min,x_max,x_true)\n",
    "\n",
    "\n",
    "\n",
    "    concatenated_heatmap_tensors =[]\n",
    "    for j in range(num_class):\n",
    "        concat_part =result[1:785,j]\n",
    "        concatenated_heatmap_tensors.append(concat_part)\n",
    "    concatenated_heatmap_tensors = torch.stack(concatenated_heatmap_tensors)\n",
    "\n",
    "    img_relevance_list.append(concatenated_heatmap_tensors)\n",
    "\n",
    "  \n",
    "    if abstract_network:\n",
    "\n",
    "        conv1_weight_concatenated_tensors = []\n",
    "        conv1_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len(x)-1:len(x)+143, j].view(16, 1, 3, 3)\n",
    "            conv1_weight_concatenated_tensors.append(concat_part)\n",
    "        conv1_weight_concatenated_tensors =torch.stack(conv1_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part= result[len(x)+143:len(x)+159, j]\n",
    "            conv1_bias_concatenated_tensors .append(concat_part)\n",
    "        conv1_bias_concatenated_tensors = torch.stack(conv1_bias_concatenated_tensors )\n",
    "\n",
    "        conv2_weight_concatenated_tensors = []\n",
    "        conv2_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb-1:len_symb+4607, j].view(32, 16, 3, 3)\n",
    "            conv2_weight_concatenated_tensors.append(concat_part)\n",
    "        conv2_weight_concatenated_tensors =torch.stack(conv2_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb+4607:len_symb+4639, j]\n",
    "            conv2_bias_concatenated_tensors.append(concat_part)\n",
    "        conv2_bias_concatenated_tensors = torch.stack(conv2_bias_concatenated_tensors)\n",
    "\n",
    "        fc1_weight_concatenated_tensors = []\n",
    "        fc1_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_c2-1:len_symb_c2+294911, j].reshape(64,4608)\n",
    "            fc1_weight_concatenated_tensors.append(concat_part)\n",
    "        fc1_weight_concatenated_tensors =torch.stack(fc1_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_c2+294911:len_symb_c2+294975, j]\n",
    "            fc1_bias_concatenated_tensors.append(concat_part)\n",
    "        fc1_bias_concatenated_tensors= torch.stack(fc1_bias_concatenated_tensors)\n",
    "\n",
    "        fc2_weight_concatenated_tensors = []\n",
    "        fc2_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_fc1-1:len_symb_fc1+639, j].view(num_class,64)\n",
    "            fc2_weight_concatenated_tensors.append(concat_part)\n",
    "        fc2_weight_concatenated_tensors =torch.stack(fc2_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_fc1+639:len_symb_fc1+649, j]\n",
    "            fc2_bias_concatenated_tensors.append(concat_part)\n",
    "        fc2_bias_concatenated_tensors= torch.stack(fc2_bias_concatenated_tensors)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    conv1_weight_relevance_list.append(conv1_weight_concatenated_tensors)\n",
    "    conv1_bias_relevance_list.append(conv1_bias_concatenated_tensors)\n",
    "    conv2_weight_relevance_list.append(conv2_weight_concatenated_tensors)\n",
    "\n",
    "    conv2_bias_relevance_list.append(conv2_bias_concatenated_tensors)\n",
    "\n",
    "    fc1_weight_relevance_list.append(fc1_weight_concatenated_tensors)\n",
    "    fc1_bias_relevance_list.append(fc1_bias_concatenated_tensors)\n",
    "    fc2_weight_relevance_list.append(fc2_weight_concatenated_tensors)\n",
    "    fc2_bias_relevance_list.append(fc2_bias_concatenated_tensors)\n",
    "\n",
    "  \n",
    "    k_plus = 0.9*num_class/batch_size\n",
    "\n",
    "    k_min = k_plus/9\n",
    "\n",
    "    \n",
    "   \n",
    "    if ((i+1)%batch_size)==0 :\n",
    "        while epoch_acc>= best_acc*0.99:\n",
    "            print(f\"Update parameter after EPOCH\")\n",
    "            for k in range (num_class):\n",
    "                model.conv1.weight.data =model.conv1.weight.data + k_plus*(conv1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv1.bias.data = model.conv1.bias.data + k_plus*(conv1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.conv2.weight.data =model.conv2.weight.data + k_plus*(conv2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv2.bias.data = model.conv2.bias.data + k_plus*(conv2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.fc1[1].weight.data =model.fc1[1].weight.data + k_plus*(fc1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(fc1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.fc2[1].weight.data =model.fc2[1].weight.data + k_plus*(fc2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(fc2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.no\n",
    "          \n",
    "            from custom_train import CustomTrainer as CT\n",
    "            eval =CT(model, device = device)\n",
    "            epoch_acc = eval.evaluate_model(dataset_test)\n",
    "            if  epoch_acc > best_acc:\n",
    "                        best_acc = epoch_acc\n",
    "                        best_epoch_indice = i\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                        \n",
    "       \n",
    "        \n",
    "        model.load_state_dict(best_model_wts)\n",
    "        model2 = AbstractNN()\n",
    "        for param,param2 in zip(model.parameters(),model2.parameters()):\n",
    "            param.data += 0.01*param2.data\n",
    "\n",
    "        best_acc =CT(model,device = device).evaluate_model(dataset_test)\n",
    "        epoch_acc=best_acc\n",
    "        print(\"new seed generated with accuracy \", best_acc)\n",
    "        if ((i+1)%batch_size==0):\n",
    "            scale = scale_init/(int((i+1)/batch_size))\n",
    "            noise = noise_init/(int((i+1)/batch_size))\n",
    "            print(f'New exploration domain with scale  = {scale} ; noise ! = {noise}')\n",
    "        batch_size+=batch_size\n",
    "        \n",
    "        \n",
    "    \n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]/home/guillaume/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 9.39 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  1%|          | 7/1200 [03:29<10:00:51, 30.22s/it]"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import copy\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from abstract import abstractTensor as AT\n",
    "from common import make_indice_and_values_tupple,plot_dominance\n",
    "abstract_network = True\n",
    "random__image_select = True\n",
    "model = AbstractNN()\n",
    "#model.load_state_dict(torch.load('model_first_step3627.pth'))\n",
    "#model.load_state_dict(torch.load('modelseeds.pth'))\n",
    "\"\"\"\n",
    "for param in model.parameters():\n",
    "    param.data = torch.zeros_like(param.data)\n",
    "\n",
    "for param in model.parameters():\n",
    "    shape = param.data.shape\n",
    "    tensor = torch.zeros(shape)\n",
    "    num_elements = tensor.numel()\n",
    "    num_random_elements = int(0.5 * num_elements)\n",
    "    random_indices = np.random.choice(num_elements, num_random_elements, replace=False)\n",
    "    random_values = 1/num_class*torch.randn(num_random_elements)\n",
    "    multi_dim_indices = np.unravel_index(random_indices, tensor.shape)\n",
    "    tensor[multi_dim_indices] = random_values\n",
    "    param.data =tensor\n",
    "    \n",
    "\"\"\"\n",
    "xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"]\n",
    "\n",
    "img_list = []\n",
    "img_relevance_list =[]\n",
    "conv1_weight_relevance_list = []\n",
    "conv1_bias_relevance_list = []\n",
    "conv2_weight_relevance_list = []\n",
    "conv2_bias_relevance_list = []\n",
    "\n",
    "fc1_weight_relevance_list = []\n",
    "fc1_bias_relevance_list = []\n",
    "fc2_weight_relevance_list = []\n",
    "fc2_bias_relevance_list  = []\n",
    "result_list =[]\n",
    "num_label_list = []\n",
    "name_label_list = []\n",
    "true_predict_list = []\n",
    "\n",
    "best_model_wts = None\n",
    "best_acc    = 0\n",
    "best_epoch_indice = 0\n",
    "\n",
    "scale_init      = 0.01\n",
    "noise_init      = 0.15\n",
    "noise           = noise_init\n",
    "scale           = scale_init\n",
    "epoch_acc       = 0\n",
    "       \n",
    "num_class       = 10\n",
    "batch_size      = 10\n",
    "for i in tqdm(range(1200)):\n",
    "    if i%num_class==0:\n",
    "        eval =CT(model, device = device)\n",
    "        eval.evaluate_model(dataset_test)\n",
    " \n",
    "   \n",
    "   \n",
    "    if abstract_network : \n",
    "    \n",
    "        conv1_eps_weight = torch.tensor([])\n",
    "        conv1__weight_span = torch.max(model.conv1.weight.data.flatten())-torch.min(model.conv1.weight.data.flatten())\n",
    "        conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "        conv1_eps_weight.values = scale*torch.ones_like(conv1_eps_weight.indices)\n",
    "        conv1_eps_bias = torch.tensor([])\n",
    "        conv1__bias_span = torch.max(model.conv1.bias.data.flatten())-torch.min(model.conv1.bias.data.flatten())\n",
    "        conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "        conv1_eps_bias.values = scale*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        conv2_eps_weight = torch.tensor([])\n",
    "        conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "        conv2_weight_span = torch.max(model.conv2.weight.data.flatten())-torch.min(model.conv2.weight.data.flatten())\n",
    "        conv2_eps_weight.values = scale*torch.ones_like(conv2_eps_weight.indices)\n",
    "        conv2_eps_bias = torch.tensor([])\n",
    "        conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "        conv2_bias_span = torch.max(model.conv2.bias.data.flatten())-torch.min(model.conv2.bias.data.flatten())\n",
    "        conv2_eps_bias.values = scale*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "        fc1_eps_weight = torch.tensor([])\n",
    "        fc1_weight_span = torch.max(model.fc1[1].weight.data.flatten())-torch.min(model.fc1[1].weight.data.flatten())\n",
    "        fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "        fc1_eps_weight.values = scale*torch.ones_like(fc1_eps_weight.indices)\n",
    "        fc1_eps_bias = torch.tensor([])\n",
    "        fc1_bias_span = torch.max(model.fc1[1].bias.data.flatten())-torch.min(model.fc1[1].bias.data.flatten())\n",
    "        fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "        fc1_eps_bias.values = scale*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        fc2_eps_weight = torch.tensor([])\n",
    "        fc2_weight_span = torch.max(model.fc2[1].weight.data.flatten())-torch.min(model.fc2[1].weight.data.flatten())\n",
    "        fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "        fc2_eps_weight.values = scale*fc2_weight_span*torch.ones_like(fc2_eps_weight.indices)\n",
    "        fc2_eps_bias = torch.tensor([])\n",
    "        fc2_bias_span = torch.max(model.fc2[1].bias.data.flatten())-torch.min(model.fc2[1].bias.data.flatten())\n",
    "        fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "        fc2_eps_bias.values = scale*torch.ones_like(fc2_eps_bias.indices)\n",
    "\n",
    "\n",
    "    else :\n",
    "        conv1_eps_weight = make_indice_and_values_tupple()\n",
    "        conv1_eps_bias = make_indice_and_values_tupple()\n",
    "       \n",
    "\n",
    "        conv2_eps_weight = make_indice_and_values_tupple()\n",
    "        conv2_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_bias = make_indice_and_values_tupple()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label = None\n",
    "\n",
    "    if random__image_select:\n",
    "        while label!= i%num_class: \n",
    "            index = np.random.randint(0,5000)\n",
    "            \n",
    "            img, label = dataset_train[index]\n",
    "    else :\n",
    "        index = 0\n",
    "        while label!= i%num_class:\n",
    "            img, label = dataset_train[index]\n",
    "            index+=1\n",
    "\n",
    "    #name_label_list.append(xticklabels[label])\n",
    "    #num_label_list.append(label)\n",
    "   \n",
    "    #img_list.append(img)\n",
    "    \n",
    "    x=AT(img,alpha =noise*torch.ones(28*28))\n",
    "    x=x.abstract_tensor()\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        \n",
    "        result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1=model.abstract_forward(x,\n",
    "                                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "\n",
    "                                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                                    add_symbol=True)\n",
    "        \n",
    "    #result_list.append([result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1])    \n",
    "    #true_predict_list.append(torch.argmax(x_true))\n",
    "    random_plot = np.random.randint(num_class)\n",
    "    if random_plot==1:\n",
    "        plot_dominance(result,x_min,x_max,x_true)\n",
    "\n",
    "\n",
    "\n",
    "    concatenated_heatmap_tensors =[]\n",
    "    for j in range(num_class):\n",
    "        concat_part =result[1:785,j]\n",
    "        concatenated_heatmap_tensors.append(concat_part)\n",
    "    concatenated_heatmap_tensors = torch.stack(concatenated_heatmap_tensors)\n",
    "\n",
    "   # img_relevance_list.append(concatenated_heatmap_tensors)\n",
    "\n",
    "  \n",
    "    if abstract_network:\n",
    "\n",
    "        conv1_weight_concatenated_tensors = []\n",
    "        conv1_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len(x)-1:len(x)+143, j].view(16, 1, 3, 3)\n",
    "            conv1_weight_concatenated_tensors.append(concat_part)\n",
    "        conv1_weight_concatenated_tensors =torch.stack(conv1_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part= result[len(x)+143:len(x)+159, j]\n",
    "            conv1_bias_concatenated_tensors .append(concat_part)\n",
    "        conv1_bias_concatenated_tensors = torch.stack(conv1_bias_concatenated_tensors )\n",
    "\n",
    "        conv2_weight_concatenated_tensors = []\n",
    "        conv2_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb-1:len_symb+4607, j].view(32, 16, 3, 3)\n",
    "            conv2_weight_concatenated_tensors.append(concat_part)\n",
    "        conv2_weight_concatenated_tensors =torch.stack(conv2_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb+4607:len_symb+4639, j]\n",
    "            conv2_bias_concatenated_tensors.append(concat_part)\n",
    "        conv2_bias_concatenated_tensors = torch.stack(conv2_bias_concatenated_tensors)\n",
    "\n",
    "        fc1_weight_concatenated_tensors = []\n",
    "        fc1_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_c2-1:len_symb_c2+294911, j].reshape(64,4608)\n",
    "            fc1_weight_concatenated_tensors.append(concat_part)\n",
    "        fc1_weight_concatenated_tensors =torch.stack(fc1_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_c2+294911:len_symb_c2+294975, j]\n",
    "            fc1_bias_concatenated_tensors.append(concat_part)\n",
    "        fc1_bias_concatenated_tensors= torch.stack(fc1_bias_concatenated_tensors)\n",
    "\n",
    "        fc2_weight_concatenated_tensors = []\n",
    "        fc2_bias_concatenated_tensors =[]\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_fc1-1:len_symb_fc1+639, j].view(num_class,64)\n",
    "            fc2_weight_concatenated_tensors.append(concat_part)\n",
    "        fc2_weight_concatenated_tensors =torch.stack(fc2_weight_concatenated_tensors)\n",
    "        for j in range(num_class):\n",
    "            concat_part = result[len_symb_fc1+639:len_symb_fc1+649, j]\n",
    "            fc2_bias_concatenated_tensors.append(concat_part)\n",
    "        fc2_bias_concatenated_tensors= torch.stack(fc2_bias_concatenated_tensors)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    conv1_weight_relevance_list.append(conv1_weight_concatenated_tensors)\n",
    "    conv1_bias_relevance_list.append(conv1_bias_concatenated_tensors)\n",
    "    conv2_weight_relevance_list.append(conv2_weight_concatenated_tensors)\n",
    "\n",
    "    conv2_bias_relevance_list.append(conv2_bias_concatenated_tensors)\n",
    "\n",
    "    fc1_weight_relevance_list.append(fc1_weight_concatenated_tensors)\n",
    "    fc1_bias_relevance_list.append(fc1_bias_concatenated_tensors)\n",
    "    fc2_weight_relevance_list.append(fc2_weight_concatenated_tensors)\n",
    "    fc2_bias_relevance_list.append(fc2_bias_concatenated_tensors)\n",
    "\n",
    "  \n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "  \n",
    "    if ((i+1)%batch_size)==0 :\n",
    "        max_iter = 20\n",
    "        iter = 0\n",
    "        eval =CT(model, device = device)\n",
    "        epoch_acc = eval.evaluate_model(dataset_test)\n",
    "        best_acc = epoch_acc\n",
    "        while (iter<10) or ((epoch_acc>= best_acc*0.99)and(iter<max_iter)):\n",
    "            iter=+1\n",
    "            print(f\"Epoch acc = {epoch_acc}\")\n",
    "            print(f\"best_acc={best_acc}\")\n",
    "            print(f'iter={iter}')\n",
    "            \n",
    "            print(f\"Update parameter after EPOCH\")\n",
    "            conv1_w_sgn = 0\n",
    "            conv1_b_sgn = 0\n",
    "            conv2_w_sgn = 0\n",
    "            conv2_b_sgn = 0\n",
    "            fc1_w_sgn   = 0\n",
    "            fc1_b_sgn   = 0\n",
    "            fc2_w_sgn   = 0\n",
    "            fc2_b_sgn   = 0\n",
    "            for k in range(num_class):\n",
    "\n",
    "                conv1_w_sgn += k_plus*(conv1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                conv1_b_sgn += k_plus*(conv1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                conv2_w_sgn += k_plus*(conv2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                conv2_b_sgn += k_plus*(conv2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                fc1_w_sgn   += k_plus*(fc1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                fc1_b_sgn   += k_plus*(fc1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                fc2_w_sgn   += k_plus*(fc2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                fc2_b_sgn   += k_plus*(fc2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.no\n",
    "\n",
    "            model.conv1.weight.data    += torch.sign(conv1_w_sgn)*scale*0.1\n",
    "            model.conv2.weight.data    += torch.sign(conv2_w_sgn)*scale*0.1\n",
    "            model.conv1.bias.data      += torch.sign(conv1_b_sgn)*scale*0.1\n",
    "            model.conv2.bias.data      += torch.sign(conv2_b_sgn)*scale*0.1\n",
    "            model.fc1[1].weight.data   += torch.sign(fc1_w_sgn)*scale*0.1\n",
    "            model.fc1[1].bias.data     += torch.sign(fc1_b_sgn)*scale*0.1\n",
    "            model.fc2[1].weight.data   += torch.sign(fc2_w_sgn)*scale*0.1\n",
    "            model.fc2[1].bias.data     += torch.sign(fc2_b_sgn)*scale*0.1\n",
    "\n",
    "\n",
    "            from custom_train import CustomTrainer as CT\n",
    "\n",
    "            \n",
    "         \n",
    "         \n",
    "            eval =CT(model, device = device)\n",
    "            epoch_acc = eval.evaluate_model(dataset_test)\n",
    "            if  epoch_acc >= best_acc:\n",
    "                        best_acc = epoch_acc\n",
    "                        best_epoch_indice = i\n",
    "                        best_model_wts = copy.deepcopy(model.state_dict())\n",
    "        \n",
    "        model.load_state_dict(best_model_wts) \n",
    "        #Robin Monroe condition for convergence///\n",
    "        scale = scale_init/(int((i+1)/batch_size))\n",
    "                \n",
    "                \n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "EPOCH ACCURACY = 23.55 %\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(23.5500)"
      ]
     },
     "execution_count": 126,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(23.5500)"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(10.)"
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epoch_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.])"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1[1].bias.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_ = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 125,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(best_model_wts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Update parameter after EPOCH\n",
      "EPOCH ACCURACY = 19.54 %\n"
     ]
    }
   ],
   "source": [
    "k_plus=1\n",
    "k_min = 1/9\n",
    "\n",
    "print(f\"Update parameter after EPOCH\")\n",
    "conv1_w_sgn = torch.zeros_like(model.conv1.weight.data)\n",
    "conv1_b_sgn = torch.zeros_like(model.conv1.bias.data)\n",
    "conv2_w_sgn = torch.zeros_like(model.conv2.weight.data)\n",
    "conv2_b_sgn = torch.zeros_like(model.conv2.bias.data)\n",
    "fc1_w_sgn   = 0\n",
    "fc1_b_sgn   = 0\n",
    "fc2_w_sgn   = 0\n",
    "fc2_b_sgn   = 0\n",
    "for k in range (num_class):\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    conv1_w_sgn += k_plus*(conv1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "    conv1_b_sgn += k_plus*(conv1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "    conv2_w_sgn += k_plus*(conv2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(conv2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "    conv2_b_sgn += k_plus*(conv2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(conv2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(conv2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "    fc1_w_sgn   += k_plus*(fc1_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc1_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "    fc1_b_sgn   += k_plus*(fc1_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc1_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc1_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "    fc2_w_sgn   += k_plus*(fc2_weight_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_weight_relevance_list[-num_class+k][:k%num_class],dim =0)+torch.sum(fc2_weight_relevance_list[-num_class+k][(k+1)%num_class:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "    fc2_b_sgn   += k_plus*(fc2_bias_relevance_list[-batch_size+k][k%num_class])-k_min*(torch.sum(fc2_bias_relevance_list[-num_class+k][:k%num_class],dim = 0)+torch.sum(fc2_bias_relevance_list[-num_class+k][(k+1)%num_class:],dim = 0))#/torch.no\n",
    "\n",
    "model.conv1.weight.data += torch.sign(conv1_w_sgn)*scale\n",
    "model.conv2.weight.data += torch.sign(conv2_w_sgn)*scale\n",
    "model.conv1.bias.data += torch.sign(conv1_b_sgn)*scale\n",
    "model.conv2.bias.data += torch.sign(conv2_b_sgn)*scale\n",
    "model.fc1[1].weight.data += torch.sign(fc1_w_sgn)*scale\n",
    "model.fc1[1].bias.data += torch.sign(fc1_b_sgn)*scale\n",
    "model.fc2[1].weight.data += torch.sign(fc2_w_sgn)*scale\n",
    "model.fc2[1].bias.data += torch.sign(fc2_b_sgn)*scale\n",
    "\n",
    "\n",
    "from custom_train import CustomTrainer as CT\n",
    "best_acc = epoch_acc\n",
    "best_epoch_indice = i\n",
    "best_model_wts = copy.deepcopy(model.state_dict())\n",
    "eval =CT(model, device = device)\n",
    "epoch_acc = eval.evaluate_model(dataset_test)\n",
    "if  epoch_acc > best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_epoch_indice = i\n",
    "            best_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-1.,  1., -1.,  1.,  1.,  1., -1.,  1., -1., -1., -1., -1.,  1., -1.,\n",
       "        -1., -1., -1.,  1., -1.,  1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,\n",
       "        -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1., -1.,  1., -1., -1.,  1.,\n",
       "        -1., -1., -1., -1.,  1.,  1., -1.,  1.,  1.,  1.,  1.,  1.,  1., -1.,\n",
       "         1., -1.,  1.,  1.,  1.,  1., -1.,  1.])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sign(fc1_b_sgn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice A optimale :\n",
      "[ 1.12459623e-05  1.06451370e-05  7.02759216e-06 ...  3.98898875e-06\n",
      "  1.02558290e-05 -9.86563286e-06]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import cvxpy as cp\n",
    "import numpy as np\n",
    "\n",
    "# Supposons que U soit une matrice de taille m x n\n",
    "U = np.random.randn(10,150351)\n",
    "\n",
    "# La taille de A est alors n x m pour obtenir une identité m x m\n",
    "n, m = U.shape\n",
    "\n",
    "# Variable de décision A de taille n x m\n",
    "A = cp.Variable((m))\n",
    "\n",
    "# Matrice identité\n",
    "I = np.array([1,0,0,0,0,0,0,0,0,0])\n",
    "\n",
    "# Formulation du problème\n",
    "objective = cp.Minimize(cp.norm(U @ A - I, 'fro'))\n",
    "constraints = [A >= -1, A <= 1]\n",
    "\n",
    "# Problème d'optimisation\n",
    "prob = cp.Problem(objective, constraints)\n",
    "\n",
    "# Résolution\n",
    "prob.solve()\n",
    "\n",
    "# Solution obtenue\n",
    "A_optimal = A.value\n",
    "\n",
    "print(\"Matrice A optimale :\")\n",
    "print(A_optimal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur A optimal :\n",
      "[2.20840214e-14 1.96820332e-13 2.23876296e-13 7.33355413e-14\n",
      " 4.76504784e-01 5.53325036e-02 1.81365030e-01 2.29596393e-01\n",
      " 1.12329017e-01 3.11546484e-13 7.65924482e-01 4.37838328e-14\n",
      " 4.39138644e-01 5.52432496e-01 6.31565148e-01 5.56923389e-14\n",
      " 1.59283398e-01 2.44915075e-13 6.60452670e-03 4.65751046e-01\n",
      " 7.15717579e-01 5.48297539e-01 1.17798115e-01 1.65231576e-13\n",
      " 5.18459070e-01 8.02955443e-01 2.43743628e-01 5.22837372e-14\n",
      " 4.39152853e-13 6.37415927e-02 1.45790562e-01 1.38974663e-13\n",
      " 3.86920379e-14 3.82309502e-01 2.04524779e-13 1.52641581e-13\n",
      " 1.20926885e-14 8.25609191e-14 3.52002691e-01 3.02226548e-13\n",
      " 2.23495431e-13 2.33260045e-01 3.12394403e-01 1.19308135e-01\n",
      " 9.81851510e-14 7.53498267e-02 3.84870892e-14 3.83585870e-01\n",
      " 5.18329088e-14 1.05010101e-13 6.55445725e-14 2.40088551e-13\n",
      " 4.24378976e-01 2.00859904e-01 4.57955997e-01 9.63897858e-02\n",
      " 2.92786169e-13 1.38015911e-01 1.05639214e-13 3.95704075e-15\n",
      " 5.84338369e-01 1.35545629e-01 1.82804820e-14 3.11335229e-13\n",
      " 3.21928730e-01 7.40131106e-14 2.02934139e-13 4.08724135e-01\n",
      " 1.27059122e-13 1.31942458e-13 4.36361286e-01 1.81534069e-13\n",
      " 3.34107337e-14 6.29092561e-14 6.66656398e-01 2.02777508e-13\n",
      " 1.61477784e-13 1.37435218e-01 2.88974681e-13 4.90720370e-14\n",
      " 2.57669945e-13 3.79872249e-13 5.04637183e-01 1.38620967e-13\n",
      " 3.44830461e-01 5.21385951e-01 4.89999568e-15 7.35713373e-14\n",
      " 5.25150196e-01 2.52542552e-13 2.70750298e-01 1.10837967e-13\n",
      " 3.64971712e-01 9.58396173e-14 4.49494430e-01 5.70593974e-01\n",
      " 1.42194050e-13 9.31727536e-14 5.12703044e-01 1.15979655e-13]\n",
      "\n",
      "Projection de U_1 * A :\n",
      "[ 2.9054801   2.19027123 -2.53231731 -1.08620389  0.7115937   3.09912753\n",
      "  1.21704037  0.35594122 -0.14663512  0.49563281]\n",
      "Vecteur cible :\n",
      "[10.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[-7.0945199   2.19027123 -2.53231731 -1.08620389  0.7115937   3.09912753\n",
      "  1.21704037  0.35594122 -0.14663512  0.49563281]\n",
      "\n",
      "Projection de U_2 * A :\n",
      "[ 0.16880772  5.57856414 -0.462358   -0.10552814 -0.61573274  1.36793087\n",
      " -2.11180962  0.32951514  3.19665616 -2.52338409]\n",
      "Vecteur cible :\n",
      "[ 0. 10.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[ 0.16880772 -4.42143586 -0.462358   -0.10552814 -0.61573274  1.36793087\n",
      " -2.11180962  0.32951514  3.19665616 -2.52338409]\n",
      "\n",
      "Projection de U_3 * A :\n",
      "[-0.12816491 -0.17687983  5.39715088 -1.21374396 -1.55392804  0.67593099\n",
      " -0.13255765  2.78150734  0.23733136  2.96219068]\n",
      "Vecteur cible :\n",
      "[ 0.  0. 10.  0.  0.  0.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[-0.12816491 -0.17687983 -4.60284912 -1.21374396 -1.55392804  0.67593099\n",
      " -0.13255765  2.78150734  0.23733136  2.96219068]\n",
      "\n",
      "Projection de U_4 * A :\n",
      "[-0.22354389  1.38641738  4.21397987  3.22502284  0.35942877 -1.87859201\n",
      "  0.53575118  2.0142705   1.40470897  0.22501307]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0. 10.  0.  0.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[-0.22354389  1.38641738  4.21397987 -6.77497716  0.35942877 -1.87859201\n",
      "  0.53575118  2.0142705   1.40470897  0.22501307]\n",
      "\n",
      "Projection de U_5 * A :\n",
      "[-1.34555478  1.0101751   1.90886779 -2.58924289  2.83638582 -0.32758319\n",
      " -0.15307177  1.70897904 -2.20927749 -0.74843073]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0. 10.  0.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[-1.34555478  1.0101751   1.90886779 -2.58924289 -7.16361418 -0.32758319\n",
      " -0.15307177  1.70897904 -2.20927749 -0.74843073]\n",
      "\n",
      "Projection de U_6 * A :\n",
      "[-2.35395406  3.39200302  1.22472766 -0.35560577  2.3842597   3.25667235\n",
      "  0.76812424  0.08904717  0.30191413  0.67506394]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0.  0. 10.  0.  0.  0.  0.]\n",
      "Différence :\n",
      "[-2.35395406  3.39200302  1.22472766 -0.35560577  2.3842597  -6.74332765\n",
      "  0.76812424  0.08904717  0.30191413  0.67506394]\n",
      "\n",
      "Projection de U_7 * A :\n",
      "[ 0.43729005  0.08489996 -0.87906731 -0.2948323   2.97640982 -3.45876079\n",
      "  1.76460233  2.35995392 -0.17411794 -2.01322989]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0.  0.  0. 10.  0.  0.  0.]\n",
      "Différence :\n",
      "[ 0.43729005  0.08489996 -0.87906731 -0.2948323   2.97640982 -3.45876079\n",
      " -8.23539767  2.35995392 -0.17411794 -2.01322989]\n",
      "\n",
      "Projection de U_8 * A :\n",
      "[ 1.49423924  1.30103879  0.34014389  1.76188568  0.55870709  2.96795854\n",
      " -0.59122827  3.73900125 -3.38754944  2.55513859]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0.  0.  0.  0. 10.  0.  0.]\n",
      "Différence :\n",
      "[ 1.49423924  1.30103879  0.34014389  1.76188568  0.55870709  2.96795854\n",
      " -0.59122827 -6.26099875 -3.38754944  2.55513859]\n",
      "\n",
      "Projection de U_9 * A :\n",
      "[ 0.1235503  -0.15165204 -0.72726969  1.5193808  -1.75763211  0.05297765\n",
      " -0.01475359  0.98745626  5.87977475  0.62846671]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0. 10.  0.]\n",
      "Différence :\n",
      "[ 0.1235503  -0.15165204 -0.72726969  1.5193808  -1.75763211  0.05297765\n",
      " -0.01475359  0.98745626 -4.12022525  0.62846671]\n",
      "\n",
      "Projection de U_10 * A :\n",
      "[ 0.28788599  1.21778006  0.0636429  -2.44276652  1.71134111  0.21961636\n",
      "  1.79097703  0.4703848   2.10929267  6.55703533]\n",
      "Vecteur cible :\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0. 10.]\n",
      "Différence :\n",
      "[ 0.28788599  1.21778006  0.0636429  -2.44276652  1.71134111  0.21961636\n",
      "  1.79097703  0.4703848   2.10929267 -3.44296467]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "# Création de 10 matrices U de dimension 10 x m\n",
    "m = 100  # Par exemple, m est 5\n",
    "U_list = [np.random.randn(10, m) for _ in range(10)]\n",
    "\n",
    "# Création des vecteurs cibles (one-hot vectors)\n",
    "targets = 10*np.eye(10)\n",
    "\n",
    "# Fonction objectif à minimiser\n",
    "def objective(A, U_list, targets):\n",
    "    total_error = 0\n",
    "    for i, U in enumerate(U_list):\n",
    "        projection = U @ A\n",
    "        error = np.linalg.norm(projection - targets[i])\n",
    "        total_error += error\n",
    "    return total_error\n",
    "\n",
    "# Contraintes : 0 <= A_i <= 1\n",
    "bounds = [(0, 1) for _ in range(m)]\n",
    "\n",
    "# Initialisation de A\n",
    "A0 = np.random.rand(m)\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "result = minimize(objective, A0, args=(U_list, targets), method='SLSQP', bounds=bounds)\n",
    "\n",
    "# Résultat optimal pour A\n",
    "A_optimal = result.x\n",
    "\n",
    "print(\"Vecteur A optimal :\")\n",
    "print(A_optimal)\n",
    "\n",
    "# Vérification du résultat pour chaque matrice U_i\n",
    "for i, U in enumerate(U_list):\n",
    "    print(f\"\\nProjection de U_{i+1} * A :\")\n",
    "    print(U @ A_optimal)\n",
    "    print(f\"Vecteur cible :\")\n",
    "    print(targets[i])\n",
    "    print(f\"Différence :\")\n",
    "    print(U @ A_optimal - targets[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vecteur A optimal :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.\n",
      " 0. 0.]\n",
      "\n",
      "Projection de U_1 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[100.   0.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[-100.    0.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_2 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0. 100.   0.   0.   0.   0.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0. -100.    0.    0.    0.    0.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_3 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0. 100.   0.   0.   0.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0. -100.    0.    0.    0.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_4 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0. 100.   0.   0.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0. -100.    0.    0.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_5 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0. 100.   0.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0. -100.    0.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_6 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0.   0. 100.   0.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0.    0. -100.    0.    0.    0.    0.]\n",
      "\n",
      "Projection de U_7 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0.   0.   0. 100.   0.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0.    0.    0. -100.    0.    0.    0.]\n",
      "\n",
      "Projection de U_8 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0.   0.   0.   0. 100.   0.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0.    0.    0.    0. -100.    0.    0.]\n",
      "\n",
      "Projection de U_9 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0. 100.   0.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0.    0.    0.    0.    0. -100.    0.]\n",
      "\n",
      "Projection de U_10 * A :\n",
      "[0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
      "Vecteur cible :\n",
      "[  0.   0.   0.   0.   0.   0.   0.   0.   0. 100.]\n",
      "Différence :\n",
      "[   0.    0.    0.    0.    0.    0.    0.    0.    0. -100.]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_833208/3013276635.py:51: DeprecationWarning: `method='simplex'` is deprecated and will be removed in SciPy 1.11.0. Please use one of the HiGHS solvers (e.g. `method='highs'`) in new code.\n",
      "  result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='simplex')\n",
      "/tmp/ipykernel_833208/3013276635.py:51: OptimizeWarning: A_eq does not appear to be of full row rank. To improve performance, check the problem formulation for redundant equality constraints.\n",
      "  result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='simplex')\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.optimize import linprog\n",
    "\n",
    "# Création de 10 matrices U de dimension 10 x m\n",
    "m = 50  # Par exemple, m est 5\n",
    "n = 10\n",
    "U_list = [np.random.randn(n, m) for _ in range(10)]\n",
    "\n",
    "# Création des vecteurs cibles (one-hot vectors)\n",
    "targets = 100*np.eye(n)\n",
    "\n",
    "# Nombre total de variables : m variables pour A et n variables pour les écarts absolus pour chaque U_i\n",
    "num_vars = m + n * 10\n",
    "\n",
    "# Fonction coût : minimiser la somme des écarts absolus\n",
    "c = np.zeros(num_vars)\n",
    "c[m:] = 1\n",
    "\n",
    "# Matrice des contraintes\n",
    "A_eq = []\n",
    "b_eq = []\n",
    "\n",
    "# Contraintes pour chaque U_i et vecteur cible\n",
    "for i in range(10):\n",
    "    U = U_list[i]\n",
    "    target = targets[i]\n",
    "    \n",
    "    # Contraintes pour les écarts positifs\n",
    "    for j in range(n):\n",
    "        constraint = np.zeros(num_vars)\n",
    "        constraint[:m] = U[j, :]\n",
    "        constraint[m + i * n + j] = -1\n",
    "        A_eq.append(constraint)\n",
    "        b_eq.append(target[j])\n",
    "        \n",
    "    # Contraintes pour les écarts négatifs\n",
    "    for j in range(n):\n",
    "        constraint = np.zeros(num_vars)\n",
    "        constraint[:m] = -U[j, :]\n",
    "        constraint[m + i * n + j] = -1\n",
    "        A_eq.append(constraint)\n",
    "        b_eq.append(-target[j])\n",
    "\n",
    "A_eq = np.array(A_eq)\n",
    "b_eq = np.array(b_eq)\n",
    "\n",
    "# Contraintes de bornes pour A (0 <= A_i <= 1)\n",
    "bounds = [(0, 1)] * m + [(0, None)] * (n * 10)\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "result = linprog(c, A_eq=A_eq, b_eq=b_eq, bounds=bounds, method='simplex')\n",
    "\n",
    "# Extraction du vecteur A optimal\n",
    "A_optimal = result.x[:m]\n",
    "\n",
    "print(\"Vecteur A optimal :\")\n",
    "print(A_optimal)\n",
    "\n",
    "# Vérification du résultat pour chaque matrice U_i\n",
    "for i, U in enumerate(U_list):\n",
    "    projection = U @ A_optimal\n",
    "    target = targets[i]\n",
    "    print(f\"\\nProjection de U_{i+1} * A :\")\n",
    "    print(projection)\n",
    "    print(f\"Vecteur cible :\")\n",
    "    print(target)\n",
    "    print(f\"Différence :\")\n",
    "    print(projection - target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 7.76511950e-01, -2.11416600e-01, -8.16243363e-01,\n",
       "         6.15934802e-01, -7.77046502e-02, -9.34587149e-01,\n",
       "        -1.14191775e+00,  1.20720608e+00, -6.02142752e-01,\n",
       "        -2.35146094e-01, -1.88232421e+00, -2.91452421e-01,\n",
       "         1.27163823e+00,  2.26413561e-02,  2.68831305e-01,\n",
       "        -7.96448349e-01,  2.08628308e-01,  6.49565394e-01,\n",
       "         4.87050819e-01, -5.06111662e-01,  1.05199116e+00,\n",
       "        -5.74469085e-01, -2.24595320e-01, -8.07182156e-01,\n",
       "         1.01983793e+00,  2.06280563e-01,  8.93093458e-01,\n",
       "        -1.20618621e+00, -5.10572517e-01,  7.64116964e-01,\n",
       "        -7.46463036e-01, -8.32280708e-01, -4.09005379e-01,\n",
       "         1.62221128e-01,  4.29375485e-01,  8.15972888e-01,\n",
       "        -6.47535372e-01,  6.20811166e-01, -4.51398384e-01,\n",
       "         1.36317571e+00, -2.96390139e-01, -6.22422725e-01,\n",
       "        -6.40961096e-01,  1.20690845e-01,  2.00782351e+00,\n",
       "         2.04628008e-01,  8.24667789e-02, -7.33323936e-01,\n",
       "         3.30273306e-01,  8.28059933e-01],\n",
       "       [ 3.42988156e-01, -8.19618194e-02,  3.21191010e-01,\n",
       "        -1.00728665e-03, -2.83358761e-01,  8.38308491e-01,\n",
       "        -1.95224752e-01, -9.16391646e-01,  8.17958970e-01,\n",
       "        -6.01122786e-01,  2.28312741e+00, -2.06758778e+00,\n",
       "         1.10292556e+00, -6.66192205e-01,  1.63623061e+00,\n",
       "         3.26114709e-01, -5.47946674e-01, -1.85560092e+00,\n",
       "         6.92779212e-01,  1.78885507e+00,  9.16288273e-01,\n",
       "        -5.74703132e-01,  2.52023490e-01, -4.31877585e-01,\n",
       "        -9.27403781e-01,  1.00155512e+00, -1.41772861e-01,\n",
       "         4.32468539e-02,  2.26101791e-01, -5.36768236e-01,\n",
       "         3.08945511e-01, -6.21522779e-01,  1.30041561e+00,\n",
       "         5.68163197e-01,  9.55185133e-01,  8.11285442e-01,\n",
       "        -2.97782622e-01,  3.02950837e-01, -1.26736280e+00,\n",
       "         9.77882989e-01,  3.59078995e-01, -4.85626195e-01,\n",
       "         4.43728666e-02, -2.27739602e-01, -6.12445648e-01,\n",
       "         6.18672688e-02,  7.16973714e-02,  1.71567567e-01,\n",
       "         1.07959477e+00, -1.32937645e+00],\n",
       "       [ 1.11520662e+00,  6.09806020e-01,  2.46610239e+00,\n",
       "        -2.21687639e+00,  6.85500952e-01, -3.21056226e-01,\n",
       "        -1.43035817e+00,  4.10653143e-01,  3.06690104e-01,\n",
       "         8.60401330e-01,  6.24823077e-01, -3.22596964e-01,\n",
       "         2.52786095e-01,  3.10483796e-01,  1.66719202e+00,\n",
       "         1.58918777e+00,  6.96139445e-01, -3.76442183e-02,\n",
       "        -2.62737503e-02,  6.33754211e-01, -5.91857228e-01,\n",
       "         4.69300838e-01, -5.17125752e-01,  2.49390072e-01,\n",
       "        -4.08712066e-01,  2.88116723e-01, -1.10096186e+00,\n",
       "        -1.35786521e+00,  3.24207222e-01,  7.09357136e-01,\n",
       "        -1.40329301e+00, -1.07602714e+00, -6.04187604e-02,\n",
       "         1.05190190e+00, -3.68432480e-01, -3.89816783e-01,\n",
       "        -1.34379705e+00, -4.52707002e-01, -2.55016384e-01,\n",
       "        -2.19263562e-01, -1.34303067e+00, -2.20756560e-02,\n",
       "         2.15133257e+00,  1.01585184e+00,  3.49810955e-01,\n",
       "        -2.58086039e-01, -3.31582450e-01, -5.64475132e-01,\n",
       "         1.05307671e+00, -8.98730437e-01],\n",
       "       [-8.49645301e-02, -1.91671538e+00,  2.64001431e-01,\n",
       "         2.79163059e-01, -4.73593374e-02,  3.07979820e-01,\n",
       "        -1.73955447e-01, -1.47109079e-01, -1.45329979e+00,\n",
       "        -3.69264015e-01,  1.51482986e+00,  2.06705027e+00,\n",
       "         2.54687261e+00, -4.25963072e-01, -4.58817561e-01,\n",
       "         1.15006005e+00, -8.17375316e-02,  7.87277473e-01,\n",
       "         5.13495238e-01, -2.06335492e+00,  2.03089533e-01,\n",
       "         9.07938575e-01,  4.33588707e-01, -1.86670938e-01,\n",
       "        -9.82083803e-03,  1.15221859e+00,  3.04172478e-01,\n",
       "        -1.59081566e+00, -1.67454872e+00, -8.43279429e-01,\n",
       "        -1.12492231e-01,  4.92722295e-01, -1.16529283e-01,\n",
       "        -1.74030996e+00,  3.39805210e-01, -9.22900795e-01,\n",
       "         4.41559628e-01,  2.37232184e-01,  2.76025079e-01,\n",
       "        -4.14799644e-01,  7.66470167e-01, -6.44174971e-01,\n",
       "         9.31374736e-01, -1.26587068e+00,  1.90285007e+00,\n",
       "         6.13698669e-01,  5.63847384e-01,  2.69972288e-01,\n",
       "         6.78492880e-01, -7.74919874e-01],\n",
       "       [ 2.07990186e-02, -2.41123210e+00, -1.04451463e+00,\n",
       "         1.95849903e-01,  1.12642571e+00,  6.50409217e-01,\n",
       "        -3.46651276e+00, -6.03964387e-01, -6.94242573e-01,\n",
       "         9.11468091e-01,  7.91682074e-01, -1.68685480e+00,\n",
       "         6.54002581e-01,  1.01906615e+00, -1.16380382e+00,\n",
       "         1.96770245e+00, -2.28308696e-01, -1.03067021e-01,\n",
       "         3.07191378e-01, -7.44848399e-02, -2.62638189e-01,\n",
       "         7.83251747e-01,  5.89034865e-01,  9.79901619e-01,\n",
       "        -7.81548742e-01, -1.34482633e+00, -1.05842915e-01,\n",
       "         8.97992745e-02,  8.65635698e-01, -7.88819611e-02,\n",
       "        -1.27664894e+00,  1.52690488e-01,  7.25452672e-01,\n",
       "        -1.39521850e+00,  3.07550798e-03,  7.04445848e-01,\n",
       "        -1.25707479e+00, -2.13864965e+00, -1.10273012e+00,\n",
       "        -1.08498228e+00, -2.89398743e-02,  5.80803481e-01,\n",
       "        -6.38869113e-01,  4.98921293e-02,  4.79582359e-01,\n",
       "        -6.89858217e-01, -1.40452713e-02,  1.40327771e+00,\n",
       "         2.12131041e+00,  1.09462028e+00],\n",
       "       [ 1.86766058e+00, -6.72271489e-01, -3.13515480e-01,\n",
       "        -1.07720678e+00,  8.54379574e-01,  5.86687405e-01,\n",
       "         5.08604605e-02, -2.91094425e-01, -1.95045273e-01,\n",
       "        -2.55233962e-01,  4.30737071e-01, -1.97588070e-02,\n",
       "         4.89556800e-01, -9.35055700e-02,  6.77239641e-01,\n",
       "         5.51992348e-01, -3.55130043e-02, -3.64552098e-01,\n",
       "        -1.50335383e-01,  2.33718942e-01, -6.06454617e-01,\n",
       "        -1.55221951e-01, -9.80138571e-01, -1.75916643e+00,\n",
       "        -5.61748092e-01,  1.45794214e+00, -4.66007511e-01,\n",
       "        -3.39207063e-01, -7.26031369e-01, -1.08139982e+00,\n",
       "         8.44347228e-01, -1.18156202e+00, -2.75646154e-01,\n",
       "         2.70622466e+00, -4.80150364e-01,  5.75857437e-01,\n",
       "        -4.34343351e-01, -5.21858382e-01, -1.27891027e+00,\n",
       "        -2.55590453e-01,  1.03071351e+00, -3.53982630e-01,\n",
       "         1.10790316e+00, -1.08498256e+00,  9.71504662e-01,\n",
       "        -1.84851959e+00, -2.10931812e-01,  1.00661061e+00,\n",
       "         1.51203749e+00,  1.47393102e+00],\n",
       "       [ 2.16099217e+00, -9.12861926e-01, -4.91967189e-01,\n",
       "         3.94430526e-01, -6.11418284e-01, -6.63103912e-02,\n",
       "        -3.34538483e+00, -1.47426881e-01,  8.72417918e-01,\n",
       "        -8.80632112e-01, -2.45645930e+00, -3.35306194e-01,\n",
       "         1.72624853e+00, -7.32183520e-01,  2.65300318e-01,\n",
       "         7.64508751e-01,  1.13443708e+00, -2.03321803e-01,\n",
       "        -8.79152845e-01,  9.58447502e-01,  6.07451985e-01,\n",
       "         8.40528189e-01, -9.56241568e-01,  6.31884001e-01,\n",
       "         1.25687379e+00,  1.37130535e+00,  4.11299510e-01,\n",
       "         2.68586296e+00, -1.93932688e+00, -2.25207822e+00,\n",
       "         8.36974211e-01, -6.95507341e-01, -1.54265412e+00,\n",
       "         9.24123764e-01, -3.85294693e-01,  1.38325891e+00,\n",
       "        -1.39872644e+00, -1.76606410e+00, -5.00080367e-01,\n",
       "         5.20579067e-01, -1.24901726e+00, -9.05069969e-01,\n",
       "        -1.20773129e+00,  1.15171268e+00,  4.44625904e-01,\n",
       "        -1.20160607e+00,  7.31442327e-01,  3.08257351e-01,\n",
       "        -1.04889582e+00, -1.16496259e+00],\n",
       "       [-7.43356189e-01,  9.09589608e-01, -1.48016937e+00,\n",
       "        -8.28066935e-01, -1.18233124e+00, -8.20994249e-01,\n",
       "        -8.13714193e-01,  3.71042988e-02, -1.03037286e+00,\n",
       "        -1.49717234e+00,  6.30137769e-02,  1.27732950e-01,\n",
       "        -7.31105134e-01, -9.24111939e-02, -1.48304815e+00,\n",
       "         1.56211502e+00, -1.00156438e+00,  1.79741017e+00,\n",
       "         1.38551275e+00, -4.15246189e-01,  1.59778943e+00,\n",
       "         1.38375263e-01, -7.41855230e-01, -4.55637753e-01,\n",
       "         5.16557250e-01,  1.03887783e+00, -1.60843540e-02,\n",
       "        -1.30258857e+00, -1.36642046e+00,  9.26828970e-01,\n",
       "        -1.20741847e+00,  5.13695696e-01,  1.67495641e-02,\n",
       "         1.21576806e+00, -8.12597950e-01, -6.92868830e-01,\n",
       "         2.55495767e-01,  6.47196447e-01, -1.18973763e+00,\n",
       "         1.01436967e+00, -6.05234653e-01, -6.06351448e-01,\n",
       "        -1.45248010e-01,  9.48870758e-01, -9.89935749e-01,\n",
       "         1.72905122e+00,  1.88924331e-02, -1.24891517e+00,\n",
       "         3.46779353e-01,  1.08823628e+00],\n",
       "       [ 1.38523928e+00, -1.96628727e-01,  1.37369688e+00,\n",
       "        -1.97950104e+00,  5.44376966e-01,  6.97764529e-01,\n",
       "         9.85863963e-01, -7.93911300e-01,  1.16915185e+00,\n",
       "         5.25385243e-01, -3.65536778e-01, -1.43646000e-01,\n",
       "        -9.49100187e-01, -7.67127359e-01, -1.09721782e+00,\n",
       "        -8.38712235e-01, -2.30382360e-01, -7.56187558e-01,\n",
       "        -3.14946620e-01, -8.48419472e-01, -3.73427606e-01,\n",
       "        -1.45636831e+00,  5.19599077e-01, -5.62312471e-02,\n",
       "         2.36588883e+00, -1.75562842e-01, -4.97326804e-01,\n",
       "        -8.37846526e-01, -4.67782968e-01,  1.06980650e+00,\n",
       "         8.21100157e-01, -1.15594311e+00, -7.61412370e-01,\n",
       "         6.32569554e-01,  4.66509537e-01, -8.46202184e-01,\n",
       "        -5.67087293e-01,  3.48036077e-01, -2.87700085e-01,\n",
       "         4.97068460e-01, -9.15260357e-01,  5.97406904e-01,\n",
       "        -8.04519310e-01,  7.10991177e-01,  6.43551800e-01,\n",
       "         7.17012357e-01,  1.44539421e+00, -4.92544732e-01,\n",
       "         3.80552739e-01,  7.13535463e-02],\n",
       "       [-5.15345781e-01, -1.39266902e+00,  5.94203963e-01,\n",
       "         5.67144111e-01,  4.90002119e-01, -1.14906683e+00,\n",
       "        -4.37839830e-02,  6.28115521e-01, -1.12643523e-01,\n",
       "         1.88585104e-01, -1.08313249e-01,  1.00577299e+00,\n",
       "        -1.32646908e+00,  1.98782417e-01,  2.17772791e+00,\n",
       "         2.08957053e-02, -5.91627799e-01, -6.11606813e-01,\n",
       "        -9.06962940e-01,  2.87762876e+00, -4.96246242e-01,\n",
       "        -7.24988714e-01,  1.42068653e+00,  5.00777988e-01,\n",
       "        -2.74016165e-01, -1.03723606e+00,  1.09853356e+00,\n",
       "        -9.71432645e-01,  5.77982232e-01, -2.08977781e-01,\n",
       "         5.33191153e-01, -6.32782177e-01,  6.69836602e-01,\n",
       "        -1.40156786e+00, -8.87893836e-01, -1.40285033e+00,\n",
       "        -8.13412639e-01, -9.62432756e-01,  6.74375855e-01,\n",
       "        -3.42504447e-01,  7.34197609e-01,  8.39475871e-01,\n",
       "         2.76323880e-01,  2.20427737e-01,  8.69237272e-01,\n",
       "        -6.16008266e-01,  2.44958790e+00, -1.24454272e+00,\n",
       "         5.42382299e-01, -1.87821419e+00]])"
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2.5170101894250237e-05"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max(A_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.59622766 -0.42862753 -0.62410684  0.0970611   0.22743921  0.67223485\n",
      " -1.11070654  0.70468959 -1.32805413 -1.6862091 ]\n"
     ]
    }
   ],
   "source": [
    "print(U[:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1.00000000e+00 -7.45388994e-18 -9.97465999e-18 -1.08420217e-19\n",
      " -3.03576608e-18  2.03830008e-17  1.15738582e-17  2.01661604e-17\n",
      " -5.76795556e-17  1.95156391e-17]\n"
     ]
    }
   ],
   "source": [
    "print(U@A_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Matrice A optimale :\n",
      "[[-0.32726402 -0.05483811 -0.27008309 ...  0.00894211 -0.01061029\n",
      "  -0.0656751 ]\n",
      " [-0.17004817 -0.13314101  0.07450138 ... -0.20929294  0.35085373\n",
      "   0.10897319]\n",
      " [ 0.06524285  0.03585318  0.01339497 ... -0.07603262 -0.04163731\n",
      "   0.21987791]\n",
      " ...\n",
      " [-0.21831855 -0.52682855 -0.05274236 ...  0.01596774  0.06934764\n",
      "  -0.25794054]\n",
      " [-0.34488701 -0.30493354 -0.00435956 ...  0.06364987  0.08555236\n",
      "   0.19018415]\n",
      " [-0.19105156  0.44460304 -0.05878165 ... -0.29982517  0.09671853\n",
      "   0.21287517]]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import numpy as np\n",
    "from scipy.optimize import minimize\n",
    "\n",
    "U_list = [np.random.randn(10, 123) for _ in range(10)]\n",
    "S_list = [np.random.randn(10, 10) for _ in range(10)]\n",
    "\n",
    "\n",
    "m, n = U_list[0].shape\n",
    "\n",
    "def objective(A_vec, U_list, S_list, m, n):\n",
    "    A = A_vec.reshape((n, m))\n",
    "    total_error = 0\n",
    "    for U, S in zip(U_list, S_list):\n",
    "        total_error += np.linalg.norm(U @ A - S, 'fro')\n",
    "    return total_error\n",
    "\n",
    "# Contraintes : -1 <= A_ij <= 1\n",
    "bounds = [(-1, 1) for _ in range(n * m)]\n",
    "\n",
    "# Initialisation de A\n",
    "A0 = np.zeros(n * m)\n",
    "\n",
    "# Résolution du problème d'optimisation\n",
    "result = minimize(objective, A0, args=(U_list, S_list, m, n), method='SLSQP', bounds=bounds)\n",
    "\n",
    "# Résultat optimal pour A\n",
    "A_optimal = result.x.reshape((n, m))\n",
    "\n",
    "print(\"Matrice A optimale :\")\n",
    "print(A_optimal)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "str = 'test'\n",
    "str2 = 'test2'\n",
    "\n",
    "dictionaire = {str :3, str2: 4}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dictionaire['test']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 1.00000000e+01, -3.87060176e-17,  6.91720986e-17,\n",
       "        -4.87890978e-19, -8.51098705e-18,  2.52293846e-16,\n",
       "        -3.28513258e-17,  1.56287743e-16, -1.99818460e-16,\n",
       "        -1.37693676e-17],\n",
       "       [-7.69783542e-18,  1.00000000e+01, -2.08112607e-16,\n",
       "         1.23273787e-16,  7.23162849e-17,  9.75781955e-17,\n",
       "         5.02527707e-17,  2.06865775e-16, -1.22514845e-16,\n",
       "        -2.86771475e-17],\n",
       "       [-5.94142791e-17,  2.30935063e-17,  1.00000000e+01,\n",
       "        -1.12214925e-16, -4.33680869e-19, -5.44269491e-17,\n",
       "        -1.19533290e-17, -5.22368607e-16, -1.30971622e-16,\n",
       "        -4.60785923e-17],\n",
       "       [ 1.14356224e-16, -2.85307802e-16, -7.13405029e-17,\n",
       "         1.00000000e+01,  3.21574364e-16, -2.81025203e-16,\n",
       "         7.77372958e-17, -1.62088225e-16,  1.90114851e-16,\n",
       "         3.49655201e-16],\n",
       "       [-1.27122705e-17, -1.31188463e-16, -3.34801631e-16,\n",
       "        -2.71050543e-17,  1.00000000e+01, -2.71917905e-16,\n",
       "         2.61292724e-17, -2.98697699e-17,  1.34115809e-16,\n",
       "         5.20417043e-18],\n",
       "       [-3.07046055e-16,  5.20417043e-18,  2.21285663e-16,\n",
       "         1.83880688e-16, -4.11996826e-17,  1.00000000e+01,\n",
       "        -2.76471554e-17, -2.25568262e-16, -3.41252634e-17,\n",
       "         2.42861287e-17],\n",
       "       [-3.26399064e-16,  1.44876515e-16, -4.29344060e-17,\n",
       "        -2.42319186e-17, -1.29020059e-16, -5.86553375e-17,\n",
       "         1.00000000e+01,  5.64869332e-17,  1.31567934e-16,\n",
       "         4.64038530e-17],\n",
       "       [ 8.99345702e-17,  7.97972799e-17, -1.25279561e-16,\n",
       "        -4.62412227e-17, -1.02348685e-16,  4.34222970e-17,\n",
       "        -7.02020907e-17,  1.00000000e+01,  2.19008839e-16,\n",
       "         2.00902663e-16],\n",
       "       [-1.96457434e-16, -2.60208521e-18, -1.56558794e-16,\n",
       "        -2.50071231e-16, -2.57064335e-16, -4.13081028e-17,\n",
       "        -3.65376132e-17,  4.60677503e-16,  1.00000000e+01,\n",
       "         1.63714528e-17],\n",
       "       [-3.42607887e-17, -3.61581425e-17, -5.36680075e-17,\n",
       "         4.68537969e-16,  2.27682456e-16, -3.46727855e-16,\n",
       "        -1.41163123e-16, -3.55076211e-17,  4.49943902e-18,\n",
       "         1.00000000e+01]])"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "U@A_optimal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval =CT(model,device).evaluate()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result[784:784+784,2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "torch.Size([16, 1, 3, 3])\n",
      "Update parameter after EPOCH 49\n",
      "EPOCH ACCURACY = 10.00 %\n"
     ]
    }
   ],
   "source": [
    "\n",
    "k_plus = 0.2\n",
    "epoch_acc = best_acc\n",
    "\n",
    "while epoch_acc>= best_acc:\n",
    "        for k in range (10):\n",
    "                print((torch.sum(conv1_weight_relevance_list[-50+k][:k%10],dim = 0).shape))\n",
    "                model.conv1.weight.data =model.conv1.weight.data + k_plus*(conv1_weight_relevance_list[-50+k][k%10])-k_min*(torch.sum(conv1_weight_relevance_list[-50+k][:k%10],dim =0)+torch.sum(conv1_weight_relevance_list[-50+k][(k+1)%10:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv1.bias.data = model.conv1.bias.data + k_plus*(conv1_bias_relevance_list[-50+k][k%10])-k_min*(torch.sum(conv1_bias_relevance_list[-50+k][:k%10],dim = 0)+torch.sum(conv1_bias_relevance_list[-50+k][(k+1)%10:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.conv2.weight.data =model.conv2.weight.data+k_plus*(conv2_weight_relevance_list[-50+k][k%10])-k_min*(torch.sum(conv2_weight_relevance_list[-50+k][:k%10],dim =0)+torch.sum(conv2_weight_relevance_list[-50+k][(k+1)%10:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv2.bias.data = model.conv2.bias.data + k_plus*(conv2_bias_relevance_list[-50+k][k%10])-k_min*(torch.sum(conv2_bias_relevance_list[-50+k][:k%10],dim = 0)+torch.sum(conv2_bias_relevance_list[-50+k][(k+1)%10:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "        \n",
    "        \n",
    "                model.fc1[1].weight.data =model.fc1[1].weight.data+k_plus*(fc1_weight_relevance_list[-50+k][k%10])-k_min*(torch.sum(fc1_weight_relevance_list[-50+k][:k%10],dim =0)+torch.sum(fc1_weight_relevance_list[-50+k][(k+1)%10:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(fc1_bias_relevance_list[-50+k][k%10])-k_min*(torch.sum(fc1_bias_relevance_list[-50+k][:k%10],dim = 0)+torch.sum(fc1_bias_relevance_list[-50+k][(k+1)%10:],dim = 0))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "        \n",
    "                model.fc2[1].weight.data =model.fc2[1].weight.data+k_plus*(fc2_weight_relevance_list[-50+k][k%10])-k_min*(torch.sum(fc2_weight_relevance_list[-50+k][:k%10],dim =0)+torch.sum(fc2_weight_relevance_list[-50+k][(k+1)%10:],dim =0))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "        \n",
    "                model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(fc2_bias_relevance_list[-50+k][k%10])-k_min*(torch.sum(fc2_bias_relevance_list[-50+k][:k%10],dim = 0)+torch.sum(fc2_bias_relevance_list[-50+k][(k+1)%10:],dim = 0))#/torch.no\n",
    "        \n",
    "        \n",
    "\n",
    "\n",
    "        from custom_train import CustomTrainer as CT\n",
    "        print(f\"Update parameter after EPOCH {i%50}\")\n",
    "        eval =CT(model, device = device)\n",
    "        epoch_acc = eval.evaluate_model(dataset_test)\n",
    "        if  epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_epoch_indice = i\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fc= torch.zeros_like(model.fc1[1].weight.data)\n",
    "k_min=k_plus\n",
    "for k in range(10):\n",
    "    \n",
    "    fc+=k_plus*(fc1_weight_relevance_list[-10+k][k])-k_min*(torch.sum(fc1_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(fc1_weight_relevance_list[-10+k][k+1:],dim =0))#\n",
    "plt.imshow(torch.abs(fc[:,4600:4650]))\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.conv1.weight.data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.max(conv1_weight_relevance_list[9][8])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.imshow(fc2_weight_relevance_list[5][6][:,:])\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 9\n",
    "((i+1)%10) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = []\n",
    "for i in range(10):\n",
    "    test.append(i)\n",
    "\n",
    "for k in range(10):\n",
    "    print(test[-10+k],k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result = result_list[1]\n",
    "plot_dominance(*result[:4])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "concat= torch.abs(img_relevance_list[0][2])\n",
    "\n",
    "concat = concat.reshape(28,28)\n",
    "plt.imshow(torch.abs(concat.cpu()).numpy(), cmap='viridis')\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.imshow(torch.abs(fc1_weight_relevance_list[0][1,:,200:400]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.figure(figsize=(12, 6)) \n",
    "plt.imshow(torch.abs(model.fc1[1].weight.data[:,200:400]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_number = np.arange(0,50,1)\n",
    "label_list_number = np.array([int(label_list_number[i]/10) for i in label_list_number])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_list_number"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\"\"\"\n",
    "This is the code to create a video of the prediction and the ground truth\n",
    "It takes the prediction and the ground truth and reshapes them to the original shape of the image.\n",
    "It then creates a video of the prediction and the ground truth.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "array_list_img=np.array(img_list)\n",
    "\n",
    "array_list_fc1=np.array(fc1_weight_relevance_list)\n",
    "\n",
    "fig, ax = plt.subplots(3, 2, figsize = (12, 12),gridspec_kw={'width_ratios': [1,3],'height_ratios': [1, 1, 2]})\n",
    "def init():\n",
    "    ax[2,1].cla()\n",
    "    ax[0,0].cla()\n",
    "    ax[1,0].cla()\n",
    "    ax[1,1].cla()\n",
    "    ax[0,1].cla()\n",
    "   \n",
    "    ax[0,0].imshow(array_list_img[0][0])\n",
    "    ax[0,1].imshow(np.abs(model.fc1[1].weight.data[:,200:400]))\n",
    "    ax[1,0].imshow(np.abs(img_relevance_list[0][0].reshape(28,28)))\n",
    "\n",
    "    ax[1,1].imshow(np.abs(array_list_fc1[0][label_list_number[0],:,200:400]))\n",
    "   \n",
    "    result,y_min,y_max,y_true = result_list[0][:4]\n",
    "    y_min       =  np.array(y_min)\n",
    "    y_max       =  np.array(y_max)\n",
    "    center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "    y_true      =  np.expand_dims(np.array(y_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_scale = np.arange(len(y_min))\n",
    "    D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "    \n",
    "    # plot:\n",
    "\n",
    "    \n",
    "    ax[2,1].eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "    ax[2,1].eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "    ax[2,1].eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "    ax[2,1].set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "            ylim=(np.min(D)-1, np.max(D)+1))\n",
    "    ax[2,0].axis('off')\n",
    "  \n",
    "    \n",
    " \n",
    "    return [ax]\n",
    "\n",
    "def update(frame):\n",
    "    \n",
    "    ax[2,1].cla()\n",
    "    ax[0,0].cla()\n",
    "    ax[1,0].cla()\n",
    "    ax[1,1].cla()\n",
    "    ax[0,1].cla()\n",
    "   \n",
    "    ax[0,0].imshow(array_list_img[frame][0])\n",
    "    ax[0,1].imshow(np.abs(model.fc1[1].weight.data[:,200:400+2*frame]))\n",
    "    ax[1,0].imshow(np.abs(img_relevance_list[frame][num_label_list[frame]].reshape(28,28)))\n",
    "   \n",
    "\n",
    "    ax[1,1].imshow(np.abs(array_list_fc1[frame][num_label_list[frame],:,200:400]))\n",
    "    result,y_min,y_max,y_true = result_list[frame][:4]\n",
    "    y_min       =  np.array(y_min)\n",
    "    y_max       =  np.array(y_max)\n",
    "    center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "    y_true      =  np.expand_dims(np.array(y_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_scale = np.arange(len(y_min))\n",
    "    D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "    \n",
    "    # plot:\n",
    "    ax[2,1].set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "            ylim=(np.min(D)-1, np.max(D)+1))\n",
    "    \n",
    "    ax[2,1].eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "    ax[2,1].plot([num_label_list[frame], num_label_list[frame]], [y_min[num_label_list[frame]], y_max[num_label_list[frame]]], color='red') \n",
    "    ax[2,1].eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "    ax[2,1].eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "    \n",
    "    ax[2,0].axis('off')\n",
    "\n",
    "\n",
    "    plt.title(f'Inference over {name_label_list[frame]} ; Relevance of fc1 layer IAW {label_list_number[frame]} label.')\n",
    "\n",
    "    return [ax]\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=50, init_func=init)\n",
    "\n",
    "output_video = 'Abstract_network_relevance_dominance.mp4'\n",
    "\n",
    "animation.save(output_video, fps=0.5, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "array = torch.randn(3,3,3)\n",
    "array"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "array[0][2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "total_list = [img_list,\n",
    "img_relevance_list,\n",
    "conv1_weight_relevance_list, \n",
    "conv1_bias_relevance_list,\n",
    "conv2_weight_relevance_list,\n",
    "conv2_bias_relevance_list, \n",
    "\n",
    "fc1_weight_relevance_list, \n",
    "fc1_bias_relevance_list, \n",
    "fc2_weight_relevance_list, \n",
    "fc2_bias_relevance_list,  \n",
    "result_list,\n",
    "num_label_list,\n",
    "name_label_list, \n",
    "true_predict_list] \n",
    "\n",
    "\n",
    "file_path = 'my_list.pkl'\n",
    "\n",
    "\n",
    "with open(file_path, 'wb') as file:\n",
    " \n",
    "    pickle.load(total_list, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pickle\n",
    "file_path = 'my_list.pkl'\n",
    "\n",
    "\n",
    "with open(file_path, 'rb') as file:\n",
    " \n",
    "    pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\"\"\"\n",
    "This is the code to create a video of the prediction and the ground truth\n",
    "It takes the prediction and the ground truth and reshapes them to the original shape of the image.\n",
    "It then creates a video of the prediction and the ground truth.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "array_list_img=np.array(img_list)\n",
    "\n",
    "array_list_fc1=np.array(fc1_weight_relevance_list)\n",
    "\n",
    "fig, ax = plt.subplots(1, 2, figsize = (12, 6),gridspec_kw={'width_ratios': [1,1]})\n",
    "def init():\n",
    "  \n",
    "   \n",
    "    ax[0].imshow(array_list_img[0][0])\n",
    "\n",
    "    ax[1].imshow(np.abs(img_relevance_list[0][0].reshape(28,28)))\n",
    "\n",
    "    \n",
    "  \n",
    "    \n",
    " \n",
    "    return [ax]\n",
    "\n",
    "def update(frame):\n",
    "    \n",
    "  \n",
    "   \n",
    "    ax[0].imshow(array_list_img[frame+20][0])\n",
    "\n",
    "    ax[1].imshow(np.abs(img_relevance_list[frame+20][num_label_list[frame+20]].reshape(28,28)))\n",
    "   \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    return [ax]\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=20, init_func=init)\n",
    "\n",
    "output_video = 'Abstract_network_relevance.mp4'\n",
    "\n",
    "animation.save(output_video, fps=0.5, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from abstract import abstractTensor as AT\n",
    "abstract_network = False\n",
    "scale =0.01\n",
    "xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"]\n",
    "\n",
    "img_list = []\n",
    "img_relevance_list =[]\n",
    "conv1_weight_relevance_list = []\n",
    "conv1_bias_relevance_list = []\n",
    "conv2_weight_relevance_list = []\n",
    "conv2_bias_relevance_list = []\n",
    "\n",
    "fc1_weight_relevance_list = []\n",
    "fc1_bias_relevance_list = []\n",
    "fc2_weight_relevance_list = []\n",
    "fc2_bias_relevance_list  = []\n",
    "result_list =[]\n",
    "num_label_list = []\n",
    "name_label_list = []\n",
    "true_predict_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(10)):\n",
    "    print(i)\n",
    "    if i%10 ==0:\n",
    "        scale = 0.01/(i/10+1)\n",
    "    if abstract_network : \n",
    "    \n",
    "        conv1_eps_weight = torch.tensor([])\n",
    "        conv1__weight_span = torch.max(model.conv1.weight.data.flatten())-torch.min(model.conv1.weight.data.flatten())\n",
    "        conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "        conv1_eps_weight.values = scale*conv1__weight_span*torch.ones_like(conv1_eps_weight.indices)\n",
    "        conv1_eps_bias = torch.tensor([])\n",
    "        conv1__bias_span = torch.max(model.conv1.bias.data.flatten())-torch.min(model.conv1.bias.data.flatten())\n",
    "        conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "        conv1_eps_bias.values = scale*conv1__bias_span*torch.ones_like(conv1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        conv2_eps_weight = torch.tensor([])\n",
    "        conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "        conv2_weight_span = torch.max(model.conv2.weight.data.flatten())-torch.min(model.conv2.weight.data.flatten())\n",
    "        conv2_eps_weight.values = scale*conv2_weight_span*torch.ones_like(conv2_eps_weight.indices)\n",
    "        conv2_eps_bias = torch.tensor([])\n",
    "        conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "        conv2_bias_span = torch.max(model.conv2.bias.data.flatten())-torch.min(model.conv2.bias.data.flatten())\n",
    "        conv2_eps_bias.values = scale/2*conv2_bias_span*torch.ones_like(conv2_eps_bias.indices)\n",
    "\n",
    "        fc1_eps_weight = torch.tensor([])\n",
    "        fc1_weight_span = torch.max(model.fc1[1].weight.data.flatten())-torch.min(model.fc1[1].weight.data.flatten())\n",
    "        fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "        fc1_eps_weight.values = scale/1*fc1_weight_span*torch.ones_like(fc1_eps_weight.indices)\n",
    "        fc1_eps_bias = torch.tensor([])\n",
    "        fc1_bias_span = torch.max(model.fc1[1].bias.data.flatten())-torch.min(model.fc1[1].bias.data.flatten())\n",
    "        fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "        fc1_eps_bias.values = scale/1*fc1_bias_span*torch.ones_like(fc1_eps_bias.indices)\n",
    "\n",
    "\n",
    "        fc2_eps_weight = torch.tensor([])\n",
    "        fc2_weight_span = torch.max(model.fc2[1].weight.data.flatten())-torch.min(model.fc2[1].weight.data.flatten())\n",
    "        fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "        fc2_eps_weight.values = scale/100*fc2_weight_span*torch.ones_like(fc2_eps_weight.indices)\n",
    "        fc2_eps_bias = torch.tensor([])\n",
    "        fc2_bias_span = torch.max(model.fc2[1].bias.data.flatten())-torch.min(model.fc2[1].bias.data.flatten())\n",
    "        fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "        fc2_eps_bias.values = scale/100*fc2_bias_span*torch.ones_like(fc2_eps_bias.indices)\n",
    "\n",
    "\n",
    "    else :\n",
    "        conv1_eps_weight = make_indice_and_values_tupple()\n",
    "        conv1_eps_bias = make_indice_and_values_tupple()\n",
    "       \n",
    "\n",
    "        conv2_eps_weight = make_indice_and_values_tupple()\n",
    "        conv2_eps_bias = make_indice_and_values_tupple()\n",
    "        \n",
    "        for param in model.named_parameters():\n",
    "            if param[0] == 'fc1.1.weight':\n",
    "                    print(torch.sum(param[1].data))        \n",
    "                    index = torch.topk(torch.abs(param[1].data).flatten(),k=294912,largest=False)\n",
    "                    mat= param[1].data\n",
    "            \n",
    "\n",
    "                    alpha= 1.1*torch.abs(index.values)\n",
    "\n",
    "        fc1_eps_bias = make_indice_and_values_tupple()\n",
    "        \n",
    "        fc1_eps_weight = make_indice_and_values_tupple()\n",
    "        fc1_eps_weight.indices = index.indices\n",
    "        fc1_eps_weight.values = alpha\n",
    "        fc2_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_bias = make_indice_and_values_tupple()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label = None\n",
    "\n",
    "  \n",
    "    while label!= i: \n",
    "        index = np.random.randint(0,5000)\n",
    "        \n",
    "        img, label = dataset_train[index]\n",
    "\n",
    "    name_label_list.append(xticklabels[label])\n",
    "    num_label_list.append(label)\n",
    "   \n",
    "    img_list.append(img)\n",
    "    \n",
    "    x=AT(img,alpha =torch.tensor([]))\n",
    "    x=x.abstract_tensor()\n",
    "    print(x.shape)\n",
    "\n",
    "    \n",
    "  \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        \n",
    "        result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1=model.abstract_forward(x,\n",
    "                                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "\n",
    "                                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                                    add_symbol=True)\n",
    "        \n",
    "    result_list.append([result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1])    \n",
    "    true_predict_list.append(torch.argmax(x_true))\n",
    "    plot_dominance(result,x_min,x_max,x_true)\n",
    "    \n",
    "\n",
    "\n",
    "    concatenated_tensors = []\n",
    "    concatenated_heatmap_tensors =[]\n",
    "    for j in range(10):\n",
    "        concat_part =result[1:785,j]\n",
    "        concatenated_heatmap_tensors.append(concat_part)\n",
    "    concatenated_heatmap_tensors = torch.stack(concatenated_heatmap_tensors)\n",
    "\n",
    "    img_relevance_list.append(concatenated_heatmap_tensors)\n",
    "\n",
    "  \n",
    "    if abstract_network:\n",
    "\n",
    "        conv1_weight_concatenated_tensors = []\n",
    "        conv1_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len(x)-1:len(x)+143, j].view(16, 1, 3, 3)\n",
    "            conv1_weight_concatenated_tensors.append(concat_part)\n",
    "        conv1_weight_concatenated_tensors =torch.stack(conv1_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part= result[len(x)+143:len(x)+159, j]\n",
    "            conv1_bias_concatenated_tensors .append(concat_part)\n",
    "        conv1_bias_concatenated_tensors = torch.stack(conv1_bias_concatenated_tensors )\n",
    "\n",
    "        conv2_weight_concatenated_tensors = []\n",
    "        conv2_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb-1:len_symb+4607, j].view(32, 16, 3, 3)\n",
    "            conv2_weight_concatenated_tensors.append(concat_part)\n",
    "        conv2_weight_concatenated_tensors =torch.stack(conv2_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb+4607:len_symb+4639, j]\n",
    "            conv2_bias_concatenated_tensors.append(concat_part)\n",
    "        conv2_bias_concatenated_tensors = torch.stack(conv2_bias_concatenated_tensors)\n",
    "\n",
    "        fc1_weight_concatenated_tensors = []\n",
    "        fc1_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_c2-1:len_symb_c2+294911, j].reshape(64,4608)\n",
    "            fc1_weight_concatenated_tensors.append(concat_part)\n",
    "        fc1_weight_concatenated_tensors =torch.stack(fc1_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_c2+294911:len_symb_c2+294975, j]\n",
    "            fc1_bias_concatenated_tensors.append(concat_part)\n",
    "        fc1_bias_concatenated_tensors= torch.stack(fc1_bias_concatenated_tensors)\n",
    "\n",
    "        fc2_weight_concatenated_tensors = []\n",
    "        fc2_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_fc1-1:len_symb_fc1+639, j].view(10,64)\n",
    "            fc2_weight_concatenated_tensors.append(concat_part)\n",
    "        fc2_weight_concatenated_tensors =torch.stack(fc2_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_fc1+639:len_symb_fc1+649, j]\n",
    "            fc2_bias_concatenated_tensors.append(concat_part)\n",
    "        fc2_bias_concatenated_tensors= torch.stack(fc2_bias_concatenated_tensors)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "        conv1_weight_relevance_list.append(conv1_weight_concatenated_tensors)\n",
    "        conv1_bias_relevance_list.append(conv1_bias_concatenated_tensors)\n",
    "        conv2_weight_relevance_list.append(conv2_weight_concatenated_tensors)\n",
    "\n",
    "        conv2_bias_relevance_list.append(conv2_bias_concatenated_tensors)\n",
    "\n",
    "        fc1_weight_relevance_list.append(fc1_weight_concatenated_tensors)\n",
    "        fc1_bias_relevance_list.append(fc1_bias_concatenated_tensors)\n",
    "        fc2_weight_relevance_list.append(fc2_weight_concatenated_tensors)\n",
    "        fc2_bias_relevance_list.append(fc2_bias_concatenated_tensors)\n",
    "\n",
    "    else :\n",
    "        fc1_bias_concatenated_tensors =[]\n",
    "        fc1_weight_concatenated_tensors = []\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_c2-1:len_symb_c2+294911, j]#.reshape(64,4608)\n",
    "            fc1_weight_concatenated_tensors.append(concat_part)\n",
    "        fc1_weight_concatenated_tensors =torch.stack(fc1_weight_concatenated_tensors)\n",
    "\n",
    "\n",
    "    fc1_weight_relevance_list.append(fc1_weight_concatenated_tensors)\n",
    "\n",
    "    \"\"\"\n",
    "    k_min = 0.01\n",
    "    k_plus = 0.6\n",
    "    if i>9:\n",
    "        k_plus = 0.5\n",
    "        k_min = 0.05\n",
    "    if i>19:\n",
    "        k_plus = 0.4\n",
    "        k_min = 0.1\n",
    "    if i>39:\n",
    "        k_min = 0.2\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "    model.conv1.weight.data =model.conv1.weight.data+k_plus*(concatenated_tensors[label]-k_min*(torch.sum(concatenated_tensors[:label],dim =0)+torch.sum(concatenated_tensors[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.conv1.bias.data = model.conv1.bias.data + k_plus*(concatenated_bias_tensors[label]-k_min*(torch.sum(concatenated_bias_tensors[:label],dim = 0)+torch.sum(concatenated_bias_tensors[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.conv2.weight.data =model.conv2.weight.data+k_plus*(concatenated_tensors_2[label]-k_min*(torch.sum(concatenated_tensors_2[:label],dim =0)+torch.sum(concatenated_tensors_2[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.conv2.bias.data = model.conv2.bias.data + k_plus*(concatenated_bias_tensors_2[label]-k_min*(torch.sum(concatenated_bias_tensors_2[:label],dim = 0)+torch.sum(concatenated_bias_tensors_2[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.fc1[1].weight.data =model.fc1[1].weight.data+k_plus*(concatenated_tensors_fc1[label]-k_min*(torch.sum(concatenated_tensors_fc1[:label],dim =0)+torch.sum(concatenated_tensors_fc1[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(concatenated_bias_tensors_fc1[label]-k_min*(torch.sum(concatenated_bias_tensors_fc1[:label],dim = 0)+torch.sum(concatenated_bias_tensors_fc1[label+1:],dim = 0)))#/torch.norm(2*concatenated_bias_tensors[label]-torch.sum(concatenated_bias_tensors,dim = 0))\n",
    "\n",
    "    model.fc2[1].weight.data =model.fc2[1].weight.data+k_plus*(concatenated_tensors_fc2[label]-k_min*(torch.sum(concatenated_tensors_fc2[:label],dim =0)+torch.sum(concatenated_tensors_fc2[label+1:],dim =0)))#/torch.norm(2*concatenated_tensors[label]-torch.sum(concatenated_tensors,dim =0))\n",
    "    model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(concatenated_bias_tensors_fc2[label]-k_min*(torch.sum(concatenated_bias_tensors_fc2[:label],dim = 0)+torch.sum(concatenated_bias_tensors_fc2[label+1:],dim = 0)))#/torch.no\n",
    "    \"\"\"\n",
    "   \n",
    "\n",
    "   \n",
    "    \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.load_state_dict(torch.load('dataset/FMNIST.pth'))\n",
    "from custom_train import CustomTrainer as CT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "eval =CT(model, device = device)\n",
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_relevance = torch.zeros_like(mat).flatten()\n",
    "map_original = torch.zeros_like(mat).flatten() \n",
    "print(fc1_weight_relevance_list[0][0].shape)\n",
    "map_relevance[ fc1_eps_weight.indices]=  fc1_weight_relevance_list[0][0]\n",
    "map_original[ fc1_eps_weight.indices]= mat.flatten()[ fc1_eps_weight.indices]\n",
    "map_relevance = map_relevance.reshape(mat.shape)\n",
    "map_original = map_original.reshape(mat.shape)\n",
    "print(f\"map_2.shape = {map_relevance.shape}\")\n",
    "\n",
    "map_normalized_relevance = torch.abs(map_relevance)/torch.abs(map_original)\n",
    "map_normalized_relevance = torch.where(torch.isnan(map_normalized_relevance), torch.zeros_like(map_normalized_relevance), map_normalized_relevance)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(map_normalized_relevance.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "map_total_normalised_relevance = []\n",
    "for i in range(100):\n",
    "    map_relevance = torch.zeros_like(mat).flatten()\n",
    "    map_original = torch.zeros_like(mat).flatten() \n",
    "\n",
    "    map_relevance[ fc1_eps_weight.indices]=  fc1_weight_relevance_list[0][0]\n",
    "    map_original[ fc1_eps_weight.indices]= mat.flatten()[ fc1_eps_weight.indices]\n",
    "    map_relevance = map_relevance.reshape(mat.shape)\n",
    "    map_original = map_original.reshape(mat.shape)\n",
    "  \n",
    "\n",
    "    map_normalized_relevance = torch.abs(map_relevance)/torch.abs(map_original)\n",
    "    map_normalized_relevance = torch.where(torch.isnan(map_normalized_relevance), torch.zeros_like(map_normalized_relevance), map_normalized_relevance)\n",
    "    print(torch.min(map_normalized_relevance))\n",
    "    map_total_normalised_relevance.append(map_normalized_relevance)\n",
    "map_total_normalised_relevance= torch.stack(map_total_normalised_relevance)\n",
    "print(map_total_normalised_relevance.shape)\n",
    "map_total_normalised_relevance = torch.sum(map_total_normalised_relevance, dim =0)\n",
    "print(map_total_normalised_relevance.shape)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.max(map_total_normalised_relevance))\n",
    "mask =torch.where(map_total_normalised_relevance>5,1,0)\n",
    "print(torch.sum(mask))\n",
    "mask_1 = torch.ones_like(mask)\n",
    "mask_1.flatten()[fc1_eps_weight.indices] = 0\n",
    "print(torch.sum(mask_1))\n",
    "mask_1 +=mask\n",
    "print(torch.sum(mask_1))\n",
    "\n",
    "model.fc1[1].weight.data *=mask_1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.sum(mask_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.imshow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\"\"\"\n",
    "This is the code to create a video of the prediction and the ground truth\n",
    "It takes the prediction and the ground truth and reshapes them to the original shape of the image.\n",
    "It then creates a video of the prediction and the ground truth.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "array_list_img=np.array(img_list)\n",
    "\n",
    "array_list_fc1=np.array(fc1_weight_relevance_list)\n",
    "\n",
    "fig, ax = plt.subplots(2, 2, figsize = (12, 12),gridspec_kw={'width_ratios': [1,3],'height_ratios': [1, 1.5]})\n",
    "def init():\n",
    "\n",
    "    ax[0,0].cla()\n",
    "    ax[1,0].cla()\n",
    "    ax[1,1].cla()\n",
    "    ax[0,1].cla()\n",
    "   \n",
    "    ax[0,0].imshow(array_list_img[0][0])\n",
    "    map_relevance = torch.zeros_like(mat).flatten()\n",
    "    map_original = torch.zeros_like(mat).flatten() \n",
    "    print(fc1_weight_relevance_list[0][0].shape)\n",
    "    map_relevance[ fc1_eps_weight.indices]=  fc1_weight_relevance_list[0][0]\n",
    "    map_original[ fc1_eps_weight.indices]= mat.flatten()[ fc1_eps_weight.indices]\n",
    "    map_relevance = map_relevance.reshape(mat.shape)\n",
    "    map_original = map_original.reshape(mat.shape)\n",
    " \n",
    "\n",
    "    map_normalized_relevance = torch.abs(map_relevance)/torch.abs(map_original)\n",
    "    map_normalized_relevance = torch.where(torch.isnan(map_normalized_relevance), torch.zeros_like(map_normalized_relevance), map_normalized_relevance)\n",
    "\n",
    "\n",
    "    ax[0,1].imshow(map_normalized_relevance[:,200:400])\n",
    "\n",
    "\n",
    "\n",
    "    result,y_min,y_max,y_true = result_list[0][:4]\n",
    "    y_min       =  np.array(y_min)\n",
    "    y_max       =  np.array(y_max)\n",
    "    center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "    y_true      =  np.expand_dims(np.array(y_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_scale = np.arange(len(y_min))\n",
    "    D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "    \n",
    "    # plot:\n",
    "\n",
    "    \n",
    "    ax[1,1].eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "    ax[1,1].eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "    ax[1,1].eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "    ax[1,1].set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "            ylim=(np.min(D)-1, np.max(D)+1))\n",
    "    ax[1,0].axis('off')\n",
    "  \n",
    "    \n",
    " \n",
    "    return [ax]\n",
    "\n",
    "def update(frame):\n",
    "    ax[0,0].cla()\n",
    "    ax[1,0].cla()\n",
    "    ax[1,1].cla()\n",
    "    ax[0,1].cla()\n",
    "   \n",
    "    ax[0,0].imshow(array_list_img[frame+20][0])\n",
    "    map_relevance = torch.zeros_like(mat).flatten()\n",
    "    map_original = torch.zeros_like(mat).flatten() \n",
    "\n",
    "    map_relevance[ fc1_eps_weight.indices]=  fc1_weight_relevance_list[frame+20][num_label_list[frame+20]]\n",
    "    map_original[ fc1_eps_weight.indices]= mat.flatten()[ fc1_eps_weight.indices]\n",
    "    map_relevance = map_relevance.reshape(mat.shape)\n",
    "    map_original = map_original.reshape(mat.shape)\n",
    " \n",
    "\n",
    "    map_normalized_relevance = torch.abs(map_relevance)/torch.abs(map_original)\n",
    "    map_normalized_relevance = torch.where(torch.isnan(map_normalized_relevance), torch.zeros_like(map_normalized_relevance), map_normalized_relevance)\n",
    "\n",
    "\n",
    "    ax[0,1].imshow(map_normalized_relevance[:,200:400])\n",
    "\n",
    "\n",
    "\n",
    "    result,y_min,y_max,y_true = result_list[frame+20][:4]\n",
    "    y_min       =  np.array(y_min)\n",
    "    y_max       =  np.array(y_max)\n",
    "    center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "    y_true      =  np.expand_dims(np.array(y_true[:])[0],axis =1)\n",
    "\n",
    "\n",
    "\n",
    "       \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    x_scale = np.arange(len(y_min))\n",
    "    D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "    \n",
    "    # plot:\n",
    "\n",
    "    \n",
    "    ax[1,1].eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "    ax[1,1].eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "    ax[1,1].eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "    ax[1,1].set(xlim=(-0.5, 10),xticks=x_scale,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "            ylim=(np.min(D)-1, np.max(D)+1))\n",
    "    ax[1,0].axis('off')\n",
    "    \n",
    " \n",
    "\n",
    "    return [ax]\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=50, init_func=init)\n",
    "\n",
    "output_video = 'Abstract_network_relevance_dominance.mp4'\n",
    "\n",
    "animation.save(output_video, fps=0.5, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "circle_center = (0, 0)\n",
    "circle_radius = 0.8\n",
    "\n",
    "# Création de la figure et de l'axe\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "# Scatter plot\n",
    "ax.scatter(x, y)\n",
    "\n",
    "# Création du cercle\n",
    "circle = plt.Circle(circle_center, circle_radius, color='r', fill=False)\n",
    "\n",
    "# Ajout du cercle à l'axe\n",
    "ax.add_patch(circle)\n",
    "\n",
    "# Définir les limites de l'axe pour s'assurer que le cercle est visible\n",
    "ax.set_xlim(-5, 5)\n",
    "ax.set_ylim(-5, 5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "fig, ax = plt.subplots(1, 1, figsize = (12, 12))\n",
    "\n",
    "x = np.random.randn(1000)\n",
    "y = np.random.randn(1000)\n",
    "x_red =[]\n",
    "y_red =[]\n",
    "\n",
    "\n",
    "def init():\n",
    "\n",
    "    circle_center = (0, 0)\n",
    "    circle_radius = 0.8\n",
    "\n",
    "    # Création de la figure et de l'axe\n",
    "    fig, ax = plt.subplots()\n",
    "\n",
    "    # Scatter plot\n",
    "    ax.scatter(x, y)\n",
    "\n",
    "    # Création du cercle\n",
    "    circle = plt.Circle(circle_center, circle_radius, color='r', fill=False)\n",
    "\n",
    "    # Ajout du cercle à l'axe\n",
    "    ax.add_patch(circle)\n",
    "\n",
    "    # Définir les limites de l'axe pour s'assurer que le cercle est visible\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "    \n",
    " \n",
    "    return [ax]\n",
    "\n",
    "def update(frame):\n",
    "\n",
    "    ax.scatter(x, y,color='blue')\n",
    "\n",
    "\n",
    "    # Définir les limites de l'axe pour s'assurer que le cercle est visible\n",
    "    circle = plt.Circle(circle_center, circle_radius, color='r', fill=False)\n",
    "\n",
    "    # Ajout du cercle à l'axe\n",
    "    ax.add_patch(circle)\n",
    "  \n",
    "\n",
    "    dist = np.sqrt(x**2+y**2)\n",
    "\n",
    "\n",
    "    for el in np.random.randint(0,999,size=10):\n",
    "        if dist[el]<0.8:\n",
    "            x_red.append(x[el])\n",
    "            y_red.append(y[el])\n",
    "    ax.scatter(x_red, y_red,color = 'red')\n",
    "\n",
    "    ax.set_xlim(-3, 3)\n",
    "    ax.set_ylim(-3, 3)\n",
    "\n",
    "    return [ax]\n",
    "\n",
    "\n",
    "animation = FuncAnimation(fig, update, frames=10, init_func=init)\n",
    "\n",
    "output_video = 'Abstract_network_relevance_dominance.mp4'\n",
    "\n",
    "animation.save(output_video, fps=0.5, extra_args=['-vcodec', 'libx264'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()\n",
    "from custom_train import CustomTrainer as CT\n",
    "print(f\"Update parameter after EPOCH {i%10}\")\n",
    "eval =CT(model, device = device)\n",
    "eval.evaluate_model(dataset_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from abstract import abstractTensor as AT\n",
    "from common import make_indice_and_values_tupple,plot_dominance\n",
    "abstract_network = True\n",
    "\n",
    "xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"]\n",
    "\n",
    "img_list = []\n",
    "img_relevance_list =[]\n",
    "conv1_weight_relevance_list = []\n",
    "conv1_bias_relevance_list = []\n",
    "conv2_weight_relevance_list = []\n",
    "conv2_bias_relevance_list = []\n",
    "\n",
    "fc1_weight_relevance_list = []\n",
    "fc1_bias_relevance_list = []\n",
    "fc2_weight_relevance_list = []\n",
    "fc2_bias_relevance_list  = []\n",
    "result_list =[]\n",
    "num_label_list = []\n",
    "name_label_list = []\n",
    "true_predict_list = []\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for i in tqdm(range(500)):\n",
    " \n",
    "   \n",
    "    scale = 0.1\n",
    "    if abstract_network : \n",
    "    \n",
    "        conv1_eps_weight = torch.tensor([])\n",
    "        conv1__weight_span = torch.max(model.conv1.weight.data.flatten())-torch.min(model.conv1.weight.data.flatten())\n",
    "        conv1_eps_weight.indices = torch.arange(0,len(model.conv1.weight.data.flatten()),1)\n",
    "        conv1_eps_weight.values = scale*model.conv1.weight.data.flatten()\n",
    "        conv1_eps_bias = torch.tensor([])\n",
    "        conv1__bias_span = torch.max(model.conv1.bias.data.flatten())-torch.min(model.conv1.bias.data.flatten())\n",
    "        conv1_eps_bias.indices = torch.arange(0,len(model.conv1.bias.data.flatten()),1)\n",
    "        conv1_eps_bias.values = scale*torch.ones_like(model.conv1.bias.data.flatten())\n",
    "\n",
    "\n",
    "        conv2_eps_weight = torch.tensor([])\n",
    "        conv2_eps_weight.indices = torch.arange(0,len(model.conv2.weight.data.flatten()),1)\n",
    "        conv2_weight_span = torch.max(model.conv2.weight.data.flatten())-torch.min(model.conv2.weight.data.flatten())\n",
    "        conv2_eps_weight.values = scale*model.conv2.weight.data.flatten()\n",
    "        conv2_eps_bias = torch.tensor([])\n",
    "        conv2_eps_bias.indices = torch.arange(0,len(model.conv2.bias.data.flatten()),1)\n",
    "        conv2_bias_span = torch.max(model.conv2.bias.data.flatten())-torch.min(model.conv2.bias.data.flatten())\n",
    "        conv2_eps_bias.values = scale*torch.ones_like(model.conv2.bias.data.flatten())\n",
    "\n",
    "        fc1_eps_weight = torch.tensor([])\n",
    "        fc1_weight_span = torch.max(model.fc1[1].weight.data.flatten())-torch.min(model.fc1[1].weight.data.flatten())\n",
    "        fc1_eps_weight.indices = torch.arange(0,len(model.fc1[1].weight.data.flatten()),1)\n",
    "        fc1_eps_weight.values = scale*model.fc1[1].weight.data.flatten()\n",
    "        fc1_eps_bias = torch.tensor([])\n",
    "        fc1_bias_span = torch.max(model.fc1[1].bias.data.flatten())-torch.min(model.fc1[1].bias.data.flatten())\n",
    "        fc1_eps_bias.indices = torch.arange(0,len(model.fc1[1].bias.data.flatten()),1)\n",
    "        fc1_eps_bias.values = scale*torch.ones_like(model.fc1[1].bias.data.flatten())\n",
    "\n",
    "\n",
    "        fc2_eps_weight = torch.tensor([])\n",
    "        fc2_weight_span = torch.max(model.fc2[1].weight.data.flatten())-torch.min(model.fc2[1].weight.data.flatten())\n",
    "        fc2_eps_weight.indices = torch.arange(0,len(model.fc2[1].weight.data.flatten()),1)\n",
    "        fc2_eps_weight.values = scale*model.fc2[1].weight.data.flatten()\n",
    "        fc2_eps_bias = torch.tensor([])\n",
    "        fc2_bias_span = torch.max(model.fc2[1].bias.data.flatten())-torch.min(model.fc2[1].bias.data.flatten())\n",
    "        fc2_eps_bias.indices = torch.arange(0,len(model.fc2[1].bias.data.flatten()),1)\n",
    "        fc2_eps_bias.values = scale*torch.ones_like(model.fc2[1].bias.data.flatten())\n",
    "\n",
    "\n",
    "    else :\n",
    "        conv1_eps_weight = make_indice_and_values_tupple()\n",
    "        conv1_eps_bias = make_indice_and_values_tupple()\n",
    "       \n",
    "\n",
    "        conv2_eps_weight = make_indice_and_values_tupple()\n",
    "        conv2_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_bias = make_indice_and_values_tupple()\n",
    "        fc1_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_weight = make_indice_and_values_tupple()\n",
    "        fc2_eps_bias = make_indice_and_values_tupple()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    label = None\n",
    "\n",
    "  \n",
    "    while label!= i%10: \n",
    "        index = np.random.randint(0,5000)\n",
    "        \n",
    "        img, label = dataset_train[index]\n",
    "\n",
    "    name_label_list.append(xticklabels[label])\n",
    "    num_label_list.append(label)\n",
    "   \n",
    "    img_list.append(img)\n",
    "    \n",
    "    x=AT(img,alpha =0.08*scale*torch.ones(28*28))\n",
    "    x=x.abstract_tensor()\n",
    " \n",
    "\n",
    "    \n",
    "  \n",
    "    with torch.no_grad():\n",
    "      \n",
    "        \n",
    "        result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1=model.abstract_forward(x,\n",
    "                                                                    conv1_eps_bias=conv1_eps_bias,\n",
    "                                                                    conv1_eps_weight=conv1_eps_weight,\n",
    "                                                                    conv2_eps_weight = conv2_eps_weight,\n",
    "                                                                    conv2_eps_bias = conv2_eps_bias,\n",
    "\n",
    "                                                                    fc1_eps_weight =fc1_eps_weight,\n",
    "                                                                    fc1_eps_bias = fc1_eps_bias, \n",
    "                                                                    fc2_eps_weight = fc2_eps_weight,\n",
    "                                                                    fc2_eps_bias = fc2_eps_bias,\n",
    "\n",
    "                                                                    add_symbol=True)\n",
    "        \n",
    "    result_list.append([result,x_min,x_max,x_true,len_symb, len_symb_c2, len_symb_fc1])    \n",
    "    true_predict_list.append(torch.argmax(x_true))\n",
    "    if i%5==0:\n",
    "        plot_dominance(result,x_min,x_max,x_true)\n",
    "    \n",
    "\n",
    "\n",
    "    concatenated_heatmap_tensors =[]\n",
    "    for j in range(10):\n",
    "        concat_part =result[1:785,j]\n",
    "        concatenated_heatmap_tensors.append(concat_part)\n",
    "    concatenated_heatmap_tensors = torch.stack(concatenated_heatmap_tensors)\n",
    "\n",
    "    img_relevance_list.append(concatenated_heatmap_tensors)\n",
    "\n",
    "  \n",
    "    if abstract_network:\n",
    "\n",
    "        conv1_weight_concatenated_tensors = []\n",
    "        conv1_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len(x)-1:len(x)+143, j].view(16, 1, 3, 3)\n",
    "            conv1_weight_concatenated_tensors.append(concat_part)\n",
    "        conv1_weight_concatenated_tensors =torch.stack(conv1_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part= result[len(x)+143:len(x)+159, j]\n",
    "            conv1_bias_concatenated_tensors .append(concat_part)\n",
    "        conv1_bias_concatenated_tensors = torch.stack(conv1_bias_concatenated_tensors )\n",
    "\n",
    "        conv2_weight_concatenated_tensors = []\n",
    "        conv2_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb-1:len_symb+4607, j].view(32, 16, 3, 3)\n",
    "            conv2_weight_concatenated_tensors.append(concat_part)\n",
    "        conv2_weight_concatenated_tensors =torch.stack(conv2_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb+4607:len_symb+4639, j]\n",
    "            conv2_bias_concatenated_tensors.append(concat_part)\n",
    "        conv2_bias_concatenated_tensors = torch.stack(conv2_bias_concatenated_tensors)\n",
    "\n",
    "        fc1_weight_concatenated_tensors = []\n",
    "        fc1_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_c2-1:len_symb_c2+294911, j].reshape(64,4608)\n",
    "            fc1_weight_concatenated_tensors.append(concat_part)\n",
    "        fc1_weight_concatenated_tensors =torch.stack(fc1_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_c2+294911:len_symb_c2+294975, j]\n",
    "            fc1_bias_concatenated_tensors.append(concat_part)\n",
    "        fc1_bias_concatenated_tensors= torch.stack(fc1_bias_concatenated_tensors)\n",
    "\n",
    "        fc2_weight_concatenated_tensors = []\n",
    "        fc2_bias_concatenated_tensors =[]\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_fc1-1:len_symb_fc1+639, j].view(10,64)\n",
    "            fc2_weight_concatenated_tensors.append(concat_part)\n",
    "        fc2_weight_concatenated_tensors =torch.stack(fc2_weight_concatenated_tensors)\n",
    "        for j in range(10):\n",
    "            concat_part = result[len_symb_fc1+639:len_symb_fc1+649, j]\n",
    "            fc2_bias_concatenated_tensors.append(concat_part)\n",
    "        fc2_bias_concatenated_tensors= torch.stack(fc2_bias_concatenated_tensors)\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "    conv1_weight_relevance_list.append(conv1_weight_concatenated_tensors)\n",
    "    conv1_bias_relevance_list.append(conv1_bias_concatenated_tensors)\n",
    "    conv2_weight_relevance_list.append(conv2_weight_concatenated_tensors)\n",
    "\n",
    "    conv2_bias_relevance_list.append(conv2_bias_concatenated_tensors)\n",
    "\n",
    "    fc1_weight_relevance_list.append(fc1_weight_concatenated_tensors)\n",
    "    fc1_bias_relevance_list.append(fc1_bias_concatenated_tensors)\n",
    "    fc2_weight_relevance_list.append(fc2_weight_concatenated_tensors)\n",
    "    fc2_bias_relevance_list.append(fc2_bias_concatenated_tensors)\n",
    "\n",
    "  \n",
    "    k_min = 0\n",
    "    k_plus = 1\n",
    "\n",
    "  \n",
    "    if ((i+1)%10)==0 :\n",
    "        print(f\"Update parameter after EPOCH {i%10}\")\n",
    "        for k in range (10):\n",
    "            model.conv1.weight.data =model.conv1.weight.data+k_plus*(conv1_weight_relevance_list[-10+k][k]-k_min*(torch.sum(conv1_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(conv1_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "            model.conv1.bias.data = model.conv1.bias.data + k_plus*(conv1_bias_relevance_list[-10+k][k]-k_min*(torch.sum(conv1_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(conv1_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "            model.conv2.weight.data =model.conv2.weight.data+k_plus*(conv2_weight_relevance_list[-10+k][k]-k_min*(torch.sum(conv2_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(conv2_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "            model.conv2.bias.data = model.conv2.bias.data + k_plus*(conv2_bias_relevance_list[-10+k][k]-k_min*(torch.sum(conv2_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(conv2_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "            model.fc1[1].weight.data =model.fc1[1].weight.data+k_plus*(fc1_weight_relevance_list[-10+k][k]-k_min*(torch.sum(fc1_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(fc1_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "            model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(fc1_bias_relevance_list[-10+k][k]-k_min*(torch.sum(fc1_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(fc1_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "            model.fc2[1].weight.data =model.fc2[1].weight.data+k_plus*(fc2_weight_relevance_list[-10+k][k]-k_min*(torch.sum(fc2_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(fc2_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "            model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(fc2_bias_relevance_list[-10+k][k]-k_min*(torch.sum(fc2_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(fc2_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.no\n",
    "    \n",
    "        from custom_train import CustomTrainer as CT\n",
    "        print(f\"Update parameter after EPOCH {i%10}\")\n",
    "        eval =CT(model, device = device)\n",
    "        eval.evaluate_model(dataset_test)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_min = 0\n",
    "k_plus = 100000000\n",
    "for j in range(10):\n",
    "\n",
    "    for k in range (10):\n",
    "                model.conv1.weight.data =model.conv1.weight.data+k_plus*(conv1_weight_relevance_list[-10+k][k]-k_min*(torch.sum(conv1_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(conv1_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv1.bias.data = model.conv1.bias.data + k_plus*(conv1_bias_relevance_list[-10+k][k]-k_min*(torch.sum(conv1_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(conv1_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.conv2.weight.data =model.conv2.weight.data+k_plus*(conv2_weight_relevance_list[-10+k][k]-k_min*(torch.sum(conv2_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(conv2_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.conv2.bias.data = model.conv2.bias.data + k_plus*(conv2_bias_relevance_list[-10+k][k]-k_min*(torch.sum(conv2_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(conv2_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.fc1[1].weight.data =model.fc1[1].weight.data+k_plus*(fc1_weight_relevance_list[-10+k][k]-k_min*(torch.sum(fc1_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(fc1_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.fc1[1].bias.data = model.fc1[1].bias.data + k_plus*(fc1_bias_relevance_list[-10+k][k]-k_min*(torch.sum(fc1_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(fc1_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.norm(2*conv1_bias_concatenated_tensors[label]-torch.sum(conv1_bias_concatenated_tensors,dim = 0))\n",
    "\n",
    "                model.fc2[1].weight.data =model.fc2[1].weight.data+k_plus*(fc2_weight_relevance_list[-10+k][k]-k_min*(torch.sum(fc2_weight_relevance_list[-10+k][:k],dim =0)+torch.sum(fc2_weight_relevance_list[-10+k][k+1:],dim =0)))#/torch.norm(2*conv1_weight_concatenated_tensors[label]-torch.sum(conv1_weight_concatenated_tensors,dim =0))\n",
    "                model.fc2[1].bias.data = model.fc2[1].bias.data + k_plus*(fc2_bias_relevance_list[-10+k][k]-k_min*(torch.sum(fc2_bias_relevance_list[-10+k][:k],dim = 0)+torch.sum(fc2_bias_relevance_list[-10+k][k+1:],dim = 0)))#/torch.no\n",
    "                print(torch.sum(conv1_weight_relevance_list[-10+k][k]))\n",
    "    from custom_train import CustomTrainer as CT\n",
    "    print(f\"Update parameter after EPOCH {i%10}\")\n",
    "    eval =CT(model, device = device)\n",
    "    eval.evaluate_model(dataset_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(torch.min(model.conv2.weight.data))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
