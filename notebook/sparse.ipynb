{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 59999, 59999, 59999],\n",
       "                       [    0,     0,     0,  ...,     0,     0,     0],\n",
       "                       [    0,     1,     1,  ...,   220,   223,   223],\n",
       "                       [   12,   135,   180,  ...,   201,   138,   180]]),\n",
       "       values=tensor([3.2747, 3.3523, 2.8170,  ..., 3.2728, 2.7796, 3.0577]),\n",
       "       size=(60000, 1, 224, 224), nnz=10439734, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(60000,1,224,224)\n",
    "card =x.numel()\n",
    "x = torch.where(x>=2.7,x,0)\n",
    "print(x.shape)\n",
    "x = x.to_sparse()\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.65306056016156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-x._nnz()/card)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30108023])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.indices().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[ 0.,  1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0., -1.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0., -2., -3.,  ...,  0.,  0.,  0.],\n",
      "         ...,\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.],\n",
      "         [ 0.,  0.,  0.,  ...,  0.,  0.,  0.]]])\n",
      "\n",
      "Tenseur sparse global:\n",
      "tensor(indices=tensor([[0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],\n",
      "                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "                       [0, 1, 1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 4, 4],\n",
      "                       [1, 1, 1, 3, 1, 3, 1, 3, 2, 4, 2, 4, 2, 4]]),\n",
      "       values=tensor([ 1.,  1., -2.,  2., -2.,  2., -2.,  2., -3.,  3., -3.,\n",
      "                       3., -3.,  3.]),\n",
      "       size=(120000, 1, 448, 448), nnz=14, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "indices = torch.tensor([\n",
    "    [0, 0, 0, 0], [0, 1, 2, 2], [0, 2, 3, 3]\n",
    "\n",
    "])\n",
    "values = torch.tensor([\n",
    "    1.0, 2.0, 3.0\n",
    "\n",
    "])\n",
    "dense_shape = (120000, 1, 448, 448)\n",
    "\n",
    "\n",
    "sparse_tensor = torch.sparse.FloatTensor(indices.t(), values, torch.Size(dense_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.zeros(4,1)\n",
    "test_add =torch.tensor([1,1,1,1])\n",
    "test_add.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 344972])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape\n",
    "for i in range(100):\n",
    "    test = torch.cat((test,test_add.unsqueeze(1)),dim=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 10439734])\n",
      "torch.Size([10439734])\n",
      "dense_shape [60000, 1, 224, 224]\n",
      "[60000, 1, 224, 224]\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/tmp/ipykernel_1846459/757326893.py:39: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
      "  chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([chunk_size, 1, 224, 224]))\n",
      "  2%|▏         | 1/60 [00:00<00:15,  3.86it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1023916])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:19<00:00,  3.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[-1278.8673,   -16.1458,   -31.9146,  ...,    20.4610,\n",
      "             83.8704,  1289.2903],\n",
      "         [-1914.9761,    -5.6840,  -127.9786,  ...,    16.5947,\n",
      "            113.3512,  1914.1028],\n",
      "         [-1890.5922,    42.6622,  -143.8203,  ...,    50.8689,\n",
      "              9.8589,  1898.8342],\n",
      "         ...,\n",
      "         [-1915.4642,    54.5628,   115.1492,  ...,   111.6100,\n",
      "             -4.4437,  1806.7754],\n",
      "         [-2013.4230,   -25.2011,   186.4000,  ...,     3.1551,\n",
      "             64.5561,  1840.2511],\n",
      "         [-1369.9341,    -9.8317,   171.8115,  ...,     4.7723,\n",
      "             90.7835,  1223.1674]]])\n",
      "\n",
      "Tenseur sparse global:\n",
      "tensor(indices=tensor([[    0,     0,     0,  ..., 59999, 59999, 59999],\n",
      "                       [    0,     0,     0,  ...,     0,     0,     0],\n",
      "                       [    0,     0,     0,  ...,   223,   223,   223],\n",
      "                       [   11,    13,   134,  ...,   139,   179,   181]]),\n",
      "       values=tensor([-3.2747,  3.2747, -3.3523,  ...,  2.7796, -3.0577,\n",
      "                       3.0577]),\n",
      "       size=(60000, 1, 224, 224), nnz=61635685, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "indices = x.indices()\n",
    "print(indices.shape)\n",
    "values = x.values()\n",
    "print(values.shape)\n",
    "dense_shape = [shape for shape in x.size()]\n",
    "print(\"dense_shape\",dense_shape)\n",
    "\n",
    "indices =indices.t()\n",
    "print(dense_shape)\n",
    "kernel = torch.tensor([\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0]\n",
    "]).reshape(1, 1, 3, 3)  \n",
    "\n",
    "convolution_sum = None\n",
    "\n",
    "\n",
    "global_indices = None\n",
    "global_values = None\n",
    "\n",
    "\n",
    "chunk_size = 1000\n",
    "num_chunks = dense_shape[0] // chunk_size\n",
    "print(num_chunks)\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(num_chunks)):\n",
    "   \n",
    "    chunk_indices = indices[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    " \n",
    "    chunk_indices[:, 0] -= i * chunk_size  \n",
    "    chunk_values = values[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "\n",
    "    chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([chunk_size, 1, 224, 224]))\n",
    "\n",
    "\n",
    "    chunk_dense_tensor = chunk_sparse_tensor.to_dense()\n",
    "\n",
    "    conv_output = F.conv2d(chunk_dense_tensor, kernel, stride=1, padding=1)\n",
    "\n",
    "    conv_sum = conv_output.sum(dim=0)\n",
    "\n",
    "   \n",
    "    if convolution_sum is None:\n",
    "        convolution_sum = conv_sum\n",
    "    else:\n",
    "        convolution_sum += conv_sum\n",
    "\n",
    "\n",
    "  \n",
    "    conv_output = conv_output.to_sparse()\n",
    "    add_indices=(conv_output.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]).to(conv_output.indices().device))\n",
    "    if global_indices is None:\n",
    "        global_indices= add_indices\n",
    "    else : \n",
    "        global_indices = torch.cat((global_indices,add_indices),dim =1)\n",
    "    #global_indices.append(conv_output.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]))#.to(conv_output_sparse.indices().device))\n",
    "    if global_values is None: \n",
    "        global_values = conv_output.values()\n",
    "        print(global_values.shape)\n",
    "    else: \n",
    "        global_values = torch.cat((global_values,conv_output.values()),dim=0)    \n",
    "    \n",
    "\n",
    "\n",
    "global_sparse_tensor = torch.sparse.FloatTensor(global_indices, global_values, torch.Size(dense_shape))\n",
    "\n",
    "\n",
    "print(\"Somme des convolutions sur la dimension 0:\")\n",
    "print(convolution_sum)\n",
    "\n",
    "print(\"\\nTenseur sparse global:\")\n",
    "print(global_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30106187])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sparse_tensor = torch.sparse.FloatTensor(indices.t(), values, torch.Size(dense_shape))\n",
    "\n",
    "kernel = torch.tensor([\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0]\n",
    "]).reshape(1, 1, 3, 3)  \n",
    "\n",
    "convolution_sum = None\n",
    "\n",
    "\n",
    "global_indices = []\n",
    "global_values = []\n",
    "\n",
    "\n",
    "chunk_size = 100\n",
    "num_chunks = dense_shape[0] // chunk_size\n",
    "\n",
    "for i in range(num_chunks):\n",
    "   \n",
    "    chunk_indices = indices[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "    chunk_indices[:, 0] -= i * chunk_size  \n",
    "    chunk_values = values[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "\n",
    "    chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([chunk_size, 1, 224, 224]))\n",
    "\n",
    "\n",
    "    chunk_dense_tensor = chunk_sparse_tensor.to_dense()\n",
    "\n",
    "    conv_output = F.conv2d(chunk_dense_tensor, kernel, stride=1, padding=1)\n",
    "\n",
    "    conv_sum = conv_output.sum(dim=0)\n",
    "\n",
    "   \n",
    "    if convolution_sum is None:\n",
    "        convolution_sum = conv_sum\n",
    "    else:\n",
    "        convolution_sum += conv_sum\n",
    "\n",
    "  \n",
    "    conv_output_sparse = conv_output.to_sparse()\n",
    "    global_indices.append(conv_output_sparse.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]).to(conv_output_sparse.indices().device))\n",
    "    global_values.append(conv_output_sparse.values())\n",
    "\n",
    "global_indices = torch.cat(global_indices, dim=1)\n",
    "global_values = torch.cat(global_values)\n",
    "global_sparse_tensor = torch.sparse.FloatTensor(global_indices, global_values, torch.Size(dense_shape))\n",
    "\n",
    "\n",
    "print(\"Somme des convolutions sur la dimension 0:\")\n",
    "print(convolution_sum)\n",
    "\n",
    "print(\"\\nTenseur sparse global:\")\n",
    "print(global_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(convolution_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense =x.to_dense()\n",
    "\n",
    "\n",
    "conv_output = F.conv2d(dense, kernel, stride=1, padding=1)\n",
    "del dense\n",
    "conv_output = conv_output.to_sparse()\n",
    "conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],\n",
       "                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 1, 1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 4, 4],\n",
       "                       [1, 1, 1, 3, 1, 3, 1, 3, 2, 4, 2, 4, 2, 4]]),\n",
       "       values=tensor([ 1.,  1., -2.,  2., -2.,  2., -2.,  2., -3.,  3., -3.,\n",
       "                       3., -3.,  3.]),\n",
       "       size=(12000, 1, 224, 224), nnz=14, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx) (1.26.4)\n",
      "Collecting protobuf>=3.20.2\n",
      "  Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 KB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.16.1 protobuf-5.27.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnx2pytorch\n",
      "  Downloading onnx2pytorch-0.4.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (0.17.2)\n",
      "Requirement already satisfied: onnx>=1.6.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (1.16.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx>=1.6.0->onnx2pytorch) (5.27.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx>=1.6.0->onnx2pytorch) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.13.3)\n",
      "Requirement already satisfied: fsspec in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/guillaume/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->onnx2pytorch) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/guillaume/.local/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/guillaume/.local/lib/python3.10/site-packages (from sympy->torch>=1.4.0->onnx2pytorch) (1.3.0)\n",
      "Installing collected packages: onnx2pytorch\n",
      "Successfully installed onnx2pytorch-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx2pytorch\n",
    "\n",
    "from onnx2pytorch import ConvertModel\n",
    "onnx_model = onnx.load('vgg19-7.onnx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic inference of operator: dropout\n",
      "Automatic inference of operator: dropout\n"
     ]
    }
   ],
   "source": [
    "# Convertir le modèle ONNX en modèle PyTorch\n",
    "pytorch_model = ConvertModel(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for param in pytorch_model.named_parameters():\n",
    "    list.append(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conv_vgg0_conv0_fwd.weight',\n",
       " 'Conv_vgg0_conv0_fwd.bias',\n",
       " 'Conv_vgg0_conv1_fwd.weight',\n",
       " 'Conv_vgg0_conv1_fwd.bias',\n",
       " 'Conv_vgg0_conv2_fwd.weight',\n",
       " 'Conv_vgg0_conv2_fwd.bias',\n",
       " 'Conv_vgg0_conv3_fwd.weight',\n",
       " 'Conv_vgg0_conv3_fwd.bias',\n",
       " 'Conv_vgg0_conv4_fwd.weight',\n",
       " 'Conv_vgg0_conv4_fwd.bias',\n",
       " 'Conv_vgg0_conv5_fwd.weight',\n",
       " 'Conv_vgg0_conv5_fwd.bias',\n",
       " 'Conv_vgg0_conv6_fwd.weight',\n",
       " 'Conv_vgg0_conv6_fwd.bias',\n",
       " 'Conv_vgg0_conv7_fwd.weight',\n",
       " 'Conv_vgg0_conv7_fwd.bias',\n",
       " 'Conv_vgg0_conv8_fwd.weight',\n",
       " 'Conv_vgg0_conv8_fwd.bias',\n",
       " 'Conv_vgg0_conv9_fwd.weight',\n",
       " 'Conv_vgg0_conv9_fwd.bias',\n",
       " 'Conv_vgg0_conv10_fwd.weight',\n",
       " 'Conv_vgg0_conv10_fwd.bias',\n",
       " 'Conv_vgg0_conv11_fwd.weight',\n",
       " 'Conv_vgg0_conv11_fwd.bias',\n",
       " 'Conv_vgg0_conv12_fwd.weight',\n",
       " 'Conv_vgg0_conv12_fwd.bias',\n",
       " 'Conv_vgg0_conv13_fwd.weight',\n",
       " 'Conv_vgg0_conv13_fwd.bias',\n",
       " 'Conv_vgg0_conv14_fwd.weight',\n",
       " 'Conv_vgg0_conv14_fwd.bias',\n",
       " 'Conv_vgg0_conv15_fwd.weight',\n",
       " 'Conv_vgg0_conv15_fwd.bias',\n",
       " 'Gemm_vgg0_dense0_fwd.weight',\n",
       " 'Gemm_vgg0_dense0_fwd.bias',\n",
       " 'Gemm_vgg0_dense1_fwd.weight',\n",
       " 'Gemm_vgg0_dense1_fwd.bias',\n",
       " 'Gemm_vgg0_dense2_fwd.weight',\n",
       " 'Gemm_vgg0_dense2_fwd.bias']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout() missing 2 required positional argument: \"p\", \"train\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m pytorch_model\u001b[38;5;241m=\u001b[39mpytorch_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpytorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/onnx2pytorch/convert/model.py:224\u001b[0m, in \u001b[0;36mConvertModel.forward\u001b[0;34m(self, *input_list, **input_dict)\u001b[0m\n\u001b[1;32m    222\u001b[0m         activations[out_op_id] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     activations[out_op_id] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Remove activations that are no longer needed\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m in_op_id \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minput:\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout() missing 2 required positional argument: \"p\", \"train\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "image = Image.open('voilier.jpeg')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))\n",
    "   \n",
    "    \n",
    "])\n",
    "\n",
    "image = transform(image)\n",
    "print(image.shape)\n",
    "pytorch_model=pytorch_model.eval()\n",
    "pytorch_model(image.unsqueeze(0))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
