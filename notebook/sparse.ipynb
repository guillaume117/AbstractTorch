{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([6000, 1, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[   0,    0,    0,  ..., 5999, 5999, 5999],\n",
       "                       [   0,    0,    0,  ...,    0,    0,    0],\n",
       "                       [   2,    5,    5,  ...,  220,  222,  222],\n",
       "                       [  31,   68,  185,  ...,   19,   35,  121]]),\n",
       "       values=tensor([2.8037, 2.8234, 3.1526,  ..., 2.8346, 3.6942, 2.7745]),\n",
       "       size=(6000, 1, 224, 224), nnz=1043903, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.randn(6000,1,224,224)\n",
    "card =x.numel()\n",
    "x = torch.where(x>=2.7,x,0)\n",
    "print(x.shape)\n",
    "x = x.to_sparse()\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "99.65306056016156"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1-x._nnz()/card)*100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30108023])"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.indices().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "indices = torch.tensor([\n",
    "    [0, 0, 0, 0], [0, 1, 2, 2], [0, 2, 3, 3]\n",
    "\n",
    "])\n",
    "values = torch.tensor([\n",
    "    1.0, 2.0, 3.0\n",
    "\n",
    "])\n",
    "dense_shape = (1200, 1, 448, 448)\n",
    "\n",
    "\n",
    "sparse_tensor = torch.sparse.FloatTensor(indices.t(), values, torch.Size(dense_shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test = torch.zeros(4,1)\n",
    "test_add =torch.tensor([1,1,1,1])\n",
    "test_add.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 344972])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape\n",
    "for i in range(100):\n",
    "    test = torch.cat((test,test_add.unsqueeze(1)),dim=1)\n",
    "test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([4, 1043903])\n",
      "torch.Size([1043903])\n",
      "dense_shape [6000, 1, 224, 224]\n",
      "[6000, 1, 224, 224]\n",
      "60\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  7%|▋         | 4/60 [00:00<00:01, 39.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([102719])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:02<00:00, 28.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[-1.1610e+02,  2.1572e+01, -5.5384e+01,  ..., -4.0113e+01,\n",
      "          -2.0901e+01,  1.4180e+02],\n",
      "         [-1.8457e+02,  3.0984e+01, -3.3409e+01,  ..., -4.6547e+01,\n",
      "           7.8012e-02,  2.0706e+02],\n",
      "         [-2.0818e+02,  1.7288e+01,  1.3909e+01,  ..., -4.0940e+01,\n",
      "          -1.2262e+01,  2.1060e+02],\n",
      "         ...,\n",
      "         [-1.6993e+02,  5.2541e+01, -2.6657e+01,  ..., -4.7880e+01,\n",
      "           3.0570e+01,  1.9438e+02],\n",
      "         [-1.7162e+02,  1.4624e+01, -5.8739e+01,  ..., -1.8731e+01,\n",
      "           2.7954e+01,  1.8683e+02],\n",
      "         [-1.0731e+02,  3.5705e+01, -4.8175e+01,  ..., -6.6451e+00,\n",
      "           2.4412e+00,  1.1077e+02]]])\n",
      "\n",
      "Tenseur sparse global:\n",
      "tensor(indices=tensor([[   0,    0,    0,  ..., 5999, 5999, 5999],\n",
      "                       [   0,    0,    0,  ...,    0,    0,    0],\n",
      "                       [   1,    1,    2,  ...,  223,  223,  223],\n",
      "                       [  30,   32,   30,  ...,   36,  120,  122]]),\n",
      "       values=tensor([-2.8037,  2.8037, -2.8037,  ...,  3.6942, -2.7745,\n",
      "                       2.7745]),\n",
      "       size=(6000, 1, 224, 224), nnz=6163777, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "\n",
    "\n",
    "indices = x.indices()\n",
    "print(indices.shape)\n",
    "values = x.values()\n",
    "print(values.shape)\n",
    "dense_shape = [shape for shape in x.size()]\n",
    "print(\"dense_shape\",dense_shape)\n",
    "\n",
    "indices =indices.t()\n",
    "print(dense_shape)\n",
    "kernel = torch.tensor([\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0]\n",
    "]).reshape(1, 1, 3, 3)  \n",
    "\n",
    "convolution_sum = None\n",
    "\n",
    "\n",
    "global_indices = None\n",
    "global_values = None\n",
    "\n",
    "\n",
    "chunk_size = 100\n",
    "num_chunks = dense_shape[0] // chunk_size\n",
    "print(num_chunks)\n",
    "from tqdm import tqdm\n",
    "\n",
    "for i in tqdm(range(num_chunks)):\n",
    "   \n",
    "    chunk_indices = indices[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    " \n",
    "    chunk_indices[:, 0] -= i * chunk_size  \n",
    "    chunk_values = values[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "\n",
    "    chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([chunk_size, 1, 224, 224]))\n",
    "\n",
    "\n",
    "    chunk_dense_tensor = chunk_sparse_tensor.to_dense()\n",
    "\n",
    "    conv_output = F.conv2d(chunk_dense_tensor, kernel, stride=1, padding=1)\n",
    "\n",
    "    conv_sum = conv_output.sum(dim=0)\n",
    "\n",
    "   \n",
    "    if convolution_sum is None:\n",
    "        convolution_sum = conv_sum\n",
    "    else:\n",
    "        convolution_sum += conv_sum\n",
    "\n",
    "\n",
    "  \n",
    "    conv_output = conv_output.to_sparse()\n",
    "    add_indices=(conv_output.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]).to(conv_output.indices().device))\n",
    "    if global_indices is None:\n",
    "        global_indices= add_indices\n",
    "    else : \n",
    "        global_indices = torch.cat((global_indices,add_indices),dim =1)\n",
    "    #global_indices.append(conv_output.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]))#.to(conv_output_sparse.indices().device))\n",
    "    if global_values is None: \n",
    "        global_values = conv_output.values()\n",
    "        print(global_values.shape)\n",
    "    else: \n",
    "        global_values = torch.cat((global_values,conv_output.values()),dim=0)    \n",
    "    \n",
    "\n",
    "\n",
    "global_sparse_tensor = torch.sparse.FloatTensor(global_indices, global_values, torch.Size(dense_shape))\n",
    "\n",
    "\n",
    "print(\"Somme des convolutions sur la dimension 0:\")\n",
    "print(convolution_sum)\n",
    "\n",
    "print(\"\\nTenseur sparse global:\")\n",
    "print(global_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 30106187])"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "sparse_tensor = torch.sparse.FloatTensor(indices.t(), values, torch.Size(dense_shape))\n",
    "\n",
    "kernel = torch.tensor([\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0],\n",
    "    [1.0, 0.0, -1.0]\n",
    "]).reshape(1, 1, 3, 3)  \n",
    "\n",
    "convolution_sum = None\n",
    "\n",
    "\n",
    "global_indices = []\n",
    "global_values = []\n",
    "\n",
    "\n",
    "chunk_size = 100\n",
    "num_chunks = dense_shape[0] // chunk_size\n",
    "\n",
    "for i in range(num_chunks):\n",
    "   \n",
    "    chunk_indices = indices[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "    chunk_indices[:, 0] -= i * chunk_size  \n",
    "    chunk_values = values[(indices[:, 0] >= i * chunk_size) & (indices[:, 0] < (i + 1) * chunk_size)]\n",
    "\n",
    "    chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([chunk_size, 1, 224, 224]))\n",
    "\n",
    "\n",
    "    chunk_dense_tensor = chunk_sparse_tensor.to_dense()\n",
    "\n",
    "    conv_output = F.conv2d(chunk_dense_tensor, kernel, stride=1, padding=1)\n",
    "\n",
    "    conv_sum = conv_output.sum(dim=0)\n",
    "\n",
    "   \n",
    "    if convolution_sum is None:\n",
    "        convolution_sum = conv_sum\n",
    "    else:\n",
    "        convolution_sum += conv_sum\n",
    "\n",
    "  \n",
    "    conv_output_sparse = conv_output.to_sparse()\n",
    "    global_indices.append(conv_output_sparse.indices() + torch.tensor([[i * chunk_size], [0], [0], [0]]).to(conv_output_sparse.indices().device))\n",
    "    global_values.append(conv_output_sparse.values())\n",
    "\n",
    "global_indices = torch.cat(global_indices, dim=1)\n",
    "global_values = torch.cat(global_values)\n",
    "global_sparse_tensor = torch.sparse.FloatTensor(global_indices, global_values, torch.Size(dense_shape))\n",
    "\n",
    "\n",
    "print(\"Somme des convolutions sur la dimension 0:\")\n",
    "print(convolution_sum)\n",
    "\n",
    "print(\"\\nTenseur sparse global:\")\n",
    "print(global_sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "print(convolution_sum.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense =x.to_dense()\n",
    "\n",
    "\n",
    "conv_output = F.conv2d(dense, kernel, stride=1, padding=1)\n",
    "del dense\n",
    "conv_output = conv_output.to_sparse()\n",
    "conv_output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[0, 0, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2],\n",
       "                       [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
       "                       [0, 1, 1, 1, 2, 2, 3, 3, 2, 2, 3, 3, 4, 4],\n",
       "                       [1, 1, 1, 3, 1, 3, 1, 3, 2, 4, 2, 4, 2, 4]]),\n",
       "       values=tensor([ 1.,  1., -2.,  2., -2.,  2., -2.,  2., -3.,  3., -3.,\n",
       "                       3., -3.,  3.]),\n",
       "       size=(12000, 1, 224, 224), nnz=14, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnx\n",
      "  Downloading onnx-1.16.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (15.9 MB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m15.9/15.9 MB\u001b[0m \u001b[31m88.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0mm eta \u001b[36m0:00:01\u001b[0m0:01\u001b[0m01\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: numpy>=1.20 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx) (1.26.4)\n",
      "Collecting protobuf>=3.20.2\n",
      "  Downloading protobuf-5.27.0-cp38-abi3-manylinux2014_x86_64.whl (309 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m309.2/309.2 KB\u001b[0m \u001b[31m44.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: protobuf, onnx\n",
      "Successfully installed onnx-1.16.1 protobuf-5.27.0\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx\n",
    "import onnx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Collecting onnx2pytorch\n",
      "  Downloading onnx2pytorch-0.4.1-py3-none-any.whl (44 kB)\n",
      "\u001b[2K     \u001b[38;2;114;156;31m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.9/44.9 KB\u001b[0m \u001b[31m1.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hRequirement already satisfied: torch>=1.4.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (2.2.2)\n",
      "Requirement already satisfied: torchvision>=0.9.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (0.17.2)\n",
      "Requirement already satisfied: onnx>=1.6.0 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx2pytorch) (1.16.1)\n",
      "Requirement already satisfied: protobuf>=3.20.2 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx>=1.6.0->onnx2pytorch) (5.27.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/guillaume/.local/lib/python3.10/site-packages (from onnx>=1.6.0->onnx2pytorch) (1.26.4)\n",
      "Requirement already satisfied: filelock in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.13.3)\n",
      "Requirement already satisfied: fsspec in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2024.3.1)\n",
      "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (11.0.2.54)\n",
      "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (8.9.2.26)\n",
      "Requirement already satisfied: triton==2.2.0 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2.2.0)\n",
      "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (10.3.2.106)\n",
      "Requirement already satisfied: networkx in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (4.10.0)\n",
      "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: nvidia-nccl-cu12==2.19.3 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (2.19.3)\n",
      "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.0.106)\n",
      "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.3.1)\n",
      "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (12.1.105)\n",
      "Requirement already satisfied: sympy in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (1.12)\n",
      "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (11.4.5.107)\n",
      "Requirement already satisfied: jinja2 in /home/guillaume/.local/lib/python3.10/site-packages (from torch>=1.4.0->onnx2pytorch) (3.1.3)\n",
      "Requirement already satisfied: nvidia-nvjitlink-cu12 in /home/guillaume/.local/lib/python3.10/site-packages (from nvidia-cusolver-cu12==11.4.5.107->torch>=1.4.0->onnx2pytorch) (12.4.127)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/lib/python3/dist-packages (from torchvision>=0.9.0->onnx2pytorch) (9.0.1)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/guillaume/.local/lib/python3.10/site-packages (from jinja2->torch>=1.4.0->onnx2pytorch) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in /home/guillaume/.local/lib/python3.10/site-packages (from sympy->torch>=1.4.0->onnx2pytorch) (1.3.0)\n",
      "Installing collected packages: onnx2pytorch\n",
      "Successfully installed onnx2pytorch-0.4.1\n"
     ]
    }
   ],
   "source": [
    "!pip install onnx2pytorch\n",
    "\n",
    "from onnx2pytorch import ConvertModel\n",
    "onnx_model = onnx.load('vgg19-7.onnx')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Automatic inference of operator: dropout\n",
      "Automatic inference of operator: dropout\n"
     ]
    }
   ],
   "source": [
    "# Convertir le modèle ONNX en modèle PyTorch\n",
    "pytorch_model = ConvertModel(onnx_model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = []\n",
    "for param in pytorch_model.named_parameters():\n",
    "    list.append(param[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Conv_vgg0_conv0_fwd.weight',\n",
       " 'Conv_vgg0_conv0_fwd.bias',\n",
       " 'Conv_vgg0_conv1_fwd.weight',\n",
       " 'Conv_vgg0_conv1_fwd.bias',\n",
       " 'Conv_vgg0_conv2_fwd.weight',\n",
       " 'Conv_vgg0_conv2_fwd.bias',\n",
       " 'Conv_vgg0_conv3_fwd.weight',\n",
       " 'Conv_vgg0_conv3_fwd.bias',\n",
       " 'Conv_vgg0_conv4_fwd.weight',\n",
       " 'Conv_vgg0_conv4_fwd.bias',\n",
       " 'Conv_vgg0_conv5_fwd.weight',\n",
       " 'Conv_vgg0_conv5_fwd.bias',\n",
       " 'Conv_vgg0_conv6_fwd.weight',\n",
       " 'Conv_vgg0_conv6_fwd.bias',\n",
       " 'Conv_vgg0_conv7_fwd.weight',\n",
       " 'Conv_vgg0_conv7_fwd.bias',\n",
       " 'Conv_vgg0_conv8_fwd.weight',\n",
       " 'Conv_vgg0_conv8_fwd.bias',\n",
       " 'Conv_vgg0_conv9_fwd.weight',\n",
       " 'Conv_vgg0_conv9_fwd.bias',\n",
       " 'Conv_vgg0_conv10_fwd.weight',\n",
       " 'Conv_vgg0_conv10_fwd.bias',\n",
       " 'Conv_vgg0_conv11_fwd.weight',\n",
       " 'Conv_vgg0_conv11_fwd.bias',\n",
       " 'Conv_vgg0_conv12_fwd.weight',\n",
       " 'Conv_vgg0_conv12_fwd.bias',\n",
       " 'Conv_vgg0_conv13_fwd.weight',\n",
       " 'Conv_vgg0_conv13_fwd.bias',\n",
       " 'Conv_vgg0_conv14_fwd.weight',\n",
       " 'Conv_vgg0_conv14_fwd.bias',\n",
       " 'Conv_vgg0_conv15_fwd.weight',\n",
       " 'Conv_vgg0_conv15_fwd.bias',\n",
       " 'Gemm_vgg0_dense0_fwd.weight',\n",
       " 'Gemm_vgg0_dense0_fwd.bias',\n",
       " 'Gemm_vgg0_dense1_fwd.weight',\n",
       " 'Gemm_vgg0_dense1_fwd.bias',\n",
       " 'Gemm_vgg0_dense2_fwd.weight',\n",
       " 'Gemm_vgg0_dense2_fwd.bias']"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([3, 224, 224])\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "dropout() missing 2 required positional argument: \"p\", \"train\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(image\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m     15\u001b[0m pytorch_model\u001b[38;5;241m=\u001b[39mpytorch_model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m---> 16\u001b[0m \u001b[43mpytorch_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43munsqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/onnx2pytorch/convert/model.py:224\u001b[0m, in \u001b[0;36mConvertModel.forward\u001b[0;34m(self, *input_list, **input_dict)\u001b[0m\n\u001b[1;32m    222\u001b[0m         activations[out_op_id] \u001b[38;5;241m=\u001b[39m output\n\u001b[1;32m    223\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 224\u001b[0m     activations[out_op_id] \u001b[38;5;241m=\u001b[39m \u001b[43mop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43min_activations\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;66;03m# Remove activations that are no longer needed\u001b[39;00m\n\u001b[1;32m    227\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m in_op_id \u001b[38;5;129;01min\u001b[39;00m node\u001b[38;5;241m.\u001b[39minput:\n",
      "\u001b[0;31mTypeError\u001b[0m: dropout() missing 2 required positional argument: \"p\", \"train\""
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import torchvision.transforms as transforms\n",
    "image = Image.open('voilier.jpeg')\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Resize((224, 224))\n",
    "   \n",
    "    \n",
    "])\n",
    "\n",
    "image = transform(image)\n",
    "print(image.shape)\n",
    "pytorch_model=pytorch_model.eval()\n",
    "pytorch_model(image.unsqueeze(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([600, 64, 224, 224])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[  0,   0,   0,  ..., 599, 599, 599],\n",
       "                       [  0,   1,   2,  ...,  61,  61,  62],\n",
       "                       [147, 221,  46,  ...,  65,  96, 178],\n",
       "                       [ 57,  24, 100,  ..., 183,  14, 106]]),\n",
       "       values=tensor([4.1743, 4.8182, 4.3344,  ..., 4.1460, 4.1132, 4.3921]),\n",
       "       size=(600, 64, 224, 224), nnz=61182, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sparse import SparseEvaluation\n",
    "x = torch.randn(600,64,224,224)\n",
    "card =x.numel()\n",
    "x = torch.where(x>=4,x,0)\n",
    "print(x.shape)\n",
    "x = x.to_sparse()\n",
    "x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    conv = nn.Conv2d(in_channels=64, out_channels=128,kernel_size=3)\n",
    "function = lambda x: x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[600, 64, 224, 224]\n",
      "Sparse evaluation, num_chunks 60\n",
      "1\n",
      "torch.Size([4, 61182])\n",
      "torch.Size([61182])\n",
      "dense_shape [600, 64, 224, 224]\n",
      "[600, 64, 224, 224]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/60 [00:00<?, ?it/s]/home/guillaume/Documents/AbstractTorch/AbstractTorch/notebook/../src/sparse.py:59: UserWarning: torch.sparse.SparseTensor(indices, values, shape, *, device=) is deprecated.  Please use torch.sparse_coo_tensor(indices, values, shape, dtype=, device=). (Triggered internally at ../torch/csrc/utils/tensor_new.cpp:618.)\n",
      "  chunk_sparse_tensor = torch.sparse.FloatTensor(chunk_indices.t(), chunk_values, torch.Size([self.chunk_size, self.dense_shape[1],self.dense_shape[2], self.dense_shape[3]]))\n",
      "  8%|▊         | 5/60 [00:00<00:01, 41.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1027])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 60/60 [00:01<00:00, 39.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]],\n",
      "\n",
      "        [[0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         ...,\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.],\n",
      "         [0., 0., 0.,  ..., 0., 0., 0.]]])\n",
      "\n",
      "Tenseur sparse global:\n",
      "tensor(indices=tensor([[  0,   0,   0,  ..., 599, 599, 599],\n",
      "                       [  0,   1,   2,  ...,  61,  61,  62],\n",
      "                       [147, 221,  46,  ...,  65,  96, 178],\n",
      "                       [ 57,  24, 100,  ..., 183,  14, 106]]),\n",
      "       values=tensor([4.1743, 4.8182, 4.3344,  ..., 4.1460, 4.1132, 4.3921]),\n",
      "       size=(600, 64, 224, 224), nnz=61182, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "test = SparseEvaluation(x,function=function,chunck_size=10)\n",
    "with torch.no_grad():\n",
    "    test.evaluate_all_chuncks()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(indices=tensor([[   0,    1,    2,  ..., 9072, 9073, 9074],\n",
      "                       [   0,    0,    0,  ...,    2,    2,    2],\n",
      "                       [   0,    0,    0,  ...,   54,   54,   54],\n",
      "                       [   0,    1,    2,  ...,   52,   53,   54]]),\n",
      "       values=tensor([1.0000e-05, 1.0000e-05, 1.0000e-05,  ...,\n",
      "                      1.0000e-05, 1.0000e-05, 1.0000e-05]),\n",
      "       size=(9075, 3, 55, 55), nnz=9075, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = torch.device(\"cpu\")\n",
    "dim_H = 55\n",
    "dim_W = 55\n",
    "num_chanel = 3\n",
    "def create_sparse_tensor(alpha, dims=(num_chanel*dim_H*dim_W, num_chanel, dim_W, dim_H)):\n",
    "    # Liste pour stocker les indices des éléments non nuls\n",
    "    indices = []\n",
    "    # Liste pour stocker les valeurs des éléments non nuls\n",
    "    values = []\n",
    "\n",
    "    # Nombre d'éléments non nuls\n",
    "    num_elements = dims[0]\n",
    "\n",
    "    # Boucle pour créer les indices et les valeurs\n",
    "    for i in range(num_elements):\n",
    "        # Calcul des indices pour les dimensions (3, 128, 128)\n",
    "        dim_3 = i // (dim_W * dim_H)\n",
    "        rem = i % (dim_W * dim_H)\n",
    "        dim_1 = rem // dim_W\n",
    "        dim_2 = rem % dim_H\n",
    "\n",
    "        # Indices des éléments non nuls\n",
    "        indices.append([i, dim_3, dim_1, dim_2])\n",
    "        # Valeurs des éléments non nuls\n",
    "        values.append(alpha)\n",
    "\n",
    "    # Conversion des listes en tenseurs PyTorch\n",
    "    indices_tensor = torch.tensor(indices, dtype=torch.int32).t()\n",
    "    values_tensor = torch.tensor(values, dtype=torch.float)\n",
    "\n",
    "    # Création du tenseur épars\n",
    "    sparse_tensor = torch.sparse_coo_tensor(indices_tensor, values_tensor, size=dims)\n",
    "\n",
    "    return sparse_tensor\n",
    "\n",
    "# Exemple d'utilisation\n",
    "alpha = 0.00001\n",
    "sparse_tensor = create_sparse_tensor(alpha).coalesce().to(device)\n",
    "print(sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.sparse import FloatTensor\n",
    "from typing import Callable\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class SparseEvaluation:\n",
    "    def __init__(self, x: FloatTensor, chunk_size: int, mask_coef : FloatTensor = None , function: Callable = None):\n",
    "        self.x = x\n",
    "        if function is None : \n",
    "            self.function = lambda x: x \n",
    "        else: \n",
    "            self.function = function\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dense_shape = list(x.size())\n",
    "        x0 = torch.zeros(1, self.dense_shape[1], self.dense_shape[2], self.dense_shape[3]).to(device)\n",
    "        if mask_coef is None: \n",
    "            self.mask_coef = torch.ones_like(x0)\n",
    "        else: self.mask_coef = mask_coef\n",
    "        \n",
    "\n",
    "        # Nombre de chunks\n",
    "        self.num_chunks = (self.dense_shape[0] + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        # Taille de sortie de la fonction\n",
    "        \n",
    "        self.output_size = list(self.function(x0).shape)\n",
    "        self.output_size[0] = self.dense_shape[0]\n",
    "\n",
    "        self.indices = self.x.indices()\n",
    "        self.values = self.x.values()\n",
    "\n",
    "        self.indices = self.indices.t()\n",
    "        self.function_sum = None\n",
    "        self.global_storage = {\n",
    "            'indices': [],\n",
    "            'values': []\n",
    "        }\n",
    "\n",
    "    def evaluate_all_chunks(self):\n",
    "        with torch.no_grad():\n",
    "            from tqdm import tqdm\n",
    "            for i in tqdm(range(self.num_chunks)):\n",
    "                chunk_start = i * self.chunk_size\n",
    "                chunk_end = min((i + 1) * self.chunk_size, self.dense_shape[0])\n",
    "                mask = (self.indices[:, 0] >= chunk_start) & (self.indices[:, 0] < chunk_end)\n",
    "\n",
    "                chunk_indices = self.indices[mask]\n",
    "                chunk_indices[:, 0] -= chunk_start\n",
    "\n",
    "                chunk_values = self.values[mask]\n",
    "                chunk_size = chunk_end - chunk_start\n",
    "\n",
    "                chunk_sparse_tensor = torch.sparse.FloatTensor(\n",
    "                    chunk_indices.t(), chunk_values,\n",
    "                    torch.Size([chunk_size] + self.dense_shape[1:])\n",
    "                )\n",
    "\n",
    "                chunk_dense_tensor = chunk_sparse_tensor.to_dense().to(device)\n",
    "                func_output = self.function(self.mask_coef *chunk_dense_tensor)\n",
    "\n",
    "           \n",
    "                func_sum = torch.abs(func_output).sum(dim=0)\n",
    "                if self.function_sum is None:\n",
    "                    self.function_sum = func_sum\n",
    "                else:\n",
    "                    self.function_sum += func_sum\n",
    "\n",
    "             \n",
    "                func_output_sparse = func_output.to_sparse()\n",
    "                add_indices = func_output_sparse.indices().to(torch.int32) + torch.tensor(\n",
    "                    [[chunk_start], [0], [0], [0]], dtype=torch.int32, device=device\n",
    "                )\n",
    "\n",
    "                self.global_storage['indices'].append(add_indices.cpu())\n",
    "                self.global_storage['values'].append(func_output_sparse.values().cpu())\n",
    "\n",
    "\n",
    "                del chunk_dense_tensor, chunk_sparse_tensor, func_output, func_output_sparse, add_indices\n",
    "                torch.cuda.empty_cache()\n",
    "\n",
    "            # Concaténation finale des indices et valeurs\n",
    "            global_indices = torch.cat(self.global_storage['indices'], dim=1)\n",
    "            global_values = torch.cat(self.global_storage['values'], dim=0)\n",
    "\n",
    "            global_sparse_tensor = torch.sparse_coo_tensor(global_indices, global_values, size=self.output_size)\n",
    "            #global_sparse_tensor = torch.sparse.FloatTensor(\n",
    "             #   global_indices, global_values, self.output_size\n",
    "            #)\n",
    "\n",
    "            print(\"Somme des convolutions sur la dimension 0:\")\n",
    "            print(self.function_sum)\n",
    "\n",
    "            print(\"\\nTenseur sparse global:\")\n",
    "            return global_sparse_tensor,self.function_sum\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "with torch.no_grad():\n",
    "    conv = nn.Conv2d(in_channels=3, out_channels=64,kernel_size=3).to(device)\n",
    "    conv.bias.data = torch.zeros_like(conv.bias.data)\n",
    "    conv2 = nn.Conv2d(in_channels=64, out_channels=128,kernel_size=3).to(device)\n",
    "    conv2.bias.data = torch.zeros_like(conv2.bias.data)\n",
    "function = lambda x: conv(x)\n",
    "function2 = lambda x: conv2(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/46 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 46/46 [00:01<00:00, 27.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05],\n",
      "         [2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05],\n",
      "         [2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05],\n",
      "         ...,\n",
      "         [2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05],\n",
      "         [2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05],\n",
      "         [2.4665e-05, 2.4665e-05, 2.4665e-05,  ..., 2.4665e-05,\n",
      "          2.4665e-05, 2.4665e-05]],\n",
      "\n",
      "        [[2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05],\n",
      "         [2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05],\n",
      "         [2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05],\n",
      "         ...,\n",
      "         [2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05],\n",
      "         [2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05],\n",
      "         [2.5257e-05, 2.5257e-05, 2.5257e-05,  ..., 2.5257e-05,\n",
      "          2.5257e-05, 2.5257e-05]],\n",
      "\n",
      "        [[2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05],\n",
      "         [2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05],\n",
      "         [2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05],\n",
      "         ...,\n",
      "         [2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05],\n",
      "         [2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05],\n",
      "         [2.6665e-05, 2.6665e-05, 2.6665e-05,  ..., 2.6665e-05,\n",
      "          2.6665e-05, 2.6665e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05],\n",
      "         [2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05],\n",
      "         [2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05],\n",
      "         ...,\n",
      "         [2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05],\n",
      "         [2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05],\n",
      "         [2.4446e-05, 2.4446e-05, 2.4446e-05,  ..., 2.4446e-05,\n",
      "          2.4446e-05, 2.4446e-05]],\n",
      "\n",
      "        [[2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05],\n",
      "         [2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05],\n",
      "         [2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05],\n",
      "         ...,\n",
      "         [2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05],\n",
      "         [2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05],\n",
      "         [2.3941e-05, 2.3941e-05, 2.3941e-05,  ..., 2.3941e-05,\n",
      "          2.3941e-05, 2.3941e-05]],\n",
      "\n",
      "        [[2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05],\n",
      "         [2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05],\n",
      "         [2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05],\n",
      "         ...,\n",
      "         [2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05],\n",
      "         [2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05],\n",
      "         [2.7552e-05, 2.7552e-05, 2.7552e-05,  ..., 2.7552e-05,\n",
      "          2.7552e-05, 2.7552e-05]]])\n",
      "\n",
      "Tenseur sparse global:\n"
     ]
    }
   ],
   "source": [
    "import timeit\n",
    "test = SparseEvaluation(sparse_tensor,function=function,chunk_size=200)\n",
    "with torch.no_grad():\n",
    "    result,sum = test.evaluate_all_chunks()\n",
    "    result = result.coalesce()\n",
    "\n",
    "    result_2 = function(sparse_tensor.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.sum(result_2-result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[[2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176],\n",
      "         [2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176],\n",
      "         [2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176],\n",
      "         ...,\n",
      "         [2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176],\n",
      "         [2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176],\n",
      "         [2.8176, 2.8176, 2.8176,  ..., 2.8176, 2.8176, 2.8176]],\n",
      "\n",
      "        [[2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920],\n",
      "         [2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920],\n",
      "         [2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920],\n",
      "         ...,\n",
      "         [2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920],\n",
      "         [2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920],\n",
      "         [2.5920, 2.5920, 2.5920,  ..., 2.5920, 2.5920, 2.5920]],\n",
      "\n",
      "        [[2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387],\n",
      "         [2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387],\n",
      "         [2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387],\n",
      "         ...,\n",
      "         [2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387],\n",
      "         [2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387],\n",
      "         [2.3387, 2.3387, 2.3387,  ..., 2.3387, 2.3387, 2.3387]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         ...,\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226],\n",
      "         [2.6226, 2.6226, 2.6226,  ..., 2.6226, 2.6226, 2.6226]],\n",
      "\n",
      "        [[2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430],\n",
      "         [2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430],\n",
      "         [2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430],\n",
      "         ...,\n",
      "         [2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430],\n",
      "         [2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430],\n",
      "         [2.3430, 2.3430, 2.3430,  ..., 2.3430, 2.3430, 2.3430]],\n",
      "\n",
      "        [[2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548],\n",
      "         [2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548],\n",
      "         [2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548],\n",
      "         ...,\n",
      "         [2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548],\n",
      "         [2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548],\n",
      "         [2.3548, 2.3548, 2.3548,  ..., 2.3548, 2.3548, 2.3548]]])\n"
     ]
    }
   ],
   "source": [
    "print(sum)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[     0,      0,      0,  ..., 150527, 150527, 150527],\n",
       "                       [     0,      1,      2,  ...,     61,     62,     63],\n",
       "                       [     0,      0,      0,  ...,    221,    221,    221],\n",
       "                       [     0,      0,      0,  ...,    221,    221,    221]]),\n",
       "       values=tensor([-0.1496, -0.0656,  0.1445,  ..., -0.1540,  0.1801,\n",
       "                      -0.0743]),\n",
       "       size=(150528, 64, 222, 222), nnz=85162752, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "dense = sparse_tensor.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result2 = function(dense)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[9075, 128, 51, 51] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_2 \u001b[38;5;241m=\u001b[39m \u001b[43mfunction2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     conv2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(conv2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     10\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: conv(x)\n\u001b[0;32m---> 11\u001b[0m function2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[9075, 128, 51, 51] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 91/91 [00:12<00:00,  7.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05],\n",
      "         [2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05],\n",
      "         [2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05],\n",
      "         ...,\n",
      "         [2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05],\n",
      "         [2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05],\n",
      "         [2.4904e-05, 2.4904e-05, 2.4904e-05,  ..., 2.4904e-05,\n",
      "          2.4904e-05, 2.4904e-05]],\n",
      "\n",
      "        [[2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05],\n",
      "         [2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05],\n",
      "         [2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05],\n",
      "         ...,\n",
      "         [2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05],\n",
      "         [2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05],\n",
      "         [2.2539e-05, 2.2539e-05, 2.2539e-05,  ..., 2.2539e-05,\n",
      "          2.2539e-05, 2.2539e-05]],\n",
      "\n",
      "        [[2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05],\n",
      "         [2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05],\n",
      "         [2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05],\n",
      "         ...,\n",
      "         [2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05],\n",
      "         [2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05],\n",
      "         [2.5059e-05, 2.5059e-05, 2.5059e-05,  ..., 2.5059e-05,\n",
      "          2.5059e-05, 2.5059e-05]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05],\n",
      "         [2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05],\n",
      "         [2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05],\n",
      "         ...,\n",
      "         [2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05],\n",
      "         [2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05],\n",
      "         [2.1958e-05, 2.1958e-05, 2.1958e-05,  ..., 2.1958e-05,\n",
      "          2.1958e-05, 2.1958e-05]],\n",
      "\n",
      "        [[2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05],\n",
      "         [2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05],\n",
      "         [2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05],\n",
      "         ...,\n",
      "         [2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05],\n",
      "         [2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05],\n",
      "         [2.4474e-05, 2.4474e-05, 2.4474e-05,  ..., 2.4474e-05,\n",
      "          2.4474e-05, 2.4474e-05]],\n",
      "\n",
      "        [[2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05],\n",
      "         [2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05],\n",
      "         [2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05],\n",
      "         ...,\n",
      "         [2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05],\n",
      "         [2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05],\n",
      "         [2.3350e-05, 2.3350e-05, 2.3350e-05,  ..., 2.3350e-05,\n",
      "          2.3350e-05, 2.3350e-05]]])\n",
      "\n",
      "Tenseur sparse global:\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = SparseEvaluation(result,function=function2,chunk_size=100)\n",
    "with torch.no_grad():\n",
    "    result_3,sum_3 = test.evaluate_all_chunks()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given groups=1, weight of size [128, 64, 3, 3], expected input[9075, 128, 51, 51] to have 64 channels, but got 128 channels instead",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[65], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m result_4 \u001b[38;5;241m=\u001b[39m \u001b[43mfunction2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresult_3\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto_dense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[51], line 11\u001b[0m, in \u001b[0;36m<lambda>\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m      9\u001b[0m     conv2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mzeros_like(conv2\u001b[38;5;241m.\u001b[39mbias\u001b[38;5;241m.\u001b[39mdata)\n\u001b[1;32m     10\u001b[0m function \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: conv(x)\n\u001b[0;32m---> 11\u001b[0m function2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[43mconv2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1509\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1510\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1511\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1520\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1515\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1516\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1517\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1518\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1519\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1520\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1522\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1523\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:460\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    459\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 460\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/torch/nn/modules/conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[0;34m(self, input, weight, bias)\u001b[0m\n\u001b[1;32m    452\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    453\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[1;32m    454\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[1;32m    455\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[0;32m--> 456\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    457\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given groups=1, weight of size [128, 64, 3, 3], expected input[9075, 128, 51, 51] to have 64 channels, but got 128 channels instead"
     ]
    }
   ],
   "source": [
    "result_4 = function2(result_3.to_dense())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1506/1506 [1:02:23<00:00,  2.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Somme des convolutions sur la dimension 0:\n",
      "tensor([[[2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933],\n",
      "         [2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933],\n",
      "         [2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933],\n",
      "         ...,\n",
      "         [2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933],\n",
      "         [2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933],\n",
      "         [2.2933, 2.2933, 2.2933,  ..., 2.2933, 2.2933, 2.2933]],\n",
      "\n",
      "        [[2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230],\n",
      "         [2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230],\n",
      "         [2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230],\n",
      "         ...,\n",
      "         [2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230],\n",
      "         [2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230],\n",
      "         [2.5230, 2.5230, 2.5230,  ..., 2.5230, 2.5230, 2.5230]],\n",
      "\n",
      "        [[2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997],\n",
      "         [2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997],\n",
      "         [2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997],\n",
      "         ...,\n",
      "         [2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997],\n",
      "         [2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997],\n",
      "         [2.0997, 2.0997, 2.0997,  ..., 2.0997, 2.0997, 2.0997]],\n",
      "\n",
      "        ...,\n",
      "\n",
      "        [[2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922],\n",
      "         [2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922],\n",
      "         [2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922],\n",
      "         ...,\n",
      "         [2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922],\n",
      "         [2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922],\n",
      "         [2.2922, 2.2922, 2.2922,  ..., 2.2922, 2.2922, 2.2922]],\n",
      "\n",
      "        [[2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688],\n",
      "         [2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688],\n",
      "         [2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688],\n",
      "         ...,\n",
      "         [2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688],\n",
      "         [2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688],\n",
      "         [2.1688, 2.1688, 2.1688,  ..., 2.1688, 2.1688, 2.1688]],\n",
      "\n",
      "        [[2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388],\n",
      "         [2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388],\n",
      "         [2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388],\n",
      "         ...,\n",
      "         [2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388],\n",
      "         [2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388],\n",
      "         [2.1388, 2.1388, 2.1388,  ..., 2.1388, 2.1388, 2.1388]]])\n",
      "\n",
      "Tenseur sparse global:\n",
      "tensor(indices=tensor([[     0,      0,      0,  ..., 150527, 150527, 150527],\n",
      "                       [     0,      1,      2,  ...,    125,    126,    127],\n",
      "                       [     0,      0,      0,  ...,    219,    219,    219],\n",
      "                       [     0,      0,      0,  ...,    219,    219,    219]]),\n",
      "       values=tensor([-0.0113,  0.0321,  0.0043,  ..., -0.0179, -0.0237,\n",
      "                      -0.0203]),\n",
      "       size=(150528, 128, 220, 220), nnz=464640000, layout=torch.sparse_coo)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test = SparseEvaluation(result,function=function2,chunk_size=100)\n",
    "with torch.no_grad():\n",
    "    result = test.evaluate_all_chunks()\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "dask.distributed is not installed.\n\nPlease either conda or pip install distributed:\n\n  conda install dask distributed             # either conda install\n  python -m pip install \"dask[distributed]\" --upgrade    # or pip install",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/distributed.py:13\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 13\u001b[0m     \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'distributed'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 5\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msparse\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m FloatTensor\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtyping\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Callable\n\u001b[0;32m----> 5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdistributed\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client, as_completed\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mdask\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01marray\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mda\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/lib/python3.10/site-packages/dask/distributed.py:16\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m e\u001b[38;5;241m.\u001b[39mmsg \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo module named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdistributed\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m:\n\u001b[0;32m---> 16\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mImportError\u001b[39;00m(_import_error_message) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m     18\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[0;31mImportError\u001b[0m: dask.distributed is not installed.\n\nPlease either conda or pip install distributed:\n\n  conda install dask distributed             # either conda install\n  python -m pip install \"dask[distributed]\" --upgrade    # or pip install"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch.sparse import FloatTensor\n",
    "from typing import Callable\n",
    "from dask.distributed import Client, as_completed\n",
    "import dask\n",
    "import dask.array as da\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "class SparseEvaluation:\n",
    "    def __init__(self, x: FloatTensor, function: Callable, chunk_size: int):\n",
    "        self.x = x\n",
    "        self.function = function\n",
    "        self.chunk_size = chunk_size\n",
    "        self.dense_shape = list(x.size())\n",
    "\n",
    "        # Nombre de chunks\n",
    "        self.num_chunks = (self.dense_shape[0] + self.chunk_size - 1) // self.chunk_size\n",
    "\n",
    "        # Taille de sortie de la fonction\n",
    "        x0 = torch.zeros(1, self.dense_shape[1], self.dense_shape[2], self.dense_shape[3]).to(device)\n",
    "        self.output_size = list(self.function(x0).shape)\n",
    "        self.output_size[0] = self.dense_shape[0]\n",
    "\n",
    "        self.indices = self.x.indices()\n",
    "        self.values = self.x.values()\n",
    "\n",
    "        self.indices = self.indices.t()\n",
    "        self.function_sum = None\n",
    "        self.global_storage = {\n",
    "            'indices': [],\n",
    "            'values': []\n",
    "        }\n",
    "\n",
    "    def process_chunk(self, chunk_start, chunk_end):\n",
    "        with torch.no_grad():\n",
    "            mask = (self.indices[:, 0] >= chunk_start) & (self.indices[:, 0] < chunk_end)\n",
    "\n",
    "            chunk_indices = self.indices[mask]\n",
    "            chunk_indices[:, 0] -= chunk_start\n",
    "\n",
    "            chunk_values = self.values[mask]\n",
    "            chunk_size = chunk_end - chunk_start\n",
    "\n",
    "            chunk_sparse_tensor = torch.sparse.FloatTensor(\n",
    "                chunk_indices.t(), chunk_values,\n",
    "                torch.Size([chunk_size] + self.dense_shape[1:])\n",
    "            )\n",
    "\n",
    "            chunk_dense_tensor = chunk_sparse_tensor.to_dense().to(device)\n",
    "            func_output = self.function(chunk_dense_tensor)\n",
    "\n",
    "            func_sum = torch.abs(func_output).sum(dim=0)\n",
    "\n",
    "            func_output_sparse = func_output.to_sparse()\n",
    "            add_indices = func_output_sparse.indices().to(torch.int32) + torch.tensor(\n",
    "                [[chunk_start], [0], [0], [0]], dtype=torch.int32, device=device\n",
    "            )\n",
    "\n",
    "            return func_sum.cpu(), add_indices.cpu(), func_output_sparse.values().cpu()\n",
    "\n",
    "    def evaluate_all_chunks(self):\n",
    "        with torch.no_grad():\n",
    "            client = Client()  # Dask client, can be pointed to a scheduler address\n",
    "\n",
    "            futures = []\n",
    "            for i in range(self.num_chunks):\n",
    "                chunk_start = i * self.chunk_size\n",
    "                chunk_end = min((i + 1) * self.chunk_size, self.dense_shape[0])\n",
    "                futures.append(client.submit(self.process_chunk, chunk_start, chunk_end))\n",
    "\n",
    "            for future in tqdm(as_completed(futures), total=len(futures)):\n",
    "                func_sum, add_indices, func_output_values = future.result()\n",
    "\n",
    "                if self.function_sum is None:\n",
    "                    self.function_sum = func_sum\n",
    "                else:\n",
    "                    self.function_sum += func_sum\n",
    "\n",
    "                self.global_storage['indices'].append(add_indices)\n",
    "                self.global_storage['values'].append(func_output_values)\n",
    "\n",
    "            # Concaténation finale des indices et valeurs\n",
    "            global_indices = torch.cat(self.global_storage['indices'], dim=1)\n",
    "            global_values = torch.cat(self.global_storage['values'], dim=0)\n",
    "\n",
    "            global_sparse_tensor = torch.sparse_coo_tensor(global_indices, global_values, size=self.output_size)\n",
    "\n",
    "            print(\"Somme des convolutions sur la dimension 0:\")\n",
    "            print(self.function_sum)\n",
    "\n",
    "            print(\"\\nTenseur sparse global:\")\n",
    "            return global_sparse_tensor,function_sum\n",
    "\n",
    "# Example usage:\n",
    "# Assuming `input_tensor` is your sparse input tensor and `your_function` is the function to apply\n",
    "# sparse_eval = SparseEvaluation(input_tensor, your_function, chunk_size=32)\n",
    "# result = sparse_eval.evaluate_all_chunks()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
