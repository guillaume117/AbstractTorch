{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Pruning\n",
    "<div class='alert alert-success'>\n",
    "Ce petit notebook permet d'évaluer AbstractWeight sur un petit réseau. \n",
    "Pour cela, on créé AbstractNN, un petit réseau composé de 3 couches de convolution suivies de ReLU, puis de deux couches FC suivies également de ReLU. \n",
    "Pour l'entrainement uniquement, un softmax est appliqué en sortie de réseau\n",
    "</div>\n",
    "\n",
    "Dans la classe AbstractNN, la méthode forward de nn.Linear est surchargée de façon standard.\n",
    "C'est à dire que si l'on instancie model=AbstractNN(), model(x) applique forward à x comme dans n'importe quel réseau. \n",
    "\n",
    "AbstractNN possède une méthode abstract_forward() qui fait appelle à AbstractLinear et AbstractReLU, AbstractMaxpool et AbstractWeights.\n",
    "\n",
    "Le réseau est préalablement entraîné. On va tenter d'analyser le comportement du réseau autour des poids que l'on souhaite retirer. Pour cela, on recherche premièrement les candidats à l'élagage (méthode heuristique, poids les plus faibles en valeur absolu). On en selectionne une quantité n, puis on analyse le comportement du réseau autour de ces poids: \n",
    "\n",
    "$$\\forall p\\in [1,m], y_p = \\sum_{k=1}^n w_{pk} x_k +\\alpha_{pk}x_k  \\delta_{pk} $$\n",
    "\n",
    "On utilise pour cela la classe AbstractWeight, qui convertit une couche FC dans un réseau en domaine abstrait. Cette classe prend en argument l'index des poids à bruiter, les valeurs du bruits. \n",
    "Dans le cadre de l'élagage, on souhaite que l'expression affine inclue la valeur pour laquelle le poids est nul. On choisit donc un alpha de bruit valant le poids en valeur absolu plus un coeficient de marge (10%). \n",
    "On est donc certain que l'expression affine en sortie du réseau inclue l'image de la matrice élaguée. \n",
    "\n",
    "Enfin on observe la relevance sur la matrice de poids. Cette relevance est normalisée par la valeur de alpha. \n",
    "Les indices ayant une forte relevance ont donc contribué à la dispersion du domaine, ils affectent en premier lieu la dominance.\n",
    "\n",
    "On étudiera par la suite une méthode pour retirer de la liste des candidats à l'élagage les poids contribuants le plus à la réduction de la dominance. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from abstractModule import AbstractLinear as AL\n",
    "from abstractModule import AbstractReLU as AR\n",
    "from abstractModule import AbstractMaxpool2D as AM\n",
    "from abstractWeight import AbstractWeight as AW\n",
    "\n",
    "from custom_train import CustomTrainer as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "\n",
    "device = torch.device('cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on importe le dataset FashionMNIST normalisation et random split pour l'entrainement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path ='dataset'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize(mean =[0.5], std =[0.2]),\n",
    "        #transforms.Resize((56,56))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = datasets.FashionMNIST(root = path,transform = transform, download = True, train = True)\n",
    "dataset_test =datasets.FashionMNIST( root =path,transform=transform ,download = True, train = False)\n",
    "val =0.2\n",
    "len_data_train = len(dataset_train)\n",
    "train_size =int((1-val)*len_data_train)\n",
    "\n",
    "val_size = int(val*len_data_train)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ici on génère notre classe AbstractNN et sa fameuse méthode abstrac_forward\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AbstractNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_depth=1,device=torch.device(\"cpu\")):\n",
    "\n",
    "        super(AbstractNN,self).__init__()\n",
    "       \n",
    "      \n",
    "        self.num_depth = num_depth\n",
    "        self.device = device\n",
    "        self.conv1=nn.Conv2d(self.num_depth,16,3,device=self.device)\n",
    "        self.conv2=nn.Conv2d(16,32,3,device=self.device)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=2,stride=2,padding=0)\n",
    "  \n",
    "\n",
    "       \n",
    "\n",
    "        self.fc1=nn.Sequential(nn.Flatten(),nn.Linear(4608,64,device=self.device))\n",
    "        self.fc2=nn.Sequential(nn.Flatten(),nn.Linear(64,10,device=self.device))\n",
    "        self.softMax =nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=torch.relu(x)\n",
    "    \n",
    "        x=self.maxpool(x)\n",
    "   \n",
    "    \n",
    "        x=self.fc1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=torch.relu(x)\n",
    "        x= self.softMax(x)\n",
    "        return x\n",
    "    \n",
    "    def abstract_forward(self,x,add_symbol=False,device=torch.device(\"cpu\"),index=None,alpha=None):\n",
    "        \n",
    "        \n",
    "        self.device=device\n",
    "        #Dans cette première partie, du réseau, on reste dans le domaine concrêt.\n",
    "        x=self.conv1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.maxpool(x)\n",
    "    \n",
    "        x_true = x\n",
    "        x_true = x_true[0].unsqueeze(0)\n",
    "\n",
    "        \n",
    "        print(f\"x.shape = {x.shape}\")\n",
    "        x,x_min,x_max,x_true = AW.generate_ztp_from_layer_and_indexes(self.fc1,index =index.indices,alpha = alpha,input = x, device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "      \n",
    "        x,x_min,x_max,x_true = AL.abstract_linear(self.fc2,x,x_true,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "       \n",
    "        \n",
    "        return x,x_min,x_max,x_true\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un petit script pour l'entrainement, disons 10 EPOCH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "model =AbstractNN(num_depth=1,device=device)\n",
    "\n",
    "num_epochs =10\n",
    "learning_rate=0.01\n",
    "batch_size = 128\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.99))\n",
    "scheduler =  optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.98)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "TD= T(model=model,\n",
    "            device=device\n",
    "                            )\n",
    "model = TD.train_model(train_dataset,val_dataset,\n",
    "                        criterion=criterion,  \n",
    "                        batch_size= batch_size,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        num_epochs=num_epochs,\n",
    "                        learning_rate=learning_rate,\n",
    "                        resname='SimpleCNN',verbose=False)\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model=AbstractNN(num_depth=1,device=device   )\n",
    "model.load_state_dict(torch.load('dataset/test.pth'))\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "On récupère la matrice de poids de fc1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.named_parameters():\n",
    "   if param[0] == 'fc1.1.weight':\n",
    "        print(torch.sum(param[1].data))        \n",
    "        index = torch.topk(torch.abs(param[1].data).flatten(),k=100_000,largest=False)\n",
    "        mat= param[1].data\n",
    "        break\n",
    "print(index.indices)\n",
    "alpha= 1.1*torch.abs(index.values)\n",
    "print(alpha)\n",
    "print(f'mat.shape = {mat.shape}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "img, label = dataset_train[230]\n",
    "plt.imshow(1-img[0],cmap='gray')\n",
    "print(f\"Label:{label}\")\n",
    "img.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.eval()\n",
    "model.to(device)\n",
    "img=img.to(device)\n",
    "print(img.unsqueeze(0).shape)\n",
    "with torch.no_grad():\n",
    "\n",
    " \n",
    "   result,x_min,x_max,x_true=model.abstract_forward(img.unsqueeze(0),add_symbol=True,device=device,index=index,alpha=alpha)\n",
    "print(f\"result.shape = {result.shape}\")\n",
    "print(f\"y_min       =  {x_min}\")\n",
    "print(f\"y_max       =  {x_max}\")\n",
    "print(f\"center Ztp  =  {result[0]}\")\n",
    "print(f\"y_true      =  {x_true[:]}\")\n",
    "print(f\"y_max-x_min =  {x_max-x_min}\")\n",
    "print(f\"Trash symbol=  {result[-1]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "y_min       =  np.array(x_min)\n",
    "y_max       =  np.array(x_max)\n",
    "center_Ztp  =  np.expand_dims(np.array(result[0]),axis =1)\n",
    "y_true      =  np.expand_dims(np.array(x_true[:])[0],axis =1)\n",
    "y_max_minus_y_min =  np.array(x_max-x_min)\n",
    "Trash_symbol=  np.array(result[-1])\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "#plt.style.use('_mpl-gallery')\n",
    "\n",
    "# make data:\n",
    "\n",
    "x = np.arange(len(y_min))\n",
    "D =np.stack((y_min,y_max),axis=1)\n",
    "\n",
    "print(D.shape)\n",
    "print(center_Ztp.shape)\n",
    "print(y_true.shape)\n",
    "# plot:\n",
    "\n",
    "fig,ax = plt.subplots(1, 1, figsize=(8,4), tight_layout=True)\n",
    "ax.eventplot(D, orientation=\"vertical\", linewidth=1,color='blue',linelengths=0.3)\n",
    "ax.eventplot(y_true, orientation=\"vertical\", linewidth=0.50,color='green',linelengths=0.4)\n",
    "ax.eventplot(center_Ztp, orientation=\"vertical\", linewidth=1,color='red',linelengths=0.5)\n",
    "\n",
    "ax.set(xlim=(0, 10),xticks=x,xticklabels=[\"T-shirt/top\",\"Trouser\",\"Pullover\",\"Dress\",\"Coat\",\"Sandal\",\"Shirt\",\"Sneaker\",\"Bag,\",\"Ankle boot\"],\n",
    "       ylim=(-20, 40))\n",
    "plt.ylabel(\"Value of the abstract domain\")\n",
    "plt.title(\"Dominance interval for the 10 classes of Fashion MNIST .\\n Abstract domain based on 100_000 lower weights of the first layer of the first fully connected layer of the model\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le premier affichage de relevance sur l'image d'une couche calculé par AbstractTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"mat.shape = {mat.shape}\")\n",
    "map_relevance = torch.zeros_like(mat).flatten()\n",
    "map_original = torch.zeros_like(mat).flatten() \n",
    "\n",
    "value=torch.argmax(result[0])\n",
    "print(f\"result[1:100001,value]= {result[1:100001,value].shape}\")\n",
    "map_relevance[index.indices]= result[1:100001,value]\n",
    "map_original[index.indices]= mat.flatten()[index.indices]\n",
    "map_relevance = map_relevance.reshape(mat.shape)\n",
    "map_original = map_original.reshape(mat.shape)\n",
    "print(f\"map_2.shape = {map_relevance.shape}\")\n",
    "\n",
    "map_normalized_relevance = torch.abs(map_relevance)/torch.abs(map_original)\n",
    "map_normalized_relevance = torch.where(torch.isnan(map_normalized_relevance), torch.zeros_like(map_normalized_relevance), map_normalized_relevance)\n",
    "\n",
    "\n",
    "\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "plt.figure(figsize=(12, 6)) \n",
    "\n",
    "im = plt.imshow(torch.abs(map_normalized_relevance.cpu())[:, 200:600].numpy(), cmap='viridis')\n",
    "\n",
    "plt.title('Normalised_Relevance')\n",
    "\n",
    "\n",
    "divider = make_axes_locatable(plt.gca())\n",
    "cax = divider.append_axes(\"right\", size=\"5%\", pad=0.05)  \n",
    "\n",
    "\n",
    "plt.colorbar(im, cax=cax)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
