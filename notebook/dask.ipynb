{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(501, 300, 20000)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import dask\n",
    "import dask.array as da\n",
    "import torch\n",
    "import numpy as np\n",
    "tensor = torch.randn(501,300,20000)\n",
    "chunks_size = (50,300,20000)\n",
    "dask_array = da.from_array(tensor.numpy(), chunks=chunks_size)  \n",
    "print(dask_array.shape)\n",
    "import torch.nn as nn\n",
    "model = nn.Sequential(nn.Flatten(),nn.Linear(in_features=300*200*100, out_features=10))  \n",
    "def inference(chunk, model):\n",
    " \n",
    "    model.eval()\n",
    "\n",
    "\n",
    "    expected_shape = (chunk.shape[0], 300, 20000)\n",
    "    assert chunk.shape == expected_shape, f\"Invalid chunk shape: {chunk.shape}\"\n",
    "\n",
    "    with torch.no_grad():\n",
    "        chunk_tensor = torch.from_numpy(chunk)\n",
    "        chunk_flat = chunk_tensor.view(chunk_tensor.size(0), -1)\n",
    "        output=model(chunk_flat)\n",
    "        \n",
    "    output=output.detach().numpy()\n",
    "    output = output[:,np.newaxis]\n",
    "    return output\n",
    "\n",
    "result = dask_array.map_blocks(inference,model=model,dtype=np.float32,chunks = chunks_size)\n",
    "result_array = result.compute()\n",
    "#\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(501, 1, 10)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_array.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([501, 10])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_serial = model(tensor)\n",
    "result_serial.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: h5py in /home/guillaume/.local/lib/python3.10/site-packages (3.11.0)\n",
      "Requirement already satisfied: numpy>=1.17.3 in /home/guillaume/.local/lib/python3.10/site-packages (from h5py) (1.26.4)\n"
     ]
    }
   ],
   "source": [
    "!pip install h5py\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "import torch\n",
    "\n",
    "# Suppose your tensor is `tensor` (batch, channel, height, width)\n",
    "tensor = torch.randn(500,60, 224, 224)\n",
    "\n",
    "# Create an HDF5 file\n",
    "with h5py.File('tensor_data.h5', 'w') as hf:\n",
    "    # Create a dataset within the HDF5 file\n",
    "    hf.create_dataset('data', data=tensor.numpy())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import h5py\n",
    "\n",
    "# Ouverture du fichier HDF5 en mode lecture\n",
    "with h5py.File('tensor_data.h5', 'r') as f:\n",
    "    # Accéder au jeu de données\n",
    "    dataset = f[\"./\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Closed HDF5 group>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "from torch import sparse\n",
    "from abstractModule import AbstractLinear as AL\n",
    "from abstract import abstractTensor as AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([37634, 3, 112, 112])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_to_abstract = torch.randn(3,112,112)\n",
    "abstract_tensor = AT(tensor_to_abstract,alpha=0.03*torch.ones(3*112*112)).abstract_tensor()\n",
    "abstract_tensor.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "abstract_tensor = abstract_tensor.to_sparse()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 12542, 12543, 12544],\n",
       "                       [    0,     0,     0,  ...,     0,     0,     0],\n",
       "                       [    0,     0,     0,  ...,   111,   111,   111],\n",
       "                       [    0,     1,     2,  ...,   109,   110,   111]]),\n",
       "       values=tensor([0.7535, 0.1596, 0.0033,  ..., 0.0300, 0.0300, 0.0300]),\n",
       "       size=(12546, 1, 112, 112), nnz=25088, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tensor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([20, 3, 112, 112])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_124719/2735095733.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  indices = torch.tensor(indices)\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::index_copy_' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_copy_' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_3.cpp:24643 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5216 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(result\u001b[38;5;241m.\u001b[39mshape)\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mto_sparse()\n\u001b[0;32m----> 8\u001b[0m \u001b[43mabstract_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_copy_\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdim\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindex\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msource\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::index_copy_' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::index_copy_' is only available for these backends: [CPU, CUDA, HIP, MPS, IPU, XPU, HPU, VE, MTIA, PrivateUse1, PrivateUse2, PrivateUse3, Meta, FPGA, ORT, Vulkan, Metal, QuantizedCPU, QuantizedCUDA, QuantizedHIP, QuantizedMPS, QuantizedIPU, QuantizedXPU, QuantizedHPU, QuantizedVE, QuantizedMTIA, QuantizedPrivateUse1, QuantizedPrivateUse2, QuantizedPrivateUse3, QuantizedMeta, CustomRNGKeyId, MkldnnCPU, SparseCsrCPU, SparseCsrCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nUndefined: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMeta: registered at /dev/null:241 [kernel]\nFPGA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nORT: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nVulkan: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMetal: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHIP: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMPS: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedIPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedXPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedHPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedVE: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMTIA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse1: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse2: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedPrivateUse3: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nQuantizedMeta: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nCustomRNGKeyId: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nMkldnnCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCPU: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nSparseCsrCUDA: registered at aten/src/ATen/RegisterCompositeExplicitAutogradNonFunctional.cpp:21592 [default backend kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_3.cpp:24643 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_1.cpp:5216 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_0.cpp:17339 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:720 [backend fallback]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "indices = torch.arange(0,20)\n",
    "indices = torch.tensor(indices)\n",
    "result = abstract_tensor.index_select(0, indices)\n",
    "\n",
    "print(result.shape)\n",
    "result = result.to_sparse()\n",
    "\n",
    "abstract_tensor.index_copy_(dim =0, index = indices, source = result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tenseur creux original:\n",
      "tensor(indices=tensor([[0, 1, 1],\n",
      "                       [1, 0, 1]]),\n",
      "       values=tensor([1, 2, 3]),\n",
      "       size=(2, 2), nnz=3, layout=torch.sparse_coo)\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'torch.sparse' has no attribute 'index_select'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[8], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Sélectionner un sous-ensemble de valeurs avec index_select\u001b[39;00m\n\u001b[1;32m     14\u001b[0m selected_indices \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m])\n\u001b[0;32m---> 15\u001b[0m selected_values \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_select\u001b[49m(sparse_tensor, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m, index\u001b[38;5;241m=\u001b[39mselected_indices)\n\u001b[1;32m     17\u001b[0m \u001b[38;5;66;03m# Afficher les indices et les valeurs sélectionnés\u001b[39;00m\n\u001b[1;32m     18\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mIndices sélectionnés:\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'torch.sparse' has no attribute 'index_select'"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Créer un tenseur creux (sparse tensor) original\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices=[[0, 1, 1],\n",
    "                                                 [1, 0, 1]],\n",
    "                                        values=[1, 2, 3],\n",
    "                                        size=(2, 2))\n",
    "\n",
    "# Afficher le tenseur creux original\n",
    "print(\"Tenseur creux original:\")\n",
    "print(sparse_tensor)\n",
    "\n",
    "# Sélectionner un sous-ensemble de valeurs avec index_select\n",
    "selected_indices = torch.tensor([0, 2])\n",
    "selected_values = torch.sparse.index_select(sparse_tensor, dim=0, index=selected_indices)\n",
    "\n",
    "# Afficher les indices et les valeurs sélectionnés\n",
    "print(\"\\nIndices sélectionnés:\")\n",
    "print(selected_indices)\n",
    "print(\"Valeurs sélectionnées:\")\n",
    "print(selected_values)\n",
    "\n",
    "# Créer de nouvelles valeurs à remplacer\n",
    "new_values = torch.tensor([10, 20])\n",
    "\n",
    "# Reconstruire le tenseur creux avec les nouvelles valeurs\n",
    "# Utilisez le constructeur sparse_coo_tensor pour reconstruire le tenseur\n",
    "# En passant les nouveaux indices sélectionnés et les nouvelles valeurs\n",
    "sparse_tensor = torch.sparse_coo_tensor(indices=selected_indices.numpy(),  # Convertir en tableau numpy\n",
    "                                        values=new_values,\n",
    "                                        size=(2, 2))\n",
    "\n",
    "# Afficher le tenseur creux modifié\n",
    "print(\"\\nTenseur creux modifié:\")\n",
    "print(sparse_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(indices=tensor([[    0,     0,     0,  ..., 12542, 12543, 12544],\n",
       "                       [    0,     0,     0,  ...,     0,     0,     0],\n",
       "                       [    0,     0,     0,  ...,   111,   111,   111],\n",
       "                       [    0,     1,     2,  ...,   109,   110,   111]]),\n",
       "       values=tensor([0.7535, 0.1596, 0.0033,  ..., 0.0300, 0.0300, 0.0300]),\n",
       "       size=(12546, 1, 112, 112), nnz=25088, layout=torch.sparse_coo)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cpu')\n",
    "abstract_tensor.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10, 11, 12, 13, 14, 15, 16, 17,\n",
       "        18, 19])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 112, 112])\n"
     ]
    },
    {
     "ename": "NotImplementedError",
     "evalue": "Could not run 'aten::_index_put_impl_' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_index_put_impl_' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNotImplementedError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[10], line 7\u001b[0m\n\u001b[1;32m      4\u001b[0m result \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mrandn(\u001b[38;5;241m20\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m224\u001b[39m,\u001b[38;5;241m224\u001b[39m)\n\u001b[1;32m      6\u001b[0m result \u001b[38;5;241m=\u001b[39m result\u001b[38;5;241m.\u001b[39mto_sparse()\n\u001b[0;32m----> 7\u001b[0m \u001b[43mabstract_tensor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mindex_put_\u001b[49m\u001b[43m(\u001b[49m\u001b[43m[\u001b[49m\u001b[43mindices\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43mresult\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mNotImplementedError\u001b[0m: Could not run 'aten::_index_put_impl_' with arguments from the 'SparseCPU' backend. This could be because the operator doesn't exist for this backend, or was omitted during the selective/custom build process (if using custom build). If you are a Facebook employee using PyTorch on mobile, please visit https://fburl.com/ptmfixes for possible resolutions. 'aten::_index_put_impl_' is only available for these backends: [CPU, CUDA, Meta, QuantizedCPU, QuantizedCUDA, BackendSelect, Python, FuncTorchDynamicLayerBackMode, Functionalize, Named, Conjugate, Negative, ZeroTensor, ADInplaceOrView, AutogradOther, AutogradCPU, AutogradCUDA, AutogradHIP, AutogradXLA, AutogradMPS, AutogradIPU, AutogradXPU, AutogradHPU, AutogradVE, AutogradLazy, AutogradMTIA, AutogradPrivateUse1, AutogradPrivateUse2, AutogradPrivateUse3, AutogradMeta, AutogradNestedTensor, Tracer, AutocastCPU, AutocastCUDA, FuncTorchBatched, BatchedNestedTensor, FuncTorchVmapMode, Batched, VmapMode, FuncTorchGradWrapper, PythonTLSSnapshot, FuncTorchDynamicLayerFrontMode, PreDispatch, PythonDispatcher].\n\nCPU: registered at aten/src/ATen/RegisterCPU.cpp:31357 [kernel]\nCUDA: registered at aten/src/ATen/RegisterCUDA.cpp:44411 [kernel]\nMeta: registered at aten/src/ATen/RegisterMeta.cpp:26984 [kernel]\nQuantizedCPU: registered at aten/src/ATen/RegisterQuantizedCPU.cpp:944 [kernel]\nQuantizedCUDA: registered at aten/src/ATen/RegisterQuantizedCUDA.cpp:459 [kernel]\nBackendSelect: fallthrough registered at ../aten/src/ATen/core/BackendSelectFallbackKernel.cpp:3 [backend fallback]\nPython: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:154 [backend fallback]\nFuncTorchDynamicLayerBackMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:498 [backend fallback]\nFunctionalize: registered at aten/src/ATen/RegisterFunctionalization_0.cpp:21977 [kernel]\nNamed: registered at ../aten/src/ATen/core/NamedRegistrations.cpp:7 [backend fallback]\nConjugate: registered at ../aten/src/ATen/ConjugateFallback.cpp:17 [backend fallback]\nNegative: registered at ../aten/src/ATen/native/NegateFallback.cpp:19 [backend fallback]\nZeroTensor: registered at ../aten/src/ATen/ZeroTensorFallback.cpp:86 [backend fallback]\nADInplaceOrView: registered at ../torch/csrc/autograd/generated/ADInplaceOrViewType_0.cpp:4832 [kernel]\nAutogradOther: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradCUDA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHIP: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXLA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMPS: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradIPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradXPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradHPU: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradVE: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradLazy: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMTIA: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse1: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse2: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradPrivateUse3: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradMeta: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nAutogradNestedTensor: registered at ../torch/csrc/autograd/generated/VariableType_1.cpp:16254 [autograd kernel]\nTracer: registered at ../torch/csrc/autograd/generated/TraceType_0.cpp:16968 [kernel]\nAutocastCPU: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:378 [backend fallback]\nAutocastCUDA: fallthrough registered at ../aten/src/ATen/autocast_mode.cpp:244 [backend fallback]\nFuncTorchBatched: registered at ../aten/src/ATen/functorch/BatchRulesScatterOps.cpp:1242 [kernel]\nBatchedNestedTensor: registered at ../aten/src/ATen/functorch/LegacyBatchingRegistrations.cpp:746 [backend fallback]\nFuncTorchVmapMode: fallthrough registered at ../aten/src/ATen/functorch/VmapModeRegistrations.cpp:28 [backend fallback]\nBatched: registered at ../aten/src/ATen/LegacyBatchingRegistrations.cpp:1075 [backend fallback]\nVmapMode: fallthrough registered at ../aten/src/ATen/VmapModeRegistrations.cpp:33 [backend fallback]\nFuncTorchGradWrapper: registered at ../aten/src/ATen/functorch/TensorWrapper.cpp:203 [backend fallback]\nPythonTLSSnapshot: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:162 [backend fallback]\nFuncTorchDynamicLayerFrontMode: registered at ../aten/src/ATen/functorch/DynamicLayer.cpp:494 [backend fallback]\nPreDispatch: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:166 [backend fallback]\nPythonDispatcher: registered at ../aten/src/ATen/core/PythonFallbackKernel.cpp:158 [backend fallback]\n"
     ]
    }
   ],
   "source": [
    "result = result.to_dense()\n",
    "print(result[0].shape)\n",
    "\n",
    "result = torch.randn(20,3,224,224)\n",
    "\n",
    "result = result.to_sparse()\n",
    "abstract_tensor.index_put_([indices],result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'abstract_tensor' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[20], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mabstract_tensor\u001b[49m\u001b[38;5;241m.\u001b[39mindices()[\u001b[38;5;241m1\u001b[39m,:,:,:]\n",
      "\u001b[0;31mNameError\u001b[0m: name 'abstract_tensor' is not defined"
     ]
    }
   ],
   "source": [
    "abstract_tensor.indices()[1,:,:,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "(100, 60, 224, 224)\n",
      "torch.Size([100, 60, 224, 224])\n",
      "(100, 60, 224, 224)\n",
      "torch.Size([100, 60, 224, 224])\n",
      "(100, 60, 224, 224)\n",
      "torch.Size([100, 60, 224, 224])\n",
      "(100, 60, 224, 224)\n",
      "torch.Size([100, 60, 224, 224])\n",
      "(100, 60, 224, 224)\n",
      "torch.Size([100, 60, 224, 224])\n",
      "Taille du subset_tensor: torch.Size([500, 60, 224, 224])\n",
      "tensor([[[[ 4.2692e-01,  1.3476e-01, -1.0956e+00,  ..., -8.5284e-01,\n",
      "            5.0291e-01,  5.5153e-01],\n",
      "          [-3.5254e-01,  1.1187e+00, -7.0639e-01,  ...,  1.2570e+00,\n",
      "            9.8905e-01, -5.1879e-01],\n",
      "          [ 4.1082e-01,  2.0111e-01,  7.4048e-01,  ..., -1.5556e+00,\n",
      "            1.3142e-02, -6.3195e-01],\n",
      "          ...,\n",
      "          [-2.5921e+00,  6.3167e-01,  3.7550e-01,  ...,  6.4648e-01,\n",
      "           -5.2637e-01, -1.8517e-01],\n",
      "          [ 1.4282e+00,  4.3522e-01, -1.3143e+00,  ..., -1.2271e+00,\n",
      "           -4.0522e-01, -1.0492e+00],\n",
      "          [ 1.5851e-01,  6.9736e-01, -1.2156e+00,  ..., -1.1821e+00,\n",
      "            8.0591e-01,  1.6482e-01]],\n",
      "\n",
      "         [[-1.6304e-01,  9.6469e-01, -6.9126e-01,  ...,  2.3110e-02,\n",
      "            7.6242e-01, -7.8960e-01],\n",
      "          [ 3.7403e-01,  2.1494e-01, -8.9274e-01,  ..., -2.3244e-01,\n",
      "            5.2240e-01,  5.5374e-01],\n",
      "          [ 5.0331e-01,  2.0055e-01,  3.5606e-01,  ...,  4.3230e-01,\n",
      "            5.9343e-01, -8.7553e-01],\n",
      "          ...,\n",
      "          [ 3.0791e+00, -1.8363e-02,  1.6925e-01,  ..., -1.0437e+00,\n",
      "           -1.7292e-01, -6.2379e-01],\n",
      "          [-8.6041e-01, -2.3075e-01, -6.9072e-01,  ..., -1.7232e+00,\n",
      "           -1.1774e+00,  8.7162e-01],\n",
      "          [-5.1385e-01, -1.0214e+00,  5.1227e-01,  ..., -1.2072e-01,\n",
      "            1.8840e+00, -1.4686e+00]],\n",
      "\n",
      "         [[ 1.9545e-01, -1.0656e+00,  1.2201e+00,  ...,  3.7045e-01,\n",
      "            1.4959e+00, -1.8845e+00],\n",
      "          [ 1.4965e-01,  1.6277e+00,  4.1610e-01,  ..., -3.8639e-01,\n",
      "            7.0677e-01,  5.1053e-01],\n",
      "          [-7.4162e-01, -4.0386e-01, -2.9792e-01,  ..., -5.5184e-01,\n",
      "            1.4228e+00,  3.0096e-01],\n",
      "          ...,\n",
      "          [-2.0845e+00, -6.3925e-01,  6.2380e-01,  ..., -1.1131e+00,\n",
      "            5.4854e-03,  3.4203e-01],\n",
      "          [ 1.2234e-01, -2.5596e-01, -1.6803e+00,  ...,  2.1119e-01,\n",
      "           -3.5528e-01,  1.1320e-02],\n",
      "          [ 1.4293e+00, -3.2725e-01,  8.0324e-01,  ..., -9.6248e-01,\n",
      "            5.3817e-01,  3.3364e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.7403e-01, -1.3868e+00,  8.0860e-01,  ...,  1.7641e-01,\n",
      "            2.9013e-01, -5.8035e-02],\n",
      "          [-6.0543e-01,  1.1747e+00, -7.2901e-01,  ..., -5.4888e-01,\n",
      "           -5.3739e-01,  1.5178e+00],\n",
      "          [-1.5968e+00, -8.1931e-01, -6.6141e-01,  ..., -9.1552e-01,\n",
      "            9.4364e-01,  8.7069e-01],\n",
      "          ...,\n",
      "          [ 9.3960e-01,  2.7170e-01,  1.3913e-02,  ...,  3.4861e-01,\n",
      "            1.4930e-01, -8.0359e-01],\n",
      "          [ 8.9978e-01, -4.1428e-01,  7.0989e-01,  ...,  4.7000e-01,\n",
      "           -1.2761e-01, -3.8659e-01],\n",
      "          [ 1.1919e+00, -1.2242e-02,  7.0736e-01,  ...,  1.7750e+00,\n",
      "            1.4744e-01, -5.1025e-01]],\n",
      "\n",
      "         [[-5.5721e-01, -1.9468e-01,  1.3317e-01,  ..., -6.4835e-01,\n",
      "            2.6933e-01, -1.3967e+00],\n",
      "          [ 7.2527e-01, -6.0054e-01, -7.2558e-01,  ..., -1.3832e-01,\n",
      "           -1.9687e+00, -4.9578e-01],\n",
      "          [ 5.5899e-01,  5.8081e-01,  1.0403e+00,  ...,  5.2367e-01,\n",
      "            1.4762e-01, -6.3422e-01],\n",
      "          ...,\n",
      "          [ 7.0061e-01, -1.4104e-01,  3.8734e-01,  ..., -1.0749e+00,\n",
      "            3.7367e-01,  1.1706e-01],\n",
      "          [ 4.5231e-01,  4.0528e-01,  3.1660e-01,  ...,  5.6357e-01,\n",
      "           -1.6204e-01, -3.9124e-01],\n",
      "          [-2.8554e-01, -3.3107e-01,  1.0634e+00,  ...,  1.1351e+00,\n",
      "           -1.8708e+00,  1.0785e+00]],\n",
      "\n",
      "         [[-3.8800e-01,  9.7648e-01, -1.1631e+00,  ...,  9.7392e-01,\n",
      "           -2.4529e-01, -4.1738e-01],\n",
      "          [-1.1072e+00,  8.0534e-01,  3.3014e-01,  ..., -5.1850e-01,\n",
      "           -7.8300e-01,  1.6097e-01],\n",
      "          [-7.4029e-01, -5.1137e-01,  1.2874e+00,  ...,  5.5070e-01,\n",
      "            5.3140e-01, -2.7085e-01],\n",
      "          ...,\n",
      "          [ 5.1587e-01, -1.3259e+00, -7.7820e-01,  ...,  1.9537e+00,\n",
      "           -4.5049e-01, -1.3615e+00],\n",
      "          [-7.1637e-01, -9.3632e-01, -9.0146e-01,  ...,  2.5499e+00,\n",
      "            6.6797e-01, -7.7986e-01],\n",
      "          [ 1.4626e+00,  1.5959e-01,  1.3241e-01,  ...,  8.5734e-01,\n",
      "            2.7674e-01, -2.2690e-01]]],\n",
      "\n",
      "\n",
      "        [[[-8.1578e-01, -1.4166e+00, -6.7319e-01,  ..., -3.4172e-01,\n",
      "           -5.2200e-01, -9.4484e-01],\n",
      "          [ 4.7688e-01, -5.4137e-01,  3.7566e-01,  ..., -2.4874e-01,\n",
      "            7.3519e-01,  4.9316e-01],\n",
      "          [-1.0641e+00, -1.9190e+00, -3.8036e-01,  ...,  1.9576e-01,\n",
      "           -7.9128e-01, -3.0513e-01],\n",
      "          ...,\n",
      "          [-1.2214e+00, -9.9545e-01, -5.9578e-01,  ..., -1.3390e+00,\n",
      "            4.1217e-01, -2.0444e+00],\n",
      "          [-3.4722e-01, -6.3399e-01, -9.3113e-01,  ..., -6.4317e-01,\n",
      "           -9.7235e-01, -3.3850e-01],\n",
      "          [ 2.0392e+00,  4.0518e-01, -1.1672e+00,  ...,  1.0705e+00,\n",
      "           -6.8524e-01, -1.6107e+00]],\n",
      "\n",
      "         [[-6.4157e-01,  1.3352e+00,  1.3089e+00,  ..., -4.1604e-01,\n",
      "           -3.0621e-01,  1.5013e-01],\n",
      "          [-3.0174e-01, -4.5793e-01,  2.2874e-01,  ...,  9.7043e-01,\n",
      "           -2.5914e-01, -8.8357e-01],\n",
      "          [ 9.1807e-01, -1.2938e+00,  2.9173e-01,  ...,  5.3270e-01,\n",
      "            2.8781e-01,  7.0944e-01],\n",
      "          ...,\n",
      "          [ 2.8573e-01,  1.5582e+00,  1.4847e+00,  ..., -6.0017e-01,\n",
      "           -2.2297e+00, -5.3397e-01],\n",
      "          [-2.3900e-01, -1.9363e-01, -1.8347e+00,  ...,  2.2246e+00,\n",
      "           -1.2341e+00,  6.6513e-01],\n",
      "          [ 1.2471e-01,  5.0001e-01,  2.5868e-01,  ...,  1.8721e+00,\n",
      "           -9.8313e-01,  1.6895e+00]],\n",
      "\n",
      "         [[-2.8920e-01,  1.6625e+00, -1.2831e+00,  ..., -8.6352e-01,\n",
      "            4.7844e-01, -1.5572e+00],\n",
      "          [ 1.1690e+00, -9.2966e-01, -1.0840e+00,  ...,  1.1883e+00,\n",
      "            1.4961e+00,  8.8792e-01],\n",
      "          [ 1.0081e+00,  1.4582e-03, -1.1368e-01,  ..., -6.6033e-01,\n",
      "           -1.8361e+00,  4.9780e-02],\n",
      "          ...,\n",
      "          [ 3.8025e-02,  7.7005e-01,  6.4691e-01,  ..., -1.1748e+00,\n",
      "            8.4642e-02, -2.3200e-01],\n",
      "          [-8.8709e-01,  4.5098e-02,  1.9340e+00,  ..., -1.8487e-01,\n",
      "           -1.8445e-01,  2.0143e+00],\n",
      "          [-1.5339e+00, -1.8083e-01,  8.8175e-02,  ..., -3.5586e-01,\n",
      "            1.4021e+00, -1.0068e+00]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 2.3175e+00, -3.6257e-01,  4.4044e-01,  ..., -7.3467e-02,\n",
      "            3.3212e-01,  3.8205e-01],\n",
      "          [ 1.2401e+00,  1.5839e-01, -3.0859e-01,  ..., -1.6985e-01,\n",
      "            1.7259e+00,  1.1608e-01],\n",
      "          [-1.3905e+00, -2.4504e-02,  2.3779e+00,  ..., -3.7899e-01,\n",
      "           -9.7262e-01,  1.9435e+00],\n",
      "          ...,\n",
      "          [-3.1182e-01, -3.8669e-01, -4.8029e-01,  ..., -1.9288e+00,\n",
      "            1.7644e+00, -1.8219e+00],\n",
      "          [-1.4387e+00, -1.1785e+00,  1.1874e+00,  ...,  6.6762e-01,\n",
      "            1.8461e-01, -1.7472e-01],\n",
      "          [-5.5775e-01,  1.6303e-01,  6.0266e-01,  ...,  1.4295e+00,\n",
      "            1.7060e+00,  9.1871e-01]],\n",
      "\n",
      "         [[-5.6353e-01, -6.0412e-01, -2.4761e-01,  ..., -9.0116e-01,\n",
      "           -6.7207e-01, -6.1096e-01],\n",
      "          [ 5.7769e-01, -1.5770e+00,  6.0893e-01,  ..., -1.0582e+00,\n",
      "            1.3028e+00,  1.9325e-01],\n",
      "          [ 8.7994e-01,  2.9621e-01,  1.0520e+00,  ..., -8.6209e-01,\n",
      "           -1.3236e+00,  1.8309e+00],\n",
      "          ...,\n",
      "          [ 1.2681e+00,  4.2767e-01,  4.7438e-01,  ..., -1.9064e-01,\n",
      "           -1.6530e+00,  7.5592e-01],\n",
      "          [-1.9594e-01, -1.2270e+00,  4.0138e-01,  ..., -1.0344e-01,\n",
      "           -8.0832e-01,  6.8409e-01],\n",
      "          [-1.0918e+00, -1.2598e+00,  3.7033e-01,  ..., -6.0697e-01,\n",
      "            1.4607e+00,  2.4095e-01]],\n",
      "\n",
      "         [[-9.2314e-01,  1.0319e+00, -6.4257e-01,  ...,  1.3935e+00,\n",
      "           -3.4106e-01, -6.3438e-01],\n",
      "          [-1.7948e+00, -1.8665e+00,  9.4390e-01,  ..., -5.3612e-01,\n",
      "            1.8476e+00, -7.7792e-01],\n",
      "          [ 9.9496e-01, -1.6855e+00,  3.4998e-01,  ..., -7.6977e-01,\n",
      "           -3.4489e-01, -9.1620e-01],\n",
      "          ...,\n",
      "          [-4.6762e-01, -2.1524e-01, -3.7772e-01,  ...,  1.1396e+00,\n",
      "            1.0141e+00, -2.9253e+00],\n",
      "          [-6.5164e-01, -8.1007e-01,  7.0775e-01,  ..., -1.3594e+00,\n",
      "           -1.4367e+00, -5.1514e-01],\n",
      "          [ 3.9797e-01,  5.8498e-01, -4.1970e-02,  ..., -1.0217e-01,\n",
      "           -1.0769e-01,  1.1879e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 5.1284e-01,  8.4226e-01, -8.2852e-02,  ...,  1.6815e+00,\n",
      "            4.4730e-02,  3.6895e-01],\n",
      "          [-1.5838e-02,  2.2158e+00, -8.3862e-01,  ...,  5.3111e-01,\n",
      "            8.0773e-01, -1.7564e+00],\n",
      "          [ 7.6130e-01,  3.7186e-01, -1.7691e+00,  ..., -2.4711e-01,\n",
      "           -1.2314e+00, -1.1355e+00],\n",
      "          ...,\n",
      "          [ 6.9674e-01, -3.0532e-02, -1.0598e+00,  ..., -5.0262e-01,\n",
      "           -2.9434e-01,  5.9013e-01],\n",
      "          [-4.4048e-01,  4.8690e-01,  3.0599e-01,  ...,  3.9608e-01,\n",
      "            1.9227e-01, -2.4037e+00],\n",
      "          [-5.6078e-02,  7.6989e-01, -8.7812e-01,  ..., -1.7002e+00,\n",
      "           -7.4942e-02,  1.5851e+00]],\n",
      "\n",
      "         [[-3.8929e-02,  1.4380e+00,  9.7740e-01,  ..., -1.6859e-02,\n",
      "           -4.0555e-01,  1.2604e+00],\n",
      "          [-5.2155e-01,  2.0258e-01,  5.8028e-02,  ...,  1.3012e+00,\n",
      "            1.3054e-01,  1.0420e+00],\n",
      "          [-9.0332e-01, -2.2115e-01, -7.5084e-01,  ..., -7.8351e-01,\n",
      "            1.7449e+00, -1.4443e+00],\n",
      "          ...,\n",
      "          [-5.2772e-01,  9.6367e-01, -1.8123e+00,  ..., -1.1278e+00,\n",
      "            6.6232e-01,  7.0757e-02],\n",
      "          [-5.0027e-01, -4.7900e-01, -9.3158e-01,  ..., -2.1646e-01,\n",
      "            1.4073e+00, -1.4569e+00],\n",
      "          [ 8.7083e-01,  3.6399e-01, -4.6509e-01,  ..., -1.7508e+00,\n",
      "           -4.5092e-02,  8.2308e-02]],\n",
      "\n",
      "         [[-1.2933e+00, -1.4308e+00,  3.9958e-01,  ...,  9.7046e-01,\n",
      "           -2.8664e-01,  3.9652e-01],\n",
      "          [-7.8991e-01,  1.2755e+00, -9.4986e-01,  ..., -4.7623e-01,\n",
      "           -8.6047e-01, -3.3016e-01],\n",
      "          [ 6.6037e-01,  6.4901e-01, -5.0980e-01,  ...,  5.9898e-01,\n",
      "            5.1727e-01, -1.0789e+00],\n",
      "          ...,\n",
      "          [-5.7159e-01, -3.3602e-01,  1.6337e-02,  ...,  3.2102e-01,\n",
      "            1.2379e+00, -1.1101e+00],\n",
      "          [ 1.1495e-01, -3.1061e+00, -1.1703e+00,  ..., -1.4168e+00,\n",
      "           -9.3526e-01,  2.7837e-01],\n",
      "          [-3.2680e-01, -2.7280e-01, -3.6710e-01,  ..., -1.6581e+00,\n",
      "           -7.4534e-01,  3.4512e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-1.4157e+00,  1.4387e+00,  8.2958e-01,  ..., -1.3659e+00,\n",
      "           -7.1567e-01, -1.1465e-01],\n",
      "          [-6.3595e-01, -2.2851e+00, -1.0971e+00,  ...,  1.3756e+00,\n",
      "            3.3711e-01, -1.8119e-01],\n",
      "          [-5.5403e-01, -5.4249e-02,  1.2686e+00,  ...,  1.7154e+00,\n",
      "           -7.4050e-01,  2.3517e-01],\n",
      "          ...,\n",
      "          [ 3.7312e-01, -7.7621e-02, -5.7695e-01,  ..., -5.7412e-02,\n",
      "            3.7255e-01, -1.8685e+00],\n",
      "          [ 1.8133e+00, -3.9188e-01,  1.1589e+00,  ...,  1.8739e+00,\n",
      "            5.2315e-01, -8.7476e-02],\n",
      "          [-4.9249e-01,  1.1872e+00,  6.8815e-02,  ..., -1.2832e+00,\n",
      "           -2.6697e-01,  2.4107e+00]],\n",
      "\n",
      "         [[ 7.4445e-01, -8.9808e-01, -1.3923e+00,  ..., -8.6494e-01,\n",
      "            2.7830e-01,  3.3869e-01],\n",
      "          [ 1.0247e+00,  2.7704e-02, -1.3669e+00,  ..., -4.4413e-01,\n",
      "           -7.1997e-01,  6.3452e-01],\n",
      "          [-1.3028e+00,  4.4063e-01,  7.0781e-01,  ..., -1.2663e+00,\n",
      "           -3.7423e-01,  3.3849e-01],\n",
      "          ...,\n",
      "          [-6.2558e-01,  4.9585e-01,  6.9411e-01,  ...,  1.0467e+00,\n",
      "            2.6802e-01, -5.3102e-01],\n",
      "          [ 6.8955e-01,  2.8255e+00, -8.0134e-01,  ...,  4.1281e-01,\n",
      "           -3.6845e-02,  4.2661e-01],\n",
      "          [ 9.8326e-01, -1.7936e-01,  1.1904e+00,  ...,  2.7310e-02,\n",
      "            4.0866e-01, -9.2536e-01]],\n",
      "\n",
      "         [[ 1.1750e+00,  6.8922e-01,  1.0593e+00,  ...,  2.0078e+00,\n",
      "           -1.8988e-01, -2.6156e-01],\n",
      "          [ 5.9816e-01, -4.4451e-01,  1.8508e-01,  ...,  2.7826e-01,\n",
      "            9.2800e-01, -1.7183e+00],\n",
      "          [ 6.4825e-01,  3.5164e-01,  2.4052e+00,  ..., -8.4854e-01,\n",
      "            5.6067e-01, -1.7092e+00],\n",
      "          ...,\n",
      "          [ 7.6159e-01, -4.9979e-01, -6.3546e-01,  ..., -8.0121e-01,\n",
      "            3.1112e-01,  2.8836e-01],\n",
      "          [-1.5439e+00, -7.1837e-01, -6.8168e-01,  ...,  2.3274e+00,\n",
      "           -2.0898e+00, -2.4624e-01],\n",
      "          [ 1.1893e+00, -2.3868e-01, -4.3298e-01,  ...,  1.2037e+00,\n",
      "            1.9620e+00, -3.6304e-01]]],\n",
      "\n",
      "\n",
      "        ...,\n",
      "\n",
      "\n",
      "        [[[ 2.3659e+00, -2.5652e-01,  1.6249e-01,  ...,  7.0890e-01,\n",
      "           -2.2768e+00, -1.4635e+00],\n",
      "          [-8.4099e-01,  2.4315e+00,  9.8314e-01,  ...,  1.1525e-01,\n",
      "           -2.1342e+00, -6.2044e-01],\n",
      "          [-1.5997e+00, -9.9527e-01, -1.0206e+00,  ...,  3.5123e-01,\n",
      "           -6.5629e-01, -2.3521e-01],\n",
      "          ...,\n",
      "          [ 1.1815e+00,  1.0431e+00, -1.4887e+00,  ...,  3.3277e-01,\n",
      "           -1.2833e+00, -5.2142e-01],\n",
      "          [-1.1063e+00, -1.0927e+00, -4.9602e-01,  ...,  1.7402e+00,\n",
      "            9.5836e-01,  6.8754e-02],\n",
      "          [-3.9136e-01, -1.0720e+00,  1.7401e+00,  ...,  1.0816e-01,\n",
      "           -6.5202e-02,  2.6754e+00]],\n",
      "\n",
      "         [[ 2.7160e-01, -1.2848e+00,  8.1221e-02,  ...,  7.1450e-01,\n",
      "            1.3637e+00, -1.4633e+00],\n",
      "          [ 1.6046e+00, -8.7559e-01,  1.2807e+00,  ..., -1.6348e+00,\n",
      "           -1.2738e-01,  7.4263e-01],\n",
      "          [ 1.1089e+00, -2.6017e+00,  8.4456e-01,  ...,  7.6360e-01,\n",
      "           -2.4863e-01,  1.5295e+00],\n",
      "          ...,\n",
      "          [ 1.2280e+00,  4.6408e-02, -2.5923e-01,  ...,  2.3090e-01,\n",
      "           -4.0692e-01, -4.4991e-01],\n",
      "          [ 1.1992e+00, -1.4988e+00, -1.3769e-01,  ..., -2.7733e-02,\n",
      "            5.3350e-01, -3.1693e-01],\n",
      "          [-8.8572e-01,  5.4121e-01,  1.6526e-01,  ...,  1.0413e+00,\n",
      "           -5.5302e-01, -8.7245e-01]],\n",
      "\n",
      "         [[-6.4882e-02,  7.8885e-01,  1.5931e+00,  ...,  7.8868e-02,\n",
      "            3.7304e-01,  1.4747e+00],\n",
      "          [-6.0580e-01,  4.8472e-01, -4.4364e-01,  ...,  4.7613e-01,\n",
      "           -3.3634e-01,  1.4796e+00],\n",
      "          [-5.3158e-01,  1.0224e+00, -1.2766e+00,  ..., -2.4700e+00,\n",
      "           -1.2828e+00, -3.3876e+00],\n",
      "          ...,\n",
      "          [ 1.0824e+00, -1.3975e+00,  1.7756e+00,  ...,  9.3103e-01,\n",
      "            6.1087e-01,  7.7004e-01],\n",
      "          [-3.7653e-01,  3.9706e-02, -4.0700e-01,  ...,  1.7188e+00,\n",
      "            6.1788e-01, -8.1654e-02],\n",
      "          [ 3.0902e-01,  4.4014e-01, -5.3253e-01,  ...,  1.2831e-01,\n",
      "           -1.2516e+00,  1.7025e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 3.9155e-01,  3.6085e-02,  1.6398e+00,  ..., -1.4319e+00,\n",
      "           -3.0751e-01,  1.6374e+00],\n",
      "          [ 2.7289e-01, -7.9226e-01,  4.9864e-01,  ...,  1.9060e-01,\n",
      "            1.8327e-01,  3.9679e-02],\n",
      "          [-5.5566e-01,  1.2066e+00,  7.1095e-01,  ..., -1.2905e+00,\n",
      "           -9.6771e-01, -6.8561e-01],\n",
      "          ...,\n",
      "          [-4.4845e-01, -4.4736e-01, -1.7790e-01,  ...,  1.2978e+00,\n",
      "           -6.2478e-01, -8.5091e-01],\n",
      "          [-3.1200e-01, -1.5606e+00, -2.0408e+00,  ...,  1.1214e+00,\n",
      "           -6.0526e-02, -1.8312e+00],\n",
      "          [ 1.2782e+00, -8.8832e-01,  6.9132e-01,  ...,  2.6338e+00,\n",
      "           -8.3869e-01,  1.6155e+00]],\n",
      "\n",
      "         [[ 3.6010e-01,  1.0731e+00, -3.3627e-01,  ..., -1.1823e+00,\n",
      "            1.0406e+00, -2.6212e-01],\n",
      "          [-1.0571e+00,  1.9718e-01,  6.8186e-01,  ...,  1.3134e+00,\n",
      "            2.6399e-01,  3.1804e-01],\n",
      "          [-1.5840e+00, -6.2458e-02,  2.0283e-01,  ...,  1.2848e+00,\n",
      "           -4.4494e-01, -1.0795e+00],\n",
      "          ...,\n",
      "          [-1.9269e+00,  1.3645e+00, -5.4156e-01,  ..., -8.4292e-01,\n",
      "            6.5821e-02,  1.8533e+00],\n",
      "          [ 2.5122e+00,  2.2335e-02,  6.7417e-01,  ...,  8.3229e-01,\n",
      "            9.0227e-01,  7.8817e-01],\n",
      "          [ 1.1944e+00,  8.7633e-01,  5.2640e-02,  ..., -8.0652e-01,\n",
      "            8.7352e-01,  9.3846e-02]],\n",
      "\n",
      "         [[-4.4768e-01,  1.2500e+00,  3.2596e-01,  ...,  3.6577e-01,\n",
      "           -1.5166e-01,  1.2929e+00],\n",
      "          [ 3.0817e+00, -7.5962e-01,  4.0040e-01,  ...,  6.1508e-01,\n",
      "            3.8526e-01,  6.5913e-01],\n",
      "          [-1.2513e+00,  5.7870e-01,  1.9475e+00,  ...,  1.2365e+00,\n",
      "           -6.7079e-01, -6.5359e-01],\n",
      "          ...,\n",
      "          [ 1.7828e+00, -6.7894e-01, -4.1327e-01,  ...,  2.4986e-01,\n",
      "           -8.0750e-01, -6.4745e-02],\n",
      "          [-7.9047e-01, -1.7042e-01, -1.4880e+00,  ..., -4.5465e-01,\n",
      "            2.0031e-01, -1.1939e-01],\n",
      "          [-3.8159e-02, -7.3892e-01, -2.1772e+00,  ...,  1.0458e-01,\n",
      "           -1.6639e-01, -1.4125e+00]]],\n",
      "\n",
      "\n",
      "        [[[ 8.7614e-01, -6.2977e-01,  9.4096e-01,  ...,  1.2407e+00,\n",
      "            2.9695e-01, -1.3982e+00],\n",
      "          [ 1.7444e+00, -5.1398e-01, -9.7597e-01,  ...,  2.4702e-01,\n",
      "            1.5838e+00,  7.7005e-03],\n",
      "          [ 1.3959e-01,  3.4418e-01, -3.2299e-01,  ..., -2.7816e-01,\n",
      "            1.7241e-01, -2.1410e-02],\n",
      "          ...,\n",
      "          [ 8.5251e-01,  8.4614e-01,  4.1017e-01,  ...,  8.1212e-01,\n",
      "           -2.0081e+00, -8.2542e-01],\n",
      "          [-5.4666e-01,  2.6098e-01,  2.9935e-01,  ..., -2.3319e+00,\n",
      "            1.2413e+00, -1.2241e+00],\n",
      "          [ 1.1497e+00,  1.4702e-01,  4.7798e-01,  ..., -4.3513e-01,\n",
      "            1.2729e+00, -4.1911e-01]],\n",
      "\n",
      "         [[-3.8900e-02,  7.9494e-01,  4.3629e-01,  ..., -3.8144e-01,\n",
      "            1.1800e+00,  5.3651e-01],\n",
      "          [-5.1057e-02,  5.3833e-01,  8.3465e-03,  ...,  1.4837e-01,\n",
      "            5.5719e-01, -1.1941e-01],\n",
      "          [-2.5391e-02, -8.8171e-01,  1.7753e+00,  ...,  1.4239e+00,\n",
      "           -1.4830e-01,  5.4395e-01],\n",
      "          ...,\n",
      "          [ 1.0169e+00,  6.1299e-01, -1.4383e+00,  ...,  3.2929e-01,\n",
      "           -1.4469e+00,  2.6483e-01],\n",
      "          [ 4.2004e-01, -1.6553e+00, -5.4005e-01,  ..., -1.3144e+00,\n",
      "           -1.0573e+00,  5.0748e-01],\n",
      "          [ 4.0150e-01, -1.4873e-01, -1.0879e-01,  ...,  2.2621e-01,\n",
      "            2.5360e-01, -1.3293e+00]],\n",
      "\n",
      "         [[-9.3941e-01,  1.4929e-02,  2.3855e+00,  ...,  1.2379e+00,\n",
      "            6.1759e-01,  6.1497e-01],\n",
      "          [-4.8008e-01,  5.9598e-01,  7.6166e-01,  ...,  5.3496e-02,\n",
      "            6.6294e-01, -7.1153e-01],\n",
      "          [-5.1961e-03, -1.2119e+00, -5.9247e-02,  ...,  2.3899e-01,\n",
      "            9.5494e-01, -2.1342e+00],\n",
      "          ...,\n",
      "          [-4.0269e-01, -1.5313e+00, -1.5853e-01,  ...,  5.2820e-02,\n",
      "           -9.2709e-01,  2.5747e-01],\n",
      "          [-1.1099e+00,  4.3811e-01,  1.0911e+00,  ...,  1.5466e-01,\n",
      "           -5.4773e-01,  1.5314e-01],\n",
      "          [ 5.8827e-01,  4.5065e-01, -1.3872e+00,  ..., -1.4326e+00,\n",
      "           -1.4345e+00,  7.8464e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[-3.5243e-01, -4.5510e-01,  9.0690e-01,  ..., -4.2235e-01,\n",
      "           -9.4153e-01, -8.4694e-01],\n",
      "          [-4.9660e-02,  1.1577e-01, -8.9286e-01,  ..., -2.4355e+00,\n",
      "            8.5357e-01,  4.0946e-01],\n",
      "          [-1.5895e-01,  1.5419e+00, -3.4918e-01,  ...,  4.9339e-01,\n",
      "           -1.7217e-01, -6.7171e-02],\n",
      "          ...,\n",
      "          [ 7.9988e-01,  1.1362e+00,  3.4544e-01,  ..., -1.5728e+00,\n",
      "            6.3259e-02,  4.2095e-01],\n",
      "          [ 2.6642e-01,  1.1303e+00, -4.5311e-01,  ...,  1.8240e+00,\n",
      "            1.1053e+00, -6.7545e-03],\n",
      "          [-9.8470e-01, -6.4204e-01,  1.0175e+00,  ..., -6.4353e-01,\n",
      "            6.6074e-01, -1.5084e+00]],\n",
      "\n",
      "         [[-3.1000e-02, -5.3707e-01,  2.0153e-01,  ..., -2.6687e-01,\n",
      "            1.2745e+00, -3.5471e-01],\n",
      "          [ 1.5317e+00, -1.2322e-01,  1.6293e+00,  ..., -5.4362e-01,\n",
      "            1.4842e+00,  1.6600e-01],\n",
      "          [ 2.9704e-01,  7.5719e-02,  2.1828e+00,  ...,  7.0357e-01,\n",
      "           -8.3143e-01,  1.3201e+00],\n",
      "          ...,\n",
      "          [ 1.4054e-01,  5.0913e-01,  7.5642e-01,  ..., -7.0894e-01,\n",
      "            6.9898e-01,  9.2972e-01],\n",
      "          [ 1.5546e+00, -7.4479e-02,  2.1533e-01,  ...,  1.4873e+00,\n",
      "           -6.7221e-01,  6.7286e-01],\n",
      "          [-9.4746e-01, -1.5242e+00,  1.5563e+00,  ...,  5.1718e-01,\n",
      "            1.2254e+00,  2.5161e-01]],\n",
      "\n",
      "         [[-6.9411e-01,  1.6379e-01, -5.7268e-01,  ..., -1.1764e+00,\n",
      "           -4.3440e-01, -2.2777e-01],\n",
      "          [ 3.1157e-01, -9.6536e-01, -6.6165e-01,  ..., -1.5249e+00,\n",
      "           -1.7952e+00,  9.1592e-01],\n",
      "          [-5.4626e-01, -6.6720e-01,  2.7824e-01,  ..., -1.8709e-01,\n",
      "            2.3119e+00,  1.8289e+00],\n",
      "          ...,\n",
      "          [-2.3540e+00,  6.8343e-01,  1.1850e+00,  ..., -1.6070e+00,\n",
      "           -1.3033e+00, -2.0679e+00],\n",
      "          [ 1.7667e-01,  8.1273e-01, -7.7061e-01,  ...,  7.4352e-02,\n",
      "            7.0597e-01, -8.5862e-01],\n",
      "          [ 1.7550e+00, -1.6291e+00,  5.8844e-01,  ...,  5.7073e-01,\n",
      "            5.0824e-01, -7.1260e-01]]],\n",
      "\n",
      "\n",
      "        [[[ 1.7990e-01, -3.3493e-01, -3.9123e-02,  ..., -9.0944e-01,\n",
      "            3.4982e-02, -4.9070e-01],\n",
      "          [-1.4650e+00, -3.2437e-01, -8.2251e-01,  ..., -5.3759e-01,\n",
      "            1.4882e+00,  7.9933e-01],\n",
      "          [-7.6253e-01, -2.7307e-01,  1.1436e-02,  ..., -4.6804e-02,\n",
      "            1.6072e+00,  7.0831e-01],\n",
      "          ...,\n",
      "          [ 7.9591e-01, -7.7842e-01, -1.0012e+00,  ..., -2.3889e-01,\n",
      "            1.4585e+00,  1.9893e-01],\n",
      "          [ 5.0361e-01,  2.1853e+00,  1.4561e+00,  ...,  7.6568e-01,\n",
      "            3.5713e-01, -3.4790e-01],\n",
      "          [-4.2058e-01, -7.5984e-01,  4.1630e-01,  ...,  4.8968e-01,\n",
      "            3.4105e-02, -7.5992e-01]],\n",
      "\n",
      "         [[ 3.4868e-01,  1.0891e+00, -9.7641e-01,  ..., -7.7717e-01,\n",
      "            6.6780e-01, -6.7120e-01],\n",
      "          [ 2.7293e-01,  5.0103e-01,  3.8806e-01,  ...,  4.3071e-01,\n",
      "           -4.1729e-01, -6.7415e-01],\n",
      "          [-1.0155e+00, -1.7737e-01, -4.1157e-01,  ...,  1.5145e-01,\n",
      "           -1.3438e+00,  1.1371e+00],\n",
      "          ...,\n",
      "          [-1.0843e+00, -3.4196e-02, -1.0229e+00,  ...,  3.4340e-01,\n",
      "           -2.0681e-01,  1.6799e+00],\n",
      "          [ 5.6433e-01,  4.2852e-01,  2.3970e+00,  ...,  4.5749e-01,\n",
      "            1.5577e+00,  6.3868e-01],\n",
      "          [ 7.0619e-01,  7.4621e-02,  2.0631e-01,  ..., -5.4521e-01,\n",
      "           -1.1062e+00,  1.3072e+00]],\n",
      "\n",
      "         [[-1.6663e+00, -1.8036e+00,  1.7213e-01,  ...,  5.3642e-01,\n",
      "           -8.1743e-01, -3.9892e-01],\n",
      "          [ 3.0293e-01, -1.3084e+00, -1.5078e+00,  ...,  4.8564e-01,\n",
      "            6.8733e-01,  7.5661e-01],\n",
      "          [ 5.8540e-01, -2.1725e+00, -1.1180e+00,  ...,  3.7424e-01,\n",
      "            1.6222e+00,  6.5251e-01],\n",
      "          ...,\n",
      "          [-9.7189e-02,  3.1591e-02, -1.3956e+00,  ...,  1.0621e+00,\n",
      "            3.0530e-01,  3.0428e-03],\n",
      "          [-1.3552e+00,  1.4357e-01, -1.0563e+00,  ..., -3.0253e-01,\n",
      "            3.3145e-01, -6.6239e-01],\n",
      "          [ 1.0378e+00, -1.8561e+00,  1.0089e+00,  ...,  2.7994e-01,\n",
      "            1.6545e+00,  6.0277e-01]],\n",
      "\n",
      "         ...,\n",
      "\n",
      "         [[ 4.6125e-01,  1.1825e+00,  3.5098e-01,  ...,  1.8999e-01,\n",
      "           -9.2302e-01,  1.4789e+00],\n",
      "          [-1.7506e+00,  4.8572e-01, -2.3355e-01,  ...,  2.8214e-01,\n",
      "            9.0253e-01,  1.0595e+00],\n",
      "          [-8.9129e-01, -1.3720e+00, -1.9266e+00,  ..., -7.3689e-01,\n",
      "           -7.1670e-01, -3.7454e-01],\n",
      "          ...,\n",
      "          [ 5.3714e-02,  1.3658e-02, -6.9606e-02,  ..., -3.7166e-01,\n",
      "            4.4435e-01,  9.2191e-01],\n",
      "          [ 7.4394e-02, -1.0834e+00,  1.5173e+00,  ...,  3.0055e-02,\n",
      "           -1.3850e+00, -1.0912e-01],\n",
      "          [ 6.6573e-01,  2.2987e-01, -2.1334e-01,  ...,  1.8720e-01,\n",
      "           -1.4672e+00, -1.3824e+00]],\n",
      "\n",
      "         [[-1.1729e+00,  1.0643e-01, -5.1306e-01,  ..., -1.0420e+00,\n",
      "           -6.2000e-01,  7.3910e-01],\n",
      "          [-2.4575e+00, -1.3502e+00, -5.6168e-01,  ..., -2.3278e+00,\n",
      "           -1.5940e-01, -2.2218e-01],\n",
      "          [-8.5488e-01, -7.5010e-01,  4.4618e-01,  ..., -1.2557e+00,\n",
      "           -8.4931e-01,  1.1323e+00],\n",
      "          ...,\n",
      "          [-1.4814e+00, -3.0807e-01,  1.0968e+00,  ...,  2.3708e-01,\n",
      "            6.2906e-01, -7.6115e-01],\n",
      "          [ 2.5451e-01, -7.8220e-01, -1.4842e-01,  ..., -5.2833e-01,\n",
      "            7.4955e-01, -4.9731e-01],\n",
      "          [-1.0424e+00, -2.3834e-01,  1.8441e+00,  ..., -1.5741e+00,\n",
      "           -1.2628e-01, -1.2554e+00]],\n",
      "\n",
      "         [[ 7.6326e-01, -7.3113e-01, -1.9063e+00,  ..., -7.8373e-02,\n",
      "            1.5896e-01, -3.2879e-01],\n",
      "          [ 1.6852e+00,  6.7035e-02,  2.9552e-01,  ..., -4.5468e-01,\n",
      "            3.2984e-01, -1.4947e+00],\n",
      "          [-2.9142e-01, -5.8461e-01,  1.0099e+00,  ...,  1.3861e+00,\n",
      "           -7.2421e-01, -1.4550e+00],\n",
      "          ...,\n",
      "          [-1.3755e+00,  8.6030e-01,  8.1819e-01,  ..., -1.5448e-01,\n",
      "            1.6657e+00, -4.5099e-01],\n",
      "          [ 5.2402e-01,  1.1558e+00, -7.7650e-01,  ..., -3.7305e-01,\n",
      "            1.0579e+00, -4.8566e-01],\n",
      "          [-9.6347e-02,  1.4147e+00, -4.1544e-01,  ...,  1.4028e-01,\n",
      "           -6.1628e-01, -1.1229e+00]]]])\n"
     ]
    }
   ],
   "source": [
    "import h5py\n",
    "import torch\n",
    "import numpy as np\n",
    "\n",
    "indices = np.arange(0,100,1) \n",
    "\n",
    "\n",
    "with h5py.File('tensor_data.h5', 'r') as hf:\n",
    "    \n",
    "    total_size = hf['data'].shape[0]\n",
    "    print(total_size)\n",
    "    \n",
    "    \n",
    "    subset_tensors = []\n",
    "    \n",
    "    \n",
    "    chunk_size = 100  \n",
    "    \n",
    "    \n",
    "    for start_index in range(0, total_size, chunk_size):\n",
    "        end_index = min(start_index + chunk_size, total_size)\n",
    "        \n",
    "\n",
    "        data_chunk = hf['data'][start_index:end_index]\n",
    "        print(data_chunk.shape)\n",
    "      \n",
    "        chunk_subset = torch.tensor(data_chunk[indices])\n",
    "        print(chunk_subset.shape)\n",
    "        \n",
    "\n",
    "        subset_tensors.append(chunk_subset)\n",
    "        \n",
    "\n",
    "subset_tensor = torch.cat(subset_tensors,dim =0)\n",
    "subset_tensor.squeeze(0)\n",
    "\n",
    "print(\"Taille du subset_tensor:\", subset_tensor.size())\n",
    "\n",
    "\n",
    "print(subset_tensor)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import sys\n",
    "import torch\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../util')\n",
    "from abstractNN import AbstractNN as NN\n",
    "from abstract import abstractTensor as AT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_to_abstract = torch.randn(1)\n",
    "abstract_tensor = AT(tensor_to_abstract,alpha=0.03*torch.ones(1)).abstract_tensor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.0871],\n",
       "        [ 0.0300],\n",
       "        [ 0.0000]])"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "lin = nn.Sequential(nn.Flatten(),nn.Linear(in_features=1,out_features=2))\n",
    "lin[1].weight.data= torch.tensor([[1.],[-1.]])\n",
    "lin[1].bias.data = torch.zeros_like(lin[1].bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "selected index k out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[168], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtopk\u001b[49m\u001b[43m(\u001b[49m\u001b[43mlin\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mflatten\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mvalues\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mtopk(lin[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mweight\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mflatten(), k \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m10\u001b[39m)\n",
      "\u001b[0;31mRuntimeError\u001b[0m: selected index k out of range"
     ]
    }
   ],
   "source": [
    "torch.topk(lin[1].weight.data.flatten(), k = 10).values\n",
    "torch.topk(lin[1].weight.data.flatten(), k = 10)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 198,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.return_types.topk(\n",
       "values=tensor([1., 1.]),\n",
       "indices=tensor([0, 1]))"
      ]
     },
     "execution_count": 198,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lin_eps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0, 1])\n",
      "tensor([1, 1])\n",
      "torch.Size([2, 2])\n",
      "torch.Size([2, 2])\n"
     ]
    }
   ],
   "source": [
    "lin_eps_weight =torch.tensor([])\n",
    "lin_eps_weight.indices = torch.topk(lin[1].weight.data.flatten(),2).indices\n",
    "\n",
    "print(lin_eps.indices)\n",
    "lin_eps_weight.values = torch.ones_like(lin_eps_weight.indices)\n",
    "print(lin_eps_weight.values)\n",
    "\n",
    "\n",
    "lin_eps_bias = torch.tensor([])\n",
    "lin_eps_bias.indices = torch.topk(lin[1].bias.data,2).indices\n",
    "lin_eps_bias.values = torch.ones_like(lin_eps_bias.indices)\n",
    "x,x_min,x_max,x_true = NN.abstract_linear(lin, \n",
    "                                          abstract_tensor,\n",
    "                                          abstract_tensor[0].unsqueeze(0),\n",
    "                                          lin_eps_weight,\n",
    "                                          lin_eps_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([7, 2])"
      ]
     },
     "execution_count": 224,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0937],\n",
       "        [ 0.0300],\n",
       "        [ 0.0000]])"
      ]
     },
     "execution_count": 225,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-3.0937,  3.0937],\n",
       "        [ 0.0300, -0.0300],\n",
       "        [-3.0937,  0.0000],\n",
       "        [ 0.0000, -3.0937],\n",
       "        [ 1.0000,  0.0000],\n",
       "        [ 0.0000,  1.0000],\n",
       "        [ 0.0000,  0.0000]])"
      ]
     },
     "execution_count": 226,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "conv = nn.Conv2d(2,2,2)\n",
    "conv.weight.data = torch.zeros_like(conv.weight.data)\n",
    "conv.bias.data = torch.zeros_like(conv.bias.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([10, 11])\n",
      "tensor([1, 1])\n",
      "torch.Size([2, 2, 1, 1])\n",
      "torch.Size([2, 2, 1, 1])\n"
     ]
    }
   ],
   "source": [
    "tensor_to_abstract = torch.randn(2,2,2)\n",
    "abstract_tensor = AT(tensor_to_abstract,alpha=0.03*torch.ones(2*1*2)).abstract_tensor()\n",
    "\n",
    "conv_eps_weight =torch.tensor([])\n",
    "conv_eps_weight.indices = torch.topk(conv.weight.data.flatten(),k =2).indices\n",
    "\n",
    "print(conv_eps_weight.indices)\n",
    "conv_eps_weight.values = torch.ones_like(conv_eps_weight.indices)\n",
    "print(conv_eps_weight.values)\n",
    "\n",
    "\n",
    "conv_eps_bias = torch.tensor([])\n",
    "conv_eps_bias.indices = torch.topk(conv.bias.data,2).indices\n",
    "conv_eps_bias.values = torch.ones_like(conv_eps_bias.indices)\n",
    "x,x_min,x_max,x_true = NN.abstract_conv2D(conv, \n",
    "                                          abstract_tensor,\n",
    "                                          abstract_tensor[0].unsqueeze(0),\n",
    "                                          conv_eps_weight,\n",
    "                                          conv_eps_bias)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.5851, -0.1173],\n",
       "         [ 0.5130,  0.1623]],\n",
       "\n",
       "        [[ 1.5964,  0.3390],\n",
       "         [-0.8472,  0.4738]]])"
      ]
     },
     "execution_count": 246,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "abstract_tensor[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[-0.1608]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[-0.4494]]],\n",
       "\n",
       "\n",
       "        [[[ 1.0000]],\n",
       "\n",
       "         [[ 0.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 1.0000]]],\n",
       "\n",
       "\n",
       "        [[[ 0.0000]],\n",
       "\n",
       "         [[ 0.0000]]]])"
      ]
     },
     "execution_count": 254,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
