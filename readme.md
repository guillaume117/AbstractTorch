### AbstractTorch ğŸ¦¸

![alt text](images/image.png)
Ceci est un prototype de moteur d'Ã©valuation s'appuyant sur Torch ğŸ˜

L'idÃ©e originale consiste Ã  utiliser la notion bien connue de batch pour faire l'Ã©valuation d'un modÃ¨le. Cette astuce permet d'obtenir des rÃ©sultats rapides ne nÃ©cessitant pas de refaire un modÃ¨le complexe. 
Par ailleurs, un autre avantage rÃ©side dans le fait que ce moteur d'Ã©valuation permet de rÃ©aliser les calculs indisctinctement sur CPU ou sur GPU. 

On surcharge nn.Linear avec des mÃ©thodes hybrides, gÃ©rant un flux abstrait pour l'Ã©valuation et un flux concret. La simultanÃ©itÃ© des deux flux permet d'oberver la position du centre du zonotope par rapport Ã  la valeur rÃ©elle de la sortie de la fonction. 

L'implÃ©mentation du modÃ¨le abstrait comporte une quantitÃ© fixe de symboles qui sont gÃ©rÃ©s comme des Ã©paisseurs de batch. La derniÃ¨re Ã©paisseur de batch correspond au symbole poubelle, les opÃ©ration linÃ©aires sont opÃ©rÃ©es pour cette Ã©paisseur par la valeur absolu de la matrice des poids. 

L'option add_symbol permet de gÃ©nÃ©rer de nouveaux symboles/ 
L'infÃ©rence peut se faire soit sur cpu (device=torch.device('cpu')) ou carte graphique (device = torch.device('cuda'ou 'mps'))

Pour l'instant sont implÃ©mentÃ©es les classes conv2D, Linear , maxpool2D(noyau 2) et ReLU.
        
# New feature :

 	ğŸ¥³ğŸ¥³ğŸ¥³ avgpool est implÃ©mentÃ©e 	ğŸ¥³ğŸ¥³ğŸ¥³     

On tire profit de la structure de base
de la mÃ©thode forward. Au lieu de considÃ©rer un batch, on considÃ¨re une entrÃ©e en dimension 0 avec dans les dimensions habituelles du batch des couches de symbole. Une couche (un Ã©paisseur de batch) reprÃ©sente
un symbole abstrait. La derniÃ¨re couche correspond au symbole poubelle. 


La couche 0 reprÃ©sente le centre du zonotope
Les couches suivantes reprÃ©sentent les symboles. Elles sont calculÃ©es pour les opÃ©ration linÃ©aires (Linear et Conv2D) par 
$$\textbf{W}(x_\epsilon)$$
    x[1:]=lin(x_epsilon)-lin(torch.zeros_like(x_epsilon))

La derniere couche est toujours celle du bruit poubelle. Sur cette couche uniquement, les opÃ©rations linÃ©aires  sont  calculÃ©es de la faÃ§on suivantes: 

$$\textbf{|W|}(x_{\epsilon_{noise}})$$


Pour implÃ©menter le tenseur linÃ©aire reprÃ©sentant la valeur absolue, on duplique la couche lin ou conv et on applique la valeur absolue Ã  la matrice de poids. 


Cette derniÃ¨re couche peut Ãªtre nulle si les symboles gÃ©nÃ©rÃ©s sont projetÃ©s sur une nouvelle dimension. 


## ImplÃ©mentation
Une classe abstractModule permet de rÃ©aliser les diffÃ©rentes opÃ©rations abstraites. 
Chacune des mÃ©thodes doit prendre en argument (centre,valeur_min,valeur_max,valeur vraie) et retourner (centre,valeur_min,valeur_max,valeur vraie). Si les arguments x_min et x_max n'ont aucune importance pour les couches linÃ©aire, cette standardisation facilite l'Ã©criture d'une mÃ©thode abstract_forward.

Une classe abstractWeight permet de tester un domaine abstrait dont les formes affines sont issues des poids d'une couche fully connected. 
 
## Empreinte mÃ©moire ğŸ§‘â€ğŸ¦½â€â¡ï¸
AbstractTorch est gourmand, trÃ¨s gourmand:
Un tenseur torch de dimension $N * C * H * W$ en float 32 possÃ¨de une empreinte mÃ©moire de  $N * C * H * W *4 *10^{-9}$ GB
Un domaine abstrait basÃ©e sur une image de taille 3 * 224 * 224 gÃ©nÃ¨re une empreinte d'environ 90 GB. 
si l'on applique une couche de convolution de noyau 64 , on aura un tenseur de 2 TO. Le code essaie d'Ã©viter les copies intÃ©grales du tenseur abstrait en cours d'Ã©valuation, la mise Ã  jours des variables est faite rÃ©curssivement dans les classes ReLU (x[index]=k*x[index]).

ğŸƒLa bonne nouvelle c'est qu'on peut borner l'empreinte mÃ©moire (caractÃ©ristique Ã  venir, ğŸ‘·... )




