{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "from abstractModule import AbstractLinear as AL\n",
    "from abstractModule import AbstractReLU as AR\n",
    "\n",
    "from util.train import Train as T\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision import datasets\n",
    "from torch.utils.data import random_split\n",
    "from torch import optim\n",
    "import os\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "path ='dataset'\n",
    "\n",
    "os.makedirs(path, exist_ok=True)\n",
    "\n",
    "transform = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize([0.5], [0.2]),\n",
    "        #transforms.Resize((56,56))\n",
    "    ])\n",
    "\n",
    "\n",
    "\n",
    "dataset_train = datasets.FashionMNIST(root = path,transform = transform, download = True, train = True)\n",
    "dataset_test =datasets.FashionMNIST( root =path,transform=transform ,download = True, train = False)\n",
    "val =0.2\n",
    "len_data_train = len(dataset_train)\n",
    "train_size =int((1-val)*len_data_train)\n",
    "\n",
    "val_size = int(val*len_data_train)\n",
    "\n",
    "train_dataset, val_dataset = random_split(dataset_train, [train_size,val_size])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self,num_classes=10, num_depth=1):\n",
    "\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.num_classes=num_classes\n",
    "        self.conv_block1 = nn.Sequential(\n",
    "            nn.Conv2d(in_channels=num_depth, out_channels=16, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "            nn.Conv2d(in_channels=16, out_channels=32, kernel_size=3),\n",
    "            nn.ReLU(),\n",
    "          \n",
    "        )\n",
    "           \n",
    "   \n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Flatten(),\n",
    "            nn.Linear(in_features=18432, out_features=64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(p=0.1),\n",
    "            nn.Linear(in_features=64, out_features=self.num_classes),\n",
    "        )\n",
    "        self.softMax =nn.Softmax()\n",
    "    def forward(self, x):\n",
    "        x = self.conv_block1(x)\n",
    " \n",
    "        x = self.classifier(x)\n",
    "        x = self.softMax(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "class AbstractNN(nn.Module):\n",
    "    \n",
    "    def __init__(self,num_depth=1,device=torch.device(\"cpu\")):\n",
    "\n",
    "        super(AbstractNN,self).__init__()\n",
    "       \n",
    "      \n",
    "        self.num_depth = num_depth\n",
    "        self.device = device\n",
    "        self.conv1=nn.Conv2d(self.num_depth,16,3,device=self.device)\n",
    "        self.conv2=nn.Conv2d(16,32,3,device=self.device)\n",
    "       \n",
    "\n",
    "        self.fc1=nn.Sequential(nn.Flatten(),nn.Linear(18432,64,device=self.device))\n",
    "        self.fc2=nn.Sequential(nn.Flatten(),nn.Linear(64,10,device=self.device))\n",
    "        self.softMax =nn.Softmax()\n",
    "    def forward(self,x):\n",
    "        x=self.conv1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.conv2(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.fc1(x)\n",
    "        x=torch.relu(x)\n",
    "        x=self.fc2(x)\n",
    "        x=torch.relu(x)\n",
    "        x= self.softMax(x)\n",
    "        return x\n",
    "    \n",
    "    def abstract_forward(self,x,add_symbol=False,device=torch.device(\"cpu\")):\n",
    "        self.device=device\n",
    "        \n",
    "        x_true = x\n",
    "        x_true = x_true[0].unsqueeze(0)\n",
    "        print(f\"lenx:{len(x)}\")\n",
    "        x,x_min,x_max,x_true = AL.abstract_conv2D(self.conv1,x,x_true,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print(f\"lenx:{len(x)}\")\n",
    "       \n",
    "        x,x_min,x_max,x_true = AL.abstract_conv2D(self.conv2,x,x_true,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu_conv2D(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print(f\"lenx:{len(x)}\")\n",
    "        \n",
    "        x,x_min,x_max,x_true = AL.abstract_linear(self.fc1,x,x_true,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print(f\"lenx:{len(x)}\")\n",
    "        x,x_min,x_max,x_true = AL.abstract_linear(self.fc2,x,x_true,device=self.device)\n",
    "        x,x_min,x_max,x_true = AR.abstract_relu(x,x_min,x_max,x_true,add_symbol=add_symbol,device =self.device)\n",
    "        print(f\"lenx:{len(x)}\")\n",
    "        \n",
    "        return x,x_min,x_max,x_true\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 0/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_0=2.30, accuracy = 7.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guillaume/.local/lib/python3.10/site-packages/torch/nn/modules/module.py:1511: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  return self._call_impl(*args, **kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_loss 100_Epoch_0=1.80, accuracy = 65.62%\n",
      "batch_loss 200_Epoch_0=1.76, accuracy = 69.53%\n",
      "batch_loss 300_Epoch_0=1.71, accuracy = 74.22%\n",
      "EPOCH ACCURACY = 54.10 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6515 Acc: 54.1016\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_0=1.64, accuracy = 82.03%\n",
      "batch_loss 2_Epoch_0=1.66, accuracy = 79.69%\n",
      "batch_loss 3_Epoch_0=1.62, accuracy = 84.38%\n",
      "batch_loss 4_Epoch_0=1.74, accuracy = 71.09%\n",
      "batch_loss 5_Epoch_0=1.72, accuracy = 75.00%\n",
      "batch_loss 6_Epoch_0=1.73, accuracy = 74.22%\n",
      "batch_loss 7_Epoch_0=1.68, accuracy = 78.91%\n",
      "batch_loss 8_Epoch_0=1.66, accuracy = 80.47%\n",
      "batch_loss 9_Epoch_0=1.65, accuracy = 81.25%\n",
      "batch_loss 10_Epoch_0=1.71, accuracy = 75.78%\n",
      "batch_loss 11_Epoch_0=1.70, accuracy = 76.56%\n",
      "batch_loss 12_Epoch_0=1.70, accuracy = 77.34%\n",
      "batch_loss 13_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 14_Epoch_0=1.73, accuracy = 71.88%\n",
      "batch_loss 15_Epoch_0=1.69, accuracy = 75.78%\n",
      "batch_loss 16_Epoch_0=1.64, accuracy = 82.81%\n",
      "batch_loss 17_Epoch_0=1.72, accuracy = 73.44%\n",
      "batch_loss 18_Epoch_0=1.65, accuracy = 78.91%\n",
      "batch_loss 19_Epoch_0=1.65, accuracy = 80.47%\n",
      "batch_loss 20_Epoch_0=1.71, accuracy = 74.22%\n",
      "batch_loss 21_Epoch_0=1.65, accuracy = 82.03%\n",
      "batch_loss 22_Epoch_0=1.65, accuracy = 83.59%\n",
      "batch_loss 23_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 24_Epoch_0=1.66, accuracy = 79.69%\n",
      "batch_loss 25_Epoch_0=1.64, accuracy = 82.03%\n",
      "batch_loss 26_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 27_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 28_Epoch_0=1.68, accuracy = 79.69%\n",
      "batch_loss 29_Epoch_0=1.58, accuracy = 89.06%\n",
      "batch_loss 30_Epoch_0=1.69, accuracy = 76.56%\n",
      "batch_loss 31_Epoch_0=1.69, accuracy = 78.91%\n",
      "batch_loss 32_Epoch_0=1.73, accuracy = 73.44%\n",
      "batch_loss 33_Epoch_0=1.68, accuracy = 76.56%\n",
      "batch_loss 34_Epoch_0=1.71, accuracy = 74.22%\n",
      "batch_loss 35_Epoch_0=1.64, accuracy = 82.03%\n",
      "batch_loss 36_Epoch_0=1.68, accuracy = 77.34%\n",
      "batch_loss 37_Epoch_0=1.69, accuracy = 76.56%\n",
      "batch_loss 38_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 39_Epoch_0=1.66, accuracy = 80.47%\n",
      "batch_loss 40_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 41_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 42_Epoch_0=1.66, accuracy = 80.47%\n",
      "batch_loss 43_Epoch_0=1.75, accuracy = 70.31%\n",
      "batch_loss 44_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 45_Epoch_0=1.68, accuracy = 76.56%\n",
      "batch_loss 46_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 47_Epoch_0=1.67, accuracy = 79.69%\n",
      "batch_loss 48_Epoch_0=1.74, accuracy = 71.88%\n",
      "batch_loss 49_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 50_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 51_Epoch_0=1.65, accuracy = 80.47%\n",
      "batch_loss 52_Epoch_0=1.65, accuracy = 81.25%\n",
      "batch_loss 53_Epoch_0=1.75, accuracy = 70.31%\n",
      "batch_loss 54_Epoch_0=1.75, accuracy = 70.31%\n",
      "batch_loss 55_Epoch_0=1.67, accuracy = 79.69%\n",
      "batch_loss 56_Epoch_0=1.67, accuracy = 77.34%\n",
      "batch_loss 57_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 58_Epoch_0=1.66, accuracy = 81.25%\n",
      "batch_loss 59_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 60_Epoch_0=1.66, accuracy = 79.69%\n",
      "batch_loss 61_Epoch_0=1.72, accuracy = 75.00%\n",
      "batch_loss 62_Epoch_0=1.69, accuracy = 76.56%\n",
      "batch_loss 63_Epoch_0=1.66, accuracy = 80.47%\n",
      "batch_loss 64_Epoch_0=1.71, accuracy = 75.78%\n",
      "batch_loss 65_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 66_Epoch_0=1.67, accuracy = 79.69%\n",
      "batch_loss 67_Epoch_0=1.73, accuracy = 71.88%\n",
      "batch_loss 68_Epoch_0=1.66, accuracy = 80.47%\n",
      "batch_loss 69_Epoch_0=1.72, accuracy = 74.22%\n",
      "batch_loss 70_Epoch_0=1.70, accuracy = 75.78%\n",
      "batch_loss 71_Epoch_0=1.69, accuracy = 76.56%\n",
      "batch_loss 72_Epoch_0=1.70, accuracy = 76.56%\n",
      "batch_loss 73_Epoch_0=1.73, accuracy = 72.66%\n",
      "batch_loss 74_Epoch_0=1.65, accuracy = 81.25%\n",
      "batch_loss 75_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 76_Epoch_0=1.70, accuracy = 76.56%\n",
      "batch_loss 77_Epoch_0=1.65, accuracy = 82.03%\n",
      "batch_loss 78_Epoch_0=1.69, accuracy = 76.56%\n",
      "batch_loss 79_Epoch_0=1.73, accuracy = 72.66%\n",
      "batch_loss 80_Epoch_0=1.66, accuracy = 79.69%\n",
      "batch_loss 81_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 82_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 83_Epoch_0=1.70, accuracy = 75.78%\n",
      "batch_loss 84_Epoch_0=1.75, accuracy = 70.31%\n",
      "batch_loss 85_Epoch_0=1.66, accuracy = 81.25%\n",
      "batch_loss 86_Epoch_0=1.69, accuracy = 77.34%\n",
      "batch_loss 87_Epoch_0=1.63, accuracy = 82.03%\n",
      "batch_loss 88_Epoch_0=1.65, accuracy = 79.69%\n",
      "batch_loss 89_Epoch_0=1.67, accuracy = 78.91%\n",
      "batch_loss 90_Epoch_0=1.68, accuracy = 78.12%\n",
      "batch_loss 91_Epoch_0=1.63, accuracy = 83.59%\n",
      "batch_loss 92_Epoch_0=1.64, accuracy = 82.03%\n",
      "batch_loss 93_Epoch_0=1.67, accuracy = 78.91%\n",
      "EPOCH ACCURACY = 77.91 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6718 Acc: 77.9066\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 1/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 100_Epoch_1=1.71, accuracy = 75.00%\n",
      "batch_loss 200_Epoch_1=1.69, accuracy = 77.34%\n",
      "batch_loss 300_Epoch_1=1.72, accuracy = 71.88%\n",
      "EPOCH ACCURACY = 76.56 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.7318 Acc: 76.5625\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_1=1.63, accuracy = 80.47%\n",
      "batch_loss 2_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 3_Epoch_1=1.66, accuracy = 78.91%\n",
      "batch_loss 4_Epoch_1=1.68, accuracy = 77.34%\n",
      "batch_loss 5_Epoch_1=1.71, accuracy = 72.66%\n",
      "batch_loss 6_Epoch_1=1.65, accuracy = 78.91%\n",
      "batch_loss 7_Epoch_1=1.63, accuracy = 82.81%\n",
      "batch_loss 8_Epoch_1=1.67, accuracy = 77.34%\n",
      "batch_loss 9_Epoch_1=1.61, accuracy = 83.59%\n",
      "batch_loss 10_Epoch_1=1.67, accuracy = 77.34%\n",
      "batch_loss 11_Epoch_1=1.63, accuracy = 82.03%\n",
      "batch_loss 12_Epoch_1=1.68, accuracy = 75.78%\n",
      "batch_loss 13_Epoch_1=1.62, accuracy = 84.38%\n",
      "batch_loss 14_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 15_Epoch_1=1.61, accuracy = 84.38%\n",
      "batch_loss 16_Epoch_1=1.64, accuracy = 80.47%\n",
      "batch_loss 17_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 18_Epoch_1=1.71, accuracy = 74.22%\n",
      "batch_loss 19_Epoch_1=1.61, accuracy = 84.38%\n",
      "batch_loss 20_Epoch_1=1.64, accuracy = 81.25%\n",
      "batch_loss 21_Epoch_1=1.62, accuracy = 82.81%\n",
      "batch_loss 22_Epoch_1=1.62, accuracy = 82.81%\n",
      "batch_loss 23_Epoch_1=1.66, accuracy = 78.91%\n",
      "batch_loss 24_Epoch_1=1.72, accuracy = 72.66%\n",
      "batch_loss 25_Epoch_1=1.65, accuracy = 81.25%\n",
      "batch_loss 26_Epoch_1=1.70, accuracy = 74.22%\n",
      "batch_loss 27_Epoch_1=1.64, accuracy = 81.25%\n",
      "batch_loss 28_Epoch_1=1.64, accuracy = 81.25%\n",
      "batch_loss 29_Epoch_1=1.62, accuracy = 83.59%\n",
      "batch_loss 30_Epoch_1=1.69, accuracy = 75.00%\n",
      "batch_loss 31_Epoch_1=1.74, accuracy = 70.31%\n",
      "batch_loss 32_Epoch_1=1.61, accuracy = 83.59%\n",
      "batch_loss 33_Epoch_1=1.66, accuracy = 79.69%\n",
      "batch_loss 34_Epoch_1=1.63, accuracy = 82.03%\n",
      "batch_loss 35_Epoch_1=1.65, accuracy = 80.47%\n",
      "batch_loss 36_Epoch_1=1.63, accuracy = 83.59%\n",
      "batch_loss 37_Epoch_1=1.63, accuracy = 82.81%\n",
      "batch_loss 38_Epoch_1=1.62, accuracy = 83.59%\n",
      "batch_loss 39_Epoch_1=1.67, accuracy = 78.12%\n",
      "batch_loss 40_Epoch_1=1.70, accuracy = 75.00%\n",
      "batch_loss 41_Epoch_1=1.65, accuracy = 79.69%\n",
      "batch_loss 42_Epoch_1=1.62, accuracy = 84.38%\n",
      "batch_loss 43_Epoch_1=1.66, accuracy = 80.47%\n",
      "batch_loss 44_Epoch_1=1.63, accuracy = 82.03%\n",
      "batch_loss 45_Epoch_1=1.64, accuracy = 80.47%\n",
      "batch_loss 46_Epoch_1=1.69, accuracy = 75.78%\n",
      "batch_loss 47_Epoch_1=1.73, accuracy = 71.88%\n",
      "batch_loss 48_Epoch_1=1.74, accuracy = 71.09%\n",
      "batch_loss 49_Epoch_1=1.66, accuracy = 78.91%\n",
      "batch_loss 50_Epoch_1=1.63, accuracy = 82.81%\n",
      "batch_loss 51_Epoch_1=1.65, accuracy = 80.47%\n",
      "batch_loss 52_Epoch_1=1.63, accuracy = 81.25%\n",
      "batch_loss 53_Epoch_1=1.64, accuracy = 80.47%\n",
      "batch_loss 54_Epoch_1=1.59, accuracy = 86.72%\n",
      "batch_loss 55_Epoch_1=1.62, accuracy = 82.81%\n",
      "batch_loss 56_Epoch_1=1.63, accuracy = 82.81%\n",
      "batch_loss 57_Epoch_1=1.65, accuracy = 78.91%\n",
      "batch_loss 58_Epoch_1=1.68, accuracy = 77.34%\n",
      "batch_loss 59_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 60_Epoch_1=1.64, accuracy = 79.69%\n",
      "batch_loss 61_Epoch_1=1.64, accuracy = 82.03%\n",
      "batch_loss 62_Epoch_1=1.68, accuracy = 76.56%\n",
      "batch_loss 63_Epoch_1=1.68, accuracy = 75.78%\n",
      "batch_loss 64_Epoch_1=1.66, accuracy = 78.91%\n",
      "batch_loss 65_Epoch_1=1.65, accuracy = 78.91%\n",
      "batch_loss 66_Epoch_1=1.68, accuracy = 76.56%\n",
      "batch_loss 67_Epoch_1=1.69, accuracy = 76.56%\n",
      "batch_loss 68_Epoch_1=1.65, accuracy = 80.47%\n",
      "batch_loss 69_Epoch_1=1.65, accuracy = 79.69%\n",
      "batch_loss 70_Epoch_1=1.73, accuracy = 71.88%\n",
      "batch_loss 71_Epoch_1=1.71, accuracy = 73.44%\n",
      "batch_loss 72_Epoch_1=1.66, accuracy = 78.12%\n",
      "batch_loss 73_Epoch_1=1.64, accuracy = 81.25%\n",
      "batch_loss 74_Epoch_1=1.73, accuracy = 71.09%\n",
      "batch_loss 75_Epoch_1=1.69, accuracy = 77.34%\n",
      "batch_loss 76_Epoch_1=1.61, accuracy = 85.16%\n",
      "batch_loss 77_Epoch_1=1.69, accuracy = 75.78%\n",
      "batch_loss 78_Epoch_1=1.67, accuracy = 76.56%\n",
      "batch_loss 79_Epoch_1=1.68, accuracy = 75.78%\n",
      "batch_loss 80_Epoch_1=1.62, accuracy = 82.81%\n",
      "batch_loss 81_Epoch_1=1.63, accuracy = 82.03%\n",
      "batch_loss 82_Epoch_1=1.69, accuracy = 75.00%\n",
      "batch_loss 83_Epoch_1=1.66, accuracy = 78.91%\n",
      "batch_loss 84_Epoch_1=1.59, accuracy = 86.72%\n",
      "batch_loss 85_Epoch_1=1.66, accuracy = 80.47%\n",
      "batch_loss 86_Epoch_1=1.65, accuracy = 80.47%\n",
      "batch_loss 87_Epoch_1=1.64, accuracy = 81.25%\n",
      "batch_loss 88_Epoch_1=1.71, accuracy = 73.44%\n",
      "batch_loss 89_Epoch_1=1.65, accuracy = 79.69%\n",
      "batch_loss 90_Epoch_1=1.72, accuracy = 72.66%\n",
      "batch_loss 91_Epoch_1=1.70, accuracy = 76.56%\n",
      "batch_loss 92_Epoch_1=1.71, accuracy = 74.22%\n",
      "batch_loss 93_Epoch_1=1.67, accuracy = 78.91%\n",
      "EPOCH ACCURACY = 79.25 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6663 Acc: 79.2507\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 2/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 100_Epoch_2=1.66, accuracy = 78.91%\n",
      "batch_loss 200_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 300_Epoch_2=1.60, accuracy = 85.94%\n",
      "EPOCH ACCURACY = 83.01 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6495 Acc: 83.0078\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_2=1.61, accuracy = 84.38%\n",
      "batch_loss 2_Epoch_2=1.70, accuracy = 75.00%\n",
      "batch_loss 3_Epoch_2=1.61, accuracy = 84.38%\n",
      "batch_loss 4_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 5_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 6_Epoch_2=1.72, accuracy = 72.66%\n",
      "batch_loss 7_Epoch_2=1.69, accuracy = 75.78%\n",
      "batch_loss 8_Epoch_2=1.67, accuracy = 77.34%\n",
      "batch_loss 9_Epoch_2=1.67, accuracy = 78.91%\n",
      "batch_loss 10_Epoch_2=1.68, accuracy = 75.78%\n",
      "batch_loss 11_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 12_Epoch_2=1.67, accuracy = 77.34%\n",
      "batch_loss 13_Epoch_2=1.68, accuracy = 75.78%\n",
      "batch_loss 14_Epoch_2=1.64, accuracy = 79.69%\n",
      "batch_loss 15_Epoch_2=1.67, accuracy = 78.12%\n",
      "batch_loss 16_Epoch_2=1.67, accuracy = 78.12%\n",
      "batch_loss 17_Epoch_2=1.63, accuracy = 82.03%\n",
      "batch_loss 18_Epoch_2=1.66, accuracy = 78.12%\n",
      "batch_loss 19_Epoch_2=1.66, accuracy = 78.12%\n",
      "batch_loss 20_Epoch_2=1.67, accuracy = 76.56%\n",
      "batch_loss 21_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 22_Epoch_2=1.63, accuracy = 82.03%\n",
      "batch_loss 23_Epoch_2=1.71, accuracy = 73.44%\n",
      "batch_loss 24_Epoch_2=1.66, accuracy = 78.91%\n",
      "batch_loss 25_Epoch_2=1.64, accuracy = 81.25%\n",
      "batch_loss 26_Epoch_2=1.72, accuracy = 71.88%\n",
      "batch_loss 27_Epoch_2=1.66, accuracy = 78.91%\n",
      "batch_loss 28_Epoch_2=1.69, accuracy = 75.00%\n",
      "batch_loss 29_Epoch_2=1.65, accuracy = 80.47%\n",
      "batch_loss 30_Epoch_2=1.66, accuracy = 77.34%\n",
      "batch_loss 31_Epoch_2=1.70, accuracy = 73.44%\n",
      "batch_loss 32_Epoch_2=1.67, accuracy = 78.12%\n",
      "batch_loss 33_Epoch_2=1.63, accuracy = 82.81%\n",
      "batch_loss 34_Epoch_2=1.65, accuracy = 79.69%\n",
      "batch_loss 35_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 36_Epoch_2=1.68, accuracy = 75.00%\n",
      "batch_loss 37_Epoch_2=1.65, accuracy = 81.25%\n",
      "batch_loss 38_Epoch_2=1.61, accuracy = 84.38%\n",
      "batch_loss 39_Epoch_2=1.65, accuracy = 79.69%\n",
      "batch_loss 40_Epoch_2=1.67, accuracy = 76.56%\n",
      "batch_loss 41_Epoch_2=1.68, accuracy = 76.56%\n",
      "batch_loss 42_Epoch_2=1.64, accuracy = 80.47%\n",
      "batch_loss 43_Epoch_2=1.61, accuracy = 84.38%\n",
      "batch_loss 44_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 45_Epoch_2=1.68, accuracy = 76.56%\n",
      "batch_loss 46_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 47_Epoch_2=1.69, accuracy = 75.78%\n",
      "batch_loss 48_Epoch_2=1.70, accuracy = 74.22%\n",
      "batch_loss 49_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 50_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 51_Epoch_2=1.65, accuracy = 81.25%\n",
      "batch_loss 52_Epoch_2=1.66, accuracy = 78.12%\n",
      "batch_loss 53_Epoch_2=1.63, accuracy = 82.03%\n",
      "batch_loss 54_Epoch_2=1.67, accuracy = 79.69%\n",
      "batch_loss 55_Epoch_2=1.65, accuracy = 80.47%\n",
      "batch_loss 56_Epoch_2=1.67, accuracy = 78.12%\n",
      "batch_loss 57_Epoch_2=1.66, accuracy = 79.69%\n",
      "batch_loss 58_Epoch_2=1.67, accuracy = 77.34%\n",
      "batch_loss 59_Epoch_2=1.65, accuracy = 79.69%\n",
      "batch_loss 60_Epoch_2=1.62, accuracy = 82.03%\n",
      "batch_loss 61_Epoch_2=1.63, accuracy = 83.59%\n",
      "batch_loss 62_Epoch_2=1.68, accuracy = 76.56%\n",
      "batch_loss 63_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 64_Epoch_2=1.64, accuracy = 81.25%\n",
      "batch_loss 65_Epoch_2=1.73, accuracy = 70.31%\n",
      "batch_loss 66_Epoch_2=1.66, accuracy = 78.91%\n",
      "batch_loss 67_Epoch_2=1.69, accuracy = 75.78%\n",
      "batch_loss 68_Epoch_2=1.67, accuracy = 77.34%\n",
      "batch_loss 69_Epoch_2=1.70, accuracy = 73.44%\n",
      "batch_loss 70_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 71_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 72_Epoch_2=1.65, accuracy = 78.91%\n",
      "batch_loss 73_Epoch_2=1.64, accuracy = 80.47%\n",
      "batch_loss 74_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 75_Epoch_2=1.67, accuracy = 78.12%\n",
      "batch_loss 76_Epoch_2=1.60, accuracy = 86.72%\n",
      "batch_loss 77_Epoch_2=1.68, accuracy = 77.34%\n",
      "batch_loss 78_Epoch_2=1.61, accuracy = 83.59%\n",
      "batch_loss 79_Epoch_2=1.70, accuracy = 74.22%\n",
      "batch_loss 80_Epoch_2=1.63, accuracy = 81.25%\n",
      "batch_loss 81_Epoch_2=1.66, accuracy = 78.91%\n",
      "batch_loss 82_Epoch_2=1.68, accuracy = 78.12%\n",
      "batch_loss 83_Epoch_2=1.68, accuracy = 76.56%\n",
      "batch_loss 84_Epoch_2=1.72, accuracy = 73.44%\n",
      "batch_loss 85_Epoch_2=1.63, accuracy = 84.38%\n",
      "batch_loss 86_Epoch_2=1.63, accuracy = 81.25%\n",
      "batch_loss 87_Epoch_2=1.62, accuracy = 83.59%\n",
      "batch_loss 88_Epoch_2=1.59, accuracy = 86.72%\n",
      "batch_loss 89_Epoch_2=1.65, accuracy = 78.91%\n",
      "batch_loss 90_Epoch_2=1.68, accuracy = 75.78%\n",
      "batch_loss 91_Epoch_2=1.65, accuracy = 78.91%\n",
      "batch_loss 92_Epoch_2=1.63, accuracy = 82.03%\n",
      "batch_loss 93_Epoch_2=1.63, accuracy = 82.81%\n",
      "EPOCH ACCURACY = 79.13 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6319 Acc: 79.1331\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 3/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_3=1.66, accuracy = 79.69%\n",
      "batch_loss 100_Epoch_3=1.65, accuracy = 80.47%\n",
      "batch_loss 200_Epoch_3=1.62, accuracy = 82.03%\n",
      "batch_loss 300_Epoch_3=1.70, accuracy = 73.44%\n",
      "EPOCH ACCURACY = 78.91 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6462 Acc: 78.9062\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_3=1.67, accuracy = 77.34%\n",
      "batch_loss 2_Epoch_3=1.60, accuracy = 85.16%\n",
      "batch_loss 3_Epoch_3=1.61, accuracy = 84.38%\n",
      "batch_loss 4_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 5_Epoch_3=1.63, accuracy = 82.81%\n",
      "batch_loss 6_Epoch_3=1.69, accuracy = 75.00%\n",
      "batch_loss 7_Epoch_3=1.65, accuracy = 78.91%\n",
      "batch_loss 8_Epoch_3=1.66, accuracy = 78.12%\n",
      "batch_loss 9_Epoch_3=1.66, accuracy = 78.91%\n",
      "batch_loss 10_Epoch_3=1.64, accuracy = 80.47%\n",
      "batch_loss 11_Epoch_3=1.67, accuracy = 77.34%\n",
      "batch_loss 12_Epoch_3=1.69, accuracy = 75.78%\n",
      "batch_loss 13_Epoch_3=1.66, accuracy = 79.69%\n",
      "batch_loss 14_Epoch_3=1.58, accuracy = 86.72%\n",
      "batch_loss 15_Epoch_3=1.63, accuracy = 82.81%\n",
      "batch_loss 16_Epoch_3=1.62, accuracy = 84.38%\n",
      "batch_loss 17_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 18_Epoch_3=1.66, accuracy = 79.69%\n",
      "batch_loss 19_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 20_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 21_Epoch_3=1.66, accuracy = 79.69%\n",
      "batch_loss 22_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 23_Epoch_3=1.61, accuracy = 83.59%\n",
      "batch_loss 24_Epoch_3=1.60, accuracy = 85.94%\n",
      "batch_loss 25_Epoch_3=1.61, accuracy = 85.16%\n",
      "batch_loss 26_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 27_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 28_Epoch_3=1.63, accuracy = 82.03%\n",
      "batch_loss 29_Epoch_3=1.62, accuracy = 83.59%\n",
      "batch_loss 30_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 31_Epoch_3=1.65, accuracy = 78.91%\n",
      "batch_loss 32_Epoch_3=1.60, accuracy = 85.16%\n",
      "batch_loss 33_Epoch_3=1.67, accuracy = 78.12%\n",
      "batch_loss 34_Epoch_3=1.64, accuracy = 82.03%\n",
      "batch_loss 35_Epoch_3=1.67, accuracy = 78.12%\n",
      "batch_loss 36_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 37_Epoch_3=1.66, accuracy = 78.91%\n",
      "batch_loss 38_Epoch_3=1.60, accuracy = 83.59%\n",
      "batch_loss 39_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 40_Epoch_3=1.63, accuracy = 82.03%\n",
      "batch_loss 41_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 42_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 43_Epoch_3=1.61, accuracy = 85.94%\n",
      "batch_loss 44_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 45_Epoch_3=1.67, accuracy = 77.34%\n",
      "batch_loss 46_Epoch_3=1.63, accuracy = 81.25%\n",
      "batch_loss 47_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 48_Epoch_3=1.63, accuracy = 82.03%\n",
      "batch_loss 49_Epoch_3=1.67, accuracy = 76.56%\n",
      "batch_loss 50_Epoch_3=1.59, accuracy = 84.38%\n",
      "batch_loss 51_Epoch_3=1.65, accuracy = 80.47%\n",
      "batch_loss 52_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 53_Epoch_3=1.62, accuracy = 81.25%\n",
      "batch_loss 54_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 55_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 56_Epoch_3=1.69, accuracy = 76.56%\n",
      "batch_loss 57_Epoch_3=1.63, accuracy = 81.25%\n",
      "batch_loss 58_Epoch_3=1.67, accuracy = 78.12%\n",
      "batch_loss 59_Epoch_3=1.72, accuracy = 72.66%\n",
      "batch_loss 60_Epoch_3=1.68, accuracy = 75.78%\n",
      "batch_loss 61_Epoch_3=1.70, accuracy = 75.00%\n",
      "batch_loss 62_Epoch_3=1.58, accuracy = 87.50%\n",
      "batch_loss 63_Epoch_3=1.60, accuracy = 85.16%\n",
      "batch_loss 64_Epoch_3=1.67, accuracy = 78.91%\n",
      "batch_loss 65_Epoch_3=1.63, accuracy = 82.81%\n",
      "batch_loss 66_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 67_Epoch_3=1.71, accuracy = 73.44%\n",
      "batch_loss 68_Epoch_3=1.70, accuracy = 75.00%\n",
      "batch_loss 69_Epoch_3=1.69, accuracy = 75.78%\n",
      "batch_loss 70_Epoch_3=1.63, accuracy = 81.25%\n",
      "batch_loss 71_Epoch_3=1.55, accuracy = 90.62%\n",
      "batch_loss 72_Epoch_3=1.62, accuracy = 83.59%\n",
      "batch_loss 73_Epoch_3=1.61, accuracy = 84.38%\n",
      "batch_loss 74_Epoch_3=1.68, accuracy = 75.78%\n",
      "batch_loss 75_Epoch_3=1.70, accuracy = 74.22%\n",
      "batch_loss 76_Epoch_3=1.72, accuracy = 73.44%\n",
      "batch_loss 77_Epoch_3=1.62, accuracy = 82.03%\n",
      "batch_loss 78_Epoch_3=1.70, accuracy = 74.22%\n",
      "batch_loss 79_Epoch_3=1.62, accuracy = 83.59%\n",
      "batch_loss 80_Epoch_3=1.67, accuracy = 76.56%\n",
      "batch_loss 81_Epoch_3=1.65, accuracy = 78.91%\n",
      "batch_loss 82_Epoch_3=1.62, accuracy = 82.81%\n",
      "batch_loss 83_Epoch_3=1.63, accuracy = 82.03%\n",
      "batch_loss 84_Epoch_3=1.68, accuracy = 78.12%\n",
      "batch_loss 85_Epoch_3=1.64, accuracy = 80.47%\n",
      "batch_loss 86_Epoch_3=1.60, accuracy = 85.94%\n",
      "batch_loss 87_Epoch_3=1.63, accuracy = 83.59%\n",
      "batch_loss 88_Epoch_3=1.67, accuracy = 77.34%\n",
      "batch_loss 89_Epoch_3=1.65, accuracy = 80.47%\n",
      "batch_loss 90_Epoch_3=1.64, accuracy = 81.25%\n",
      "batch_loss 91_Epoch_3=1.65, accuracy = 79.69%\n",
      "batch_loss 92_Epoch_3=1.63, accuracy = 81.25%\n",
      "batch_loss 93_Epoch_3=1.66, accuracy = 78.12%\n",
      "EPOCH ACCURACY = 80.56 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6561 Acc: 80.5612\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 4/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_4=1.69, accuracy = 75.00%\n",
      "batch_loss 100_Epoch_4=1.58, accuracy = 87.50%\n",
      "batch_loss 200_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 300_Epoch_4=1.62, accuracy = 82.81%\n",
      "EPOCH ACCURACY = 81.64 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.5823 Acc: 81.6406\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_4=1.62, accuracy = 82.81%\n",
      "batch_loss 2_Epoch_4=1.59, accuracy = 85.94%\n",
      "batch_loss 3_Epoch_4=1.67, accuracy = 77.34%\n",
      "batch_loss 4_Epoch_4=1.61, accuracy = 84.38%\n",
      "batch_loss 5_Epoch_4=1.57, accuracy = 88.28%\n",
      "batch_loss 6_Epoch_4=1.59, accuracy = 86.72%\n",
      "batch_loss 7_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 8_Epoch_4=1.63, accuracy = 82.81%\n",
      "batch_loss 9_Epoch_4=1.62, accuracy = 82.03%\n",
      "batch_loss 10_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 11_Epoch_4=1.61, accuracy = 84.38%\n",
      "batch_loss 12_Epoch_4=1.60, accuracy = 85.16%\n",
      "batch_loss 13_Epoch_4=1.65, accuracy = 81.25%\n",
      "batch_loss 14_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 15_Epoch_4=1.71, accuracy = 71.88%\n",
      "batch_loss 16_Epoch_4=1.62, accuracy = 82.81%\n",
      "batch_loss 17_Epoch_4=1.64, accuracy = 81.25%\n",
      "batch_loss 18_Epoch_4=1.72, accuracy = 72.66%\n",
      "batch_loss 19_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 20_Epoch_4=1.65, accuracy = 78.91%\n",
      "batch_loss 21_Epoch_4=1.66, accuracy = 78.12%\n",
      "batch_loss 22_Epoch_4=1.67, accuracy = 78.12%\n",
      "batch_loss 23_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 24_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 25_Epoch_4=1.70, accuracy = 74.22%\n",
      "batch_loss 26_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 27_Epoch_4=1.62, accuracy = 83.59%\n",
      "batch_loss 28_Epoch_4=1.59, accuracy = 86.72%\n",
      "batch_loss 29_Epoch_4=1.66, accuracy = 78.12%\n",
      "batch_loss 30_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 31_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 32_Epoch_4=1.57, accuracy = 88.28%\n",
      "batch_loss 33_Epoch_4=1.61, accuracy = 83.59%\n",
      "batch_loss 34_Epoch_4=1.62, accuracy = 82.81%\n",
      "batch_loss 35_Epoch_4=1.62, accuracy = 82.03%\n",
      "batch_loss 36_Epoch_4=1.66, accuracy = 78.12%\n",
      "batch_loss 37_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 38_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 39_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 40_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 41_Epoch_4=1.64, accuracy = 81.25%\n",
      "batch_loss 42_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 43_Epoch_4=1.62, accuracy = 81.25%\n",
      "batch_loss 44_Epoch_4=1.60, accuracy = 85.16%\n",
      "batch_loss 45_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 46_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 47_Epoch_4=1.67, accuracy = 77.34%\n",
      "batch_loss 48_Epoch_4=1.67, accuracy = 76.56%\n",
      "batch_loss 49_Epoch_4=1.60, accuracy = 85.16%\n",
      "batch_loss 50_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 51_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 52_Epoch_4=1.61, accuracy = 84.38%\n",
      "batch_loss 53_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 54_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 55_Epoch_4=1.61, accuracy = 84.38%\n",
      "batch_loss 56_Epoch_4=1.64, accuracy = 79.69%\n",
      "batch_loss 57_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 58_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 59_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 60_Epoch_4=1.62, accuracy = 81.25%\n",
      "batch_loss 61_Epoch_4=1.66, accuracy = 78.12%\n",
      "batch_loss 62_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 63_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 64_Epoch_4=1.56, accuracy = 89.84%\n",
      "batch_loss 65_Epoch_4=1.69, accuracy = 75.00%\n",
      "batch_loss 66_Epoch_4=1.60, accuracy = 84.38%\n",
      "batch_loss 67_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 68_Epoch_4=1.64, accuracy = 81.25%\n",
      "batch_loss 69_Epoch_4=1.66, accuracy = 78.91%\n",
      "batch_loss 70_Epoch_4=1.61, accuracy = 83.59%\n",
      "batch_loss 71_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 72_Epoch_4=1.64, accuracy = 81.25%\n",
      "batch_loss 73_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 74_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 75_Epoch_4=1.64, accuracy = 80.47%\n",
      "batch_loss 76_Epoch_4=1.62, accuracy = 83.59%\n",
      "batch_loss 77_Epoch_4=1.71, accuracy = 72.66%\n",
      "batch_loss 78_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 79_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 80_Epoch_4=1.68, accuracy = 75.78%\n",
      "batch_loss 81_Epoch_4=1.68, accuracy = 76.56%\n",
      "batch_loss 82_Epoch_4=1.67, accuracy = 77.34%\n",
      "batch_loss 83_Epoch_4=1.67, accuracy = 78.12%\n",
      "batch_loss 84_Epoch_4=1.67, accuracy = 78.12%\n",
      "batch_loss 85_Epoch_4=1.64, accuracy = 81.25%\n",
      "batch_loss 86_Epoch_4=1.67, accuracy = 78.12%\n",
      "batch_loss 87_Epoch_4=1.63, accuracy = 81.25%\n",
      "batch_loss 88_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 89_Epoch_4=1.67, accuracy = 78.12%\n",
      "batch_loss 90_Epoch_4=1.65, accuracy = 79.69%\n",
      "batch_loss 91_Epoch_4=1.63, accuracy = 82.03%\n",
      "batch_loss 92_Epoch_4=1.68, accuracy = 77.34%\n",
      "batch_loss 93_Epoch_4=1.65, accuracy = 81.25%\n",
      "EPOCH ACCURACY = 80.53 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6451 Acc: 80.5276\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 5/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_5=1.62, accuracy = 82.03%\n",
      "batch_loss 100_Epoch_5=1.61, accuracy = 83.59%\n",
      "batch_loss 200_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 300_Epoch_5=1.70, accuracy = 73.44%\n",
      "EPOCH ACCURACY = 80.08 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6005 Acc: 80.0781\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_5=1.66, accuracy = 77.34%\n",
      "batch_loss 2_Epoch_5=1.60, accuracy = 85.16%\n",
      "batch_loss 3_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 4_Epoch_5=1.69, accuracy = 74.22%\n",
      "batch_loss 5_Epoch_5=1.61, accuracy = 83.59%\n",
      "batch_loss 6_Epoch_5=1.68, accuracy = 75.00%\n",
      "batch_loss 7_Epoch_5=1.67, accuracy = 76.56%\n",
      "batch_loss 8_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 9_Epoch_5=1.70, accuracy = 74.22%\n",
      "batch_loss 10_Epoch_5=1.62, accuracy = 83.59%\n",
      "batch_loss 11_Epoch_5=1.56, accuracy = 89.84%\n",
      "batch_loss 12_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 13_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 14_Epoch_5=1.68, accuracy = 76.56%\n",
      "batch_loss 15_Epoch_5=1.61, accuracy = 84.38%\n",
      "batch_loss 16_Epoch_5=1.69, accuracy = 75.78%\n",
      "batch_loss 17_Epoch_5=1.65, accuracy = 80.47%\n",
      "batch_loss 18_Epoch_5=1.67, accuracy = 77.34%\n",
      "batch_loss 19_Epoch_5=1.65, accuracy = 78.12%\n",
      "batch_loss 20_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 21_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 22_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 23_Epoch_5=1.69, accuracy = 75.00%\n",
      "batch_loss 24_Epoch_5=1.65, accuracy = 79.69%\n",
      "batch_loss 25_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 26_Epoch_5=1.60, accuracy = 84.38%\n",
      "batch_loss 27_Epoch_5=1.67, accuracy = 77.34%\n",
      "batch_loss 28_Epoch_5=1.62, accuracy = 83.59%\n",
      "batch_loss 29_Epoch_5=1.60, accuracy = 85.94%\n",
      "batch_loss 30_Epoch_5=1.60, accuracy = 84.38%\n",
      "batch_loss 31_Epoch_5=1.63, accuracy = 81.25%\n",
      "batch_loss 32_Epoch_5=1.61, accuracy = 83.59%\n",
      "batch_loss 33_Epoch_5=1.59, accuracy = 85.94%\n",
      "batch_loss 34_Epoch_5=1.57, accuracy = 88.28%\n",
      "batch_loss 35_Epoch_5=1.59, accuracy = 85.94%\n",
      "batch_loss 36_Epoch_5=1.66, accuracy = 79.69%\n",
      "batch_loss 37_Epoch_5=1.58, accuracy = 87.50%\n",
      "batch_loss 38_Epoch_5=1.69, accuracy = 75.00%\n",
      "batch_loss 39_Epoch_5=1.60, accuracy = 85.16%\n",
      "batch_loss 40_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 41_Epoch_5=1.62, accuracy = 83.59%\n",
      "batch_loss 42_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 43_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 44_Epoch_5=1.60, accuracy = 85.16%\n",
      "batch_loss 45_Epoch_5=1.62, accuracy = 82.81%\n",
      "batch_loss 46_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 47_Epoch_5=1.64, accuracy = 82.03%\n",
      "batch_loss 48_Epoch_5=1.63, accuracy = 82.81%\n",
      "batch_loss 49_Epoch_5=1.66, accuracy = 78.91%\n",
      "batch_loss 50_Epoch_5=1.65, accuracy = 78.91%\n",
      "batch_loss 51_Epoch_5=1.66, accuracy = 78.12%\n",
      "batch_loss 52_Epoch_5=1.66, accuracy = 78.91%\n",
      "batch_loss 53_Epoch_5=1.66, accuracy = 78.12%\n",
      "batch_loss 54_Epoch_5=1.56, accuracy = 89.06%\n",
      "batch_loss 55_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 56_Epoch_5=1.61, accuracy = 84.38%\n",
      "batch_loss 57_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 58_Epoch_5=1.60, accuracy = 85.16%\n",
      "batch_loss 59_Epoch_5=1.66, accuracy = 78.12%\n",
      "batch_loss 60_Epoch_5=1.66, accuracy = 78.12%\n",
      "batch_loss 61_Epoch_5=1.62, accuracy = 82.81%\n",
      "batch_loss 62_Epoch_5=1.64, accuracy = 80.47%\n",
      "batch_loss 63_Epoch_5=1.68, accuracy = 75.78%\n",
      "batch_loss 64_Epoch_5=1.62, accuracy = 82.81%\n",
      "batch_loss 65_Epoch_5=1.69, accuracy = 75.00%\n",
      "batch_loss 66_Epoch_5=1.66, accuracy = 78.91%\n",
      "batch_loss 67_Epoch_5=1.60, accuracy = 85.16%\n",
      "batch_loss 68_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 69_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 70_Epoch_5=1.63, accuracy = 81.25%\n",
      "batch_loss 71_Epoch_5=1.59, accuracy = 85.16%\n",
      "batch_loss 72_Epoch_5=1.63, accuracy = 81.25%\n",
      "batch_loss 73_Epoch_5=1.60, accuracy = 84.38%\n",
      "batch_loss 74_Epoch_5=1.61, accuracy = 82.81%\n",
      "batch_loss 75_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 76_Epoch_5=1.65, accuracy = 78.91%\n",
      "batch_loss 77_Epoch_5=1.64, accuracy = 79.69%\n",
      "batch_loss 78_Epoch_5=1.66, accuracy = 78.91%\n",
      "batch_loss 79_Epoch_5=1.66, accuracy = 78.91%\n",
      "batch_loss 80_Epoch_5=1.64, accuracy = 81.25%\n",
      "batch_loss 81_Epoch_5=1.66, accuracy = 78.12%\n",
      "batch_loss 82_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 83_Epoch_5=1.59, accuracy = 85.94%\n",
      "batch_loss 84_Epoch_5=1.62, accuracy = 82.81%\n",
      "batch_loss 85_Epoch_5=1.56, accuracy = 89.84%\n",
      "batch_loss 86_Epoch_5=1.61, accuracy = 83.59%\n",
      "batch_loss 87_Epoch_5=1.62, accuracy = 82.81%\n",
      "batch_loss 88_Epoch_5=1.63, accuracy = 82.03%\n",
      "batch_loss 89_Epoch_5=1.68, accuracy = 77.34%\n",
      "batch_loss 90_Epoch_5=1.70, accuracy = 74.22%\n",
      "batch_loss 91_Epoch_5=1.57, accuracy = 88.28%\n",
      "batch_loss 92_Epoch_5=1.69, accuracy = 75.00%\n",
      "batch_loss 93_Epoch_5=1.68, accuracy = 75.78%\n",
      "EPOCH ACCURACY = 81.21 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6795 Acc: 81.2080\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 6/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_6=1.58, accuracy = 85.94%\n",
      "batch_loss 100_Epoch_6=1.64, accuracy = 81.25%\n",
      "batch_loss 200_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 300_Epoch_6=1.62, accuracy = 82.03%\n",
      "EPOCH ACCURACY = 82.62 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6648 Acc: 82.6172\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_6=1.59, accuracy = 87.50%\n",
      "batch_loss 2_Epoch_6=1.68, accuracy = 75.78%\n",
      "batch_loss 3_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 4_Epoch_6=1.63, accuracy = 82.03%\n",
      "batch_loss 5_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 6_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 7_Epoch_6=1.60, accuracy = 85.16%\n",
      "batch_loss 8_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 9_Epoch_6=1.65, accuracy = 80.47%\n",
      "batch_loss 10_Epoch_6=1.60, accuracy = 84.38%\n",
      "batch_loss 11_Epoch_6=1.70, accuracy = 74.22%\n",
      "batch_loss 12_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 13_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 14_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 15_Epoch_6=1.62, accuracy = 82.81%\n",
      "batch_loss 16_Epoch_6=1.59, accuracy = 85.16%\n",
      "batch_loss 17_Epoch_6=1.58, accuracy = 85.94%\n",
      "batch_loss 18_Epoch_6=1.62, accuracy = 82.81%\n",
      "batch_loss 19_Epoch_6=1.67, accuracy = 77.34%\n",
      "batch_loss 20_Epoch_6=1.65, accuracy = 81.25%\n",
      "batch_loss 21_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 22_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 23_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 24_Epoch_6=1.60, accuracy = 84.38%\n",
      "batch_loss 25_Epoch_6=1.62, accuracy = 82.03%\n",
      "batch_loss 26_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 27_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 28_Epoch_6=1.57, accuracy = 88.28%\n",
      "batch_loss 29_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 30_Epoch_6=1.62, accuracy = 82.81%\n",
      "batch_loss 31_Epoch_6=1.65, accuracy = 78.91%\n",
      "batch_loss 32_Epoch_6=1.65, accuracy = 78.91%\n",
      "batch_loss 33_Epoch_6=1.64, accuracy = 79.69%\n",
      "batch_loss 34_Epoch_6=1.64, accuracy = 81.25%\n",
      "batch_loss 35_Epoch_6=1.66, accuracy = 78.91%\n",
      "batch_loss 36_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 37_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 38_Epoch_6=1.66, accuracy = 78.91%\n",
      "batch_loss 39_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 40_Epoch_6=1.66, accuracy = 78.91%\n",
      "batch_loss 41_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 42_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 43_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 44_Epoch_6=1.65, accuracy = 80.47%\n",
      "batch_loss 45_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 46_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 47_Epoch_6=1.61, accuracy = 85.16%\n",
      "batch_loss 48_Epoch_6=1.64, accuracy = 81.25%\n",
      "batch_loss 49_Epoch_6=1.57, accuracy = 88.28%\n",
      "batch_loss 50_Epoch_6=1.59, accuracy = 85.16%\n",
      "batch_loss 51_Epoch_6=1.61, accuracy = 84.38%\n",
      "batch_loss 52_Epoch_6=1.63, accuracy = 82.03%\n",
      "batch_loss 53_Epoch_6=1.62, accuracy = 82.03%\n",
      "batch_loss 54_Epoch_6=1.60, accuracy = 85.16%\n",
      "batch_loss 55_Epoch_6=1.61, accuracy = 84.38%\n",
      "batch_loss 56_Epoch_6=1.68, accuracy = 75.00%\n",
      "batch_loss 57_Epoch_6=1.65, accuracy = 80.47%\n",
      "batch_loss 58_Epoch_6=1.65, accuracy = 81.25%\n",
      "batch_loss 59_Epoch_6=1.61, accuracy = 84.38%\n",
      "batch_loss 60_Epoch_6=1.61, accuracy = 85.16%\n",
      "batch_loss 61_Epoch_6=1.69, accuracy = 76.56%\n",
      "batch_loss 62_Epoch_6=1.65, accuracy = 80.47%\n",
      "batch_loss 63_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 64_Epoch_6=1.58, accuracy = 87.50%\n",
      "batch_loss 65_Epoch_6=1.60, accuracy = 84.38%\n",
      "batch_loss 66_Epoch_6=1.62, accuracy = 82.81%\n",
      "batch_loss 67_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 68_Epoch_6=1.60, accuracy = 84.38%\n",
      "batch_loss 69_Epoch_6=1.65, accuracy = 78.91%\n",
      "batch_loss 70_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 71_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 72_Epoch_6=1.60, accuracy = 84.38%\n",
      "batch_loss 73_Epoch_6=1.64, accuracy = 81.25%\n",
      "batch_loss 74_Epoch_6=1.65, accuracy = 81.25%\n",
      "batch_loss 75_Epoch_6=1.63, accuracy = 82.03%\n",
      "batch_loss 76_Epoch_6=1.63, accuracy = 82.03%\n",
      "batch_loss 77_Epoch_6=1.65, accuracy = 78.91%\n",
      "batch_loss 78_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 79_Epoch_6=1.68, accuracy = 75.78%\n",
      "batch_loss 80_Epoch_6=1.68, accuracy = 76.56%\n",
      "batch_loss 81_Epoch_6=1.62, accuracy = 82.81%\n",
      "batch_loss 82_Epoch_6=1.61, accuracy = 84.38%\n",
      "batch_loss 83_Epoch_6=1.66, accuracy = 78.12%\n",
      "batch_loss 84_Epoch_6=1.61, accuracy = 83.59%\n",
      "batch_loss 85_Epoch_6=1.65, accuracy = 79.69%\n",
      "batch_loss 86_Epoch_6=1.63, accuracy = 82.03%\n",
      "batch_loss 87_Epoch_6=1.64, accuracy = 80.47%\n",
      "batch_loss 88_Epoch_6=1.66, accuracy = 79.69%\n",
      "batch_loss 89_Epoch_6=1.58, accuracy = 86.72%\n",
      "batch_loss 90_Epoch_6=1.63, accuracy = 81.25%\n",
      "batch_loss 91_Epoch_6=1.58, accuracy = 86.72%\n",
      "batch_loss 92_Epoch_6=1.64, accuracy = 79.69%\n",
      "batch_loss 93_Epoch_6=1.64, accuracy = 82.03%\n",
      "EPOCH ACCURACY = 81.31 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6351 Acc: 81.3088\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 7/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_7=1.58, accuracy = 87.50%\n",
      "batch_loss 100_Epoch_7=1.59, accuracy = 85.94%\n",
      "batch_loss 200_Epoch_7=1.58, accuracy = 86.72%\n",
      "batch_loss 300_Epoch_7=1.58, accuracy = 85.94%\n",
      "EPOCH ACCURACY = 86.52 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6048 Acc: 86.5234\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_7=1.66, accuracy = 78.12%\n",
      "batch_loss 2_Epoch_7=1.62, accuracy = 83.59%\n",
      "batch_loss 3_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 4_Epoch_7=1.64, accuracy = 79.69%\n",
      "batch_loss 5_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 6_Epoch_7=1.56, accuracy = 89.06%\n",
      "batch_loss 7_Epoch_7=1.61, accuracy = 84.38%\n",
      "batch_loss 8_Epoch_7=1.60, accuracy = 85.16%\n",
      "batch_loss 9_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 10_Epoch_7=1.57, accuracy = 87.50%\n",
      "batch_loss 11_Epoch_7=1.61, accuracy = 83.59%\n",
      "batch_loss 12_Epoch_7=1.58, accuracy = 87.50%\n",
      "batch_loss 13_Epoch_7=1.60, accuracy = 85.16%\n",
      "batch_loss 14_Epoch_7=1.65, accuracy = 80.47%\n",
      "batch_loss 15_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 16_Epoch_7=1.68, accuracy = 75.78%\n",
      "batch_loss 17_Epoch_7=1.58, accuracy = 86.72%\n",
      "batch_loss 18_Epoch_7=1.63, accuracy = 83.59%\n",
      "batch_loss 19_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 20_Epoch_7=1.62, accuracy = 82.81%\n",
      "batch_loss 21_Epoch_7=1.66, accuracy = 79.69%\n",
      "batch_loss 22_Epoch_7=1.64, accuracy = 80.47%\n",
      "batch_loss 23_Epoch_7=1.69, accuracy = 75.00%\n",
      "batch_loss 24_Epoch_7=1.67, accuracy = 77.34%\n",
      "batch_loss 25_Epoch_7=1.65, accuracy = 79.69%\n",
      "batch_loss 26_Epoch_7=1.66, accuracy = 78.91%\n",
      "batch_loss 27_Epoch_7=1.65, accuracy = 81.25%\n",
      "batch_loss 28_Epoch_7=1.63, accuracy = 82.03%\n",
      "batch_loss 29_Epoch_7=1.68, accuracy = 76.56%\n",
      "batch_loss 30_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 31_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 32_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 33_Epoch_7=1.62, accuracy = 82.81%\n",
      "batch_loss 34_Epoch_7=1.60, accuracy = 85.94%\n",
      "batch_loss 35_Epoch_7=1.60, accuracy = 85.16%\n",
      "batch_loss 36_Epoch_7=1.66, accuracy = 78.91%\n",
      "batch_loss 37_Epoch_7=1.68, accuracy = 76.56%\n",
      "batch_loss 38_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 39_Epoch_7=1.63, accuracy = 82.03%\n",
      "batch_loss 40_Epoch_7=1.60, accuracy = 85.94%\n",
      "batch_loss 41_Epoch_7=1.66, accuracy = 78.12%\n",
      "batch_loss 42_Epoch_7=1.62, accuracy = 82.81%\n",
      "batch_loss 43_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 44_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 45_Epoch_7=1.62, accuracy = 83.59%\n",
      "batch_loss 46_Epoch_7=1.63, accuracy = 82.03%\n",
      "batch_loss 47_Epoch_7=1.64, accuracy = 80.47%\n",
      "batch_loss 48_Epoch_7=1.68, accuracy = 76.56%\n",
      "batch_loss 49_Epoch_7=1.68, accuracy = 75.00%\n",
      "batch_loss 50_Epoch_7=1.69, accuracy = 75.00%\n",
      "batch_loss 51_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 52_Epoch_7=1.65, accuracy = 78.91%\n",
      "batch_loss 53_Epoch_7=1.67, accuracy = 78.12%\n",
      "batch_loss 54_Epoch_7=1.61, accuracy = 83.59%\n",
      "batch_loss 55_Epoch_7=1.63, accuracy = 82.81%\n",
      "batch_loss 56_Epoch_7=1.61, accuracy = 84.38%\n",
      "batch_loss 57_Epoch_7=1.69, accuracy = 75.00%\n",
      "batch_loss 58_Epoch_7=1.61, accuracy = 83.59%\n",
      "batch_loss 59_Epoch_7=1.62, accuracy = 83.59%\n",
      "batch_loss 60_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 61_Epoch_7=1.65, accuracy = 78.91%\n",
      "batch_loss 62_Epoch_7=1.67, accuracy = 77.34%\n",
      "batch_loss 63_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 64_Epoch_7=1.68, accuracy = 76.56%\n",
      "batch_loss 65_Epoch_7=1.61, accuracy = 84.38%\n",
      "batch_loss 66_Epoch_7=1.65, accuracy = 80.47%\n",
      "batch_loss 67_Epoch_7=1.65, accuracy = 79.69%\n",
      "batch_loss 68_Epoch_7=1.61, accuracy = 84.38%\n",
      "batch_loss 69_Epoch_7=1.61, accuracy = 85.16%\n",
      "batch_loss 70_Epoch_7=1.64, accuracy = 80.47%\n",
      "batch_loss 71_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 72_Epoch_7=1.61, accuracy = 83.59%\n",
      "batch_loss 73_Epoch_7=1.60, accuracy = 85.16%\n",
      "batch_loss 74_Epoch_7=1.59, accuracy = 86.72%\n",
      "batch_loss 75_Epoch_7=1.72, accuracy = 71.88%\n",
      "batch_loss 76_Epoch_7=1.64, accuracy = 80.47%\n",
      "batch_loss 77_Epoch_7=1.68, accuracy = 76.56%\n",
      "batch_loss 78_Epoch_7=1.65, accuracy = 79.69%\n",
      "batch_loss 79_Epoch_7=1.62, accuracy = 82.81%\n",
      "batch_loss 80_Epoch_7=1.62, accuracy = 82.03%\n",
      "batch_loss 81_Epoch_7=1.65, accuracy = 79.69%\n",
      "batch_loss 82_Epoch_7=1.64, accuracy = 80.47%\n",
      "batch_loss 83_Epoch_7=1.62, accuracy = 84.38%\n",
      "batch_loss 84_Epoch_7=1.67, accuracy = 78.12%\n",
      "batch_loss 85_Epoch_7=1.64, accuracy = 81.25%\n",
      "batch_loss 86_Epoch_7=1.62, accuracy = 83.59%\n",
      "batch_loss 87_Epoch_7=1.60, accuracy = 85.16%\n",
      "batch_loss 88_Epoch_7=1.64, accuracy = 82.03%\n",
      "batch_loss 89_Epoch_7=1.63, accuracy = 81.25%\n",
      "batch_loss 90_Epoch_7=1.65, accuracy = 78.91%\n",
      "batch_loss 91_Epoch_7=1.62, accuracy = 82.81%\n",
      "batch_loss 92_Epoch_7=1.61, accuracy = 82.81%\n",
      "batch_loss 93_Epoch_7=1.62, accuracy = 82.81%\n",
      "EPOCH ACCURACY = 81.37 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6201 Acc: 81.3676\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 8/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_8=1.56, accuracy = 89.84%\n",
      "batch_loss 100_Epoch_8=1.58, accuracy = 87.50%\n",
      "batch_loss 200_Epoch_8=1.59, accuracy = 85.94%\n",
      "batch_loss 300_Epoch_8=1.58, accuracy = 87.50%\n",
      "EPOCH ACCURACY = 87.70 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6274 Acc: 87.6953\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_8=1.64, accuracy = 81.25%\n",
      "batch_loss 2_Epoch_8=1.67, accuracy = 76.56%\n",
      "batch_loss 3_Epoch_8=1.57, accuracy = 88.28%\n",
      "batch_loss 4_Epoch_8=1.62, accuracy = 82.81%\n",
      "batch_loss 5_Epoch_8=1.64, accuracy = 79.69%\n",
      "batch_loss 6_Epoch_8=1.66, accuracy = 78.12%\n",
      "batch_loss 7_Epoch_8=1.60, accuracy = 85.94%\n",
      "batch_loss 8_Epoch_8=1.63, accuracy = 81.25%\n",
      "batch_loss 9_Epoch_8=1.60, accuracy = 85.16%\n",
      "batch_loss 10_Epoch_8=1.62, accuracy = 82.81%\n",
      "batch_loss 11_Epoch_8=1.62, accuracy = 83.59%\n",
      "batch_loss 12_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 13_Epoch_8=1.62, accuracy = 82.81%\n",
      "batch_loss 14_Epoch_8=1.63, accuracy = 82.03%\n",
      "batch_loss 15_Epoch_8=1.64, accuracy = 80.47%\n",
      "batch_loss 16_Epoch_8=1.64, accuracy = 81.25%\n",
      "batch_loss 17_Epoch_8=1.63, accuracy = 81.25%\n",
      "batch_loss 18_Epoch_8=1.63, accuracy = 81.25%\n",
      "batch_loss 19_Epoch_8=1.58, accuracy = 88.28%\n",
      "batch_loss 20_Epoch_8=1.69, accuracy = 75.78%\n",
      "batch_loss 21_Epoch_8=1.62, accuracy = 84.38%\n",
      "batch_loss 22_Epoch_8=1.60, accuracy = 83.59%\n",
      "batch_loss 23_Epoch_8=1.69, accuracy = 74.22%\n",
      "batch_loss 24_Epoch_8=1.64, accuracy = 80.47%\n",
      "batch_loss 25_Epoch_8=1.62, accuracy = 82.03%\n",
      "batch_loss 26_Epoch_8=1.66, accuracy = 77.34%\n",
      "batch_loss 27_Epoch_8=1.65, accuracy = 79.69%\n",
      "batch_loss 28_Epoch_8=1.61, accuracy = 82.81%\n",
      "batch_loss 29_Epoch_8=1.68, accuracy = 76.56%\n",
      "batch_loss 30_Epoch_8=1.65, accuracy = 79.69%\n",
      "batch_loss 31_Epoch_8=1.60, accuracy = 85.94%\n",
      "batch_loss 32_Epoch_8=1.62, accuracy = 82.81%\n",
      "batch_loss 33_Epoch_8=1.67, accuracy = 78.12%\n",
      "batch_loss 34_Epoch_8=1.59, accuracy = 85.94%\n",
      "batch_loss 35_Epoch_8=1.64, accuracy = 80.47%\n",
      "batch_loss 36_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 37_Epoch_8=1.59, accuracy = 86.72%\n",
      "batch_loss 38_Epoch_8=1.68, accuracy = 76.56%\n",
      "batch_loss 39_Epoch_8=1.61, accuracy = 83.59%\n",
      "batch_loss 40_Epoch_8=1.61, accuracy = 82.81%\n",
      "batch_loss 41_Epoch_8=1.59, accuracy = 86.72%\n",
      "batch_loss 42_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 43_Epoch_8=1.62, accuracy = 83.59%\n",
      "batch_loss 44_Epoch_8=1.63, accuracy = 81.25%\n",
      "batch_loss 45_Epoch_8=1.63, accuracy = 82.81%\n",
      "batch_loss 46_Epoch_8=1.66, accuracy = 78.91%\n",
      "batch_loss 47_Epoch_8=1.63, accuracy = 82.03%\n",
      "batch_loss 48_Epoch_8=1.67, accuracy = 78.12%\n",
      "batch_loss 49_Epoch_8=1.69, accuracy = 75.00%\n",
      "batch_loss 50_Epoch_8=1.64, accuracy = 82.03%\n",
      "batch_loss 51_Epoch_8=1.62, accuracy = 82.81%\n",
      "batch_loss 52_Epoch_8=1.59, accuracy = 85.16%\n",
      "batch_loss 53_Epoch_8=1.60, accuracy = 84.38%\n",
      "batch_loss 54_Epoch_8=1.60, accuracy = 85.16%\n",
      "batch_loss 55_Epoch_8=1.65, accuracy = 80.47%\n",
      "batch_loss 56_Epoch_8=1.65, accuracy = 79.69%\n",
      "batch_loss 57_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 58_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 59_Epoch_8=1.62, accuracy = 82.03%\n",
      "batch_loss 60_Epoch_8=1.58, accuracy = 88.28%\n",
      "batch_loss 61_Epoch_8=1.70, accuracy = 73.44%\n",
      "batch_loss 62_Epoch_8=1.64, accuracy = 79.69%\n",
      "batch_loss 63_Epoch_8=1.66, accuracy = 78.91%\n",
      "batch_loss 64_Epoch_8=1.67, accuracy = 76.56%\n",
      "batch_loss 65_Epoch_8=1.61, accuracy = 84.38%\n",
      "batch_loss 66_Epoch_8=1.59, accuracy = 86.72%\n",
      "batch_loss 67_Epoch_8=1.59, accuracy = 85.94%\n",
      "batch_loss 68_Epoch_8=1.63, accuracy = 82.03%\n",
      "batch_loss 69_Epoch_8=1.63, accuracy = 80.47%\n",
      "batch_loss 70_Epoch_8=1.57, accuracy = 89.06%\n",
      "batch_loss 71_Epoch_8=1.60, accuracy = 85.16%\n",
      "batch_loss 72_Epoch_8=1.60, accuracy = 84.38%\n",
      "batch_loss 73_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 74_Epoch_8=1.59, accuracy = 85.16%\n",
      "batch_loss 75_Epoch_8=1.60, accuracy = 84.38%\n",
      "batch_loss 76_Epoch_8=1.64, accuracy = 81.25%\n",
      "batch_loss 77_Epoch_8=1.64, accuracy = 80.47%\n",
      "batch_loss 78_Epoch_8=1.60, accuracy = 84.38%\n",
      "batch_loss 79_Epoch_8=1.65, accuracy = 79.69%\n",
      "batch_loss 80_Epoch_8=1.67, accuracy = 77.34%\n",
      "batch_loss 81_Epoch_8=1.65, accuracy = 79.69%\n",
      "batch_loss 82_Epoch_8=1.66, accuracy = 78.12%\n",
      "batch_loss 83_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 84_Epoch_8=1.62, accuracy = 84.38%\n",
      "batch_loss 85_Epoch_8=1.64, accuracy = 80.47%\n",
      "batch_loss 86_Epoch_8=1.67, accuracy = 78.12%\n",
      "batch_loss 87_Epoch_8=1.61, accuracy = 85.16%\n",
      "batch_loss 88_Epoch_8=1.61, accuracy = 83.59%\n",
      "batch_loss 89_Epoch_8=1.67, accuracy = 78.91%\n",
      "batch_loss 90_Epoch_8=1.61, accuracy = 83.59%\n",
      "batch_loss 91_Epoch_8=1.58, accuracy = 87.50%\n",
      "batch_loss 92_Epoch_8=1.65, accuracy = 78.91%\n",
      "batch_loss 93_Epoch_8=1.67, accuracy = 77.34%\n",
      "EPOCH ACCURACY = 81.59 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.6686 Acc: 81.5944\n",
      "****************************************************************************************************\n",
      "\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Epoch 9/10\n",
      "----------------------------------------------------------------------------------------------------\n",
      "batch_loss 1_Epoch_9=1.62, accuracy = 83.59%\n",
      "batch_loss 100_Epoch_9=1.57, accuracy = 87.50%\n",
      "batch_loss 200_Epoch_9=1.66, accuracy = 78.12%\n",
      "batch_loss 300_Epoch_9=1.56, accuracy = 89.06%\n",
      "EPOCH ACCURACY = 84.57 %\n",
      "****************************************************************************************************\n",
      "train Loss: 1.6407 Acc: 84.5703\n",
      "****************************************************************************************************\n",
      "batch_loss 1_Epoch_9=1.62, accuracy = 83.59%\n",
      "batch_loss 2_Epoch_9=1.62, accuracy = 82.03%\n",
      "batch_loss 3_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 4_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 5_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 6_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 7_Epoch_9=1.68, accuracy = 77.34%\n",
      "batch_loss 8_Epoch_9=1.64, accuracy = 81.25%\n",
      "batch_loss 9_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 10_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 11_Epoch_9=1.65, accuracy = 78.91%\n",
      "batch_loss 12_Epoch_9=1.63, accuracy = 81.25%\n",
      "batch_loss 13_Epoch_9=1.62, accuracy = 82.81%\n",
      "batch_loss 14_Epoch_9=1.63, accuracy = 82.03%\n",
      "batch_loss 15_Epoch_9=1.59, accuracy = 85.94%\n",
      "batch_loss 16_Epoch_9=1.65, accuracy = 78.91%\n",
      "batch_loss 17_Epoch_9=1.62, accuracy = 82.03%\n",
      "batch_loss 18_Epoch_9=1.63, accuracy = 81.25%\n",
      "batch_loss 19_Epoch_9=1.66, accuracy = 77.34%\n",
      "batch_loss 20_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 21_Epoch_9=1.69, accuracy = 75.78%\n",
      "batch_loss 22_Epoch_9=1.64, accuracy = 79.69%\n",
      "batch_loss 23_Epoch_9=1.62, accuracy = 82.81%\n",
      "batch_loss 24_Epoch_9=1.58, accuracy = 88.28%\n",
      "batch_loss 25_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 26_Epoch_9=1.61, accuracy = 84.38%\n",
      "batch_loss 27_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 28_Epoch_9=1.60, accuracy = 85.94%\n",
      "batch_loss 29_Epoch_9=1.68, accuracy = 75.78%\n",
      "batch_loss 30_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 31_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 32_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 33_Epoch_9=1.63, accuracy = 82.03%\n",
      "batch_loss 34_Epoch_9=1.62, accuracy = 83.59%\n",
      "batch_loss 35_Epoch_9=1.68, accuracy = 76.56%\n",
      "batch_loss 36_Epoch_9=1.65, accuracy = 78.12%\n",
      "batch_loss 37_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 38_Epoch_9=1.64, accuracy = 79.69%\n",
      "batch_loss 39_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 40_Epoch_9=1.71, accuracy = 73.44%\n",
      "batch_loss 41_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 42_Epoch_9=1.63, accuracy = 81.25%\n",
      "batch_loss 43_Epoch_9=1.63, accuracy = 82.03%\n",
      "batch_loss 44_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 45_Epoch_9=1.59, accuracy = 85.94%\n",
      "batch_loss 46_Epoch_9=1.58, accuracy = 86.72%\n",
      "batch_loss 47_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 48_Epoch_9=1.59, accuracy = 86.72%\n",
      "batch_loss 49_Epoch_9=1.64, accuracy = 79.69%\n",
      "batch_loss 50_Epoch_9=1.67, accuracy = 76.56%\n",
      "batch_loss 51_Epoch_9=1.60, accuracy = 85.94%\n",
      "batch_loss 52_Epoch_9=1.66, accuracy = 78.12%\n",
      "batch_loss 53_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 54_Epoch_9=1.62, accuracy = 82.03%\n",
      "batch_loss 55_Epoch_9=1.62, accuracy = 82.81%\n",
      "batch_loss 56_Epoch_9=1.65, accuracy = 80.47%\n",
      "batch_loss 57_Epoch_9=1.65, accuracy = 79.69%\n",
      "batch_loss 58_Epoch_9=1.61, accuracy = 85.16%\n",
      "batch_loss 59_Epoch_9=1.58, accuracy = 88.28%\n",
      "batch_loss 60_Epoch_9=1.68, accuracy = 75.78%\n",
      "batch_loss 61_Epoch_9=1.63, accuracy = 80.47%\n",
      "batch_loss 62_Epoch_9=1.71, accuracy = 72.66%\n",
      "batch_loss 63_Epoch_9=1.69, accuracy = 75.78%\n",
      "batch_loss 64_Epoch_9=1.65, accuracy = 79.69%\n",
      "batch_loss 65_Epoch_9=1.61, accuracy = 82.81%\n",
      "batch_loss 66_Epoch_9=1.66, accuracy = 78.12%\n",
      "batch_loss 67_Epoch_9=1.66, accuracy = 78.91%\n",
      "batch_loss 68_Epoch_9=1.59, accuracy = 85.94%\n",
      "batch_loss 69_Epoch_9=1.62, accuracy = 82.81%\n",
      "batch_loss 70_Epoch_9=1.68, accuracy = 76.56%\n",
      "batch_loss 71_Epoch_9=1.62, accuracy = 82.03%\n",
      "batch_loss 72_Epoch_9=1.59, accuracy = 85.16%\n",
      "batch_loss 73_Epoch_9=1.58, accuracy = 88.28%\n",
      "batch_loss 74_Epoch_9=1.66, accuracy = 78.91%\n",
      "batch_loss 75_Epoch_9=1.61, accuracy = 82.81%\n",
      "batch_loss 76_Epoch_9=1.67, accuracy = 77.34%\n",
      "batch_loss 77_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 78_Epoch_9=1.64, accuracy = 81.25%\n",
      "batch_loss 79_Epoch_9=1.66, accuracy = 78.91%\n",
      "batch_loss 80_Epoch_9=1.59, accuracy = 85.94%\n",
      "batch_loss 81_Epoch_9=1.65, accuracy = 79.69%\n",
      "batch_loss 82_Epoch_9=1.71, accuracy = 73.44%\n",
      "batch_loss 83_Epoch_9=1.66, accuracy = 78.91%\n",
      "batch_loss 84_Epoch_9=1.60, accuracy = 85.16%\n",
      "batch_loss 85_Epoch_9=1.64, accuracy = 80.47%\n",
      "batch_loss 86_Epoch_9=1.62, accuracy = 82.03%\n",
      "batch_loss 87_Epoch_9=1.65, accuracy = 80.47%\n",
      "batch_loss 88_Epoch_9=1.63, accuracy = 82.03%\n",
      "batch_loss 89_Epoch_9=1.63, accuracy = 82.03%\n",
      "batch_loss 90_Epoch_9=1.62, accuracy = 81.25%\n",
      "batch_loss 91_Epoch_9=1.71, accuracy = 73.44%\n",
      "batch_loss 92_Epoch_9=1.62, accuracy = 83.59%\n",
      "batch_loss 93_Epoch_9=1.59, accuracy = 85.94%\n",
      "EPOCH ACCURACY = 81.20 %\n",
      "****************************************************************************************************\n",
      "val Loss: 1.5871 Acc: 81.1996\n",
      "****************************************************************************************************\n",
      "\n",
      "Training complete in 2m 14s\n",
      "Best val Acc: 81.594421\n"
     ]
    }
   ],
   "source": [
    "model =AbstractNN()\n",
    "num_epochs =10\n",
    "learning_rate=0.01\n",
    "batch_size = 128\n",
    "device = torch.device(\"cpu\")\n",
    "optimizer = optim.Adam(model.parameters(), betas=(0.9, 0.99))\n",
    "scheduler =  optim.lr_scheduler.StepLR(optimizer, step_size=40, gamma=0.98)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "TD= T(model=model,\n",
    "            device=device\n",
    "                            )\n",
    "model = TD.train_model(train_dataset,val_dataset,\n",
    "                        criterion=criterion,  \n",
    "                        batch_size= batch_size,\n",
    "                        optimizer=optimizer,\n",
    "                        scheduler=scheduler,\n",
    "                        num_epochs=num_epochs,\n",
    "                        learning_rate=learning_rate,\n",
    "                        resname='SimpleCNN')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = AbstractNN()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "AbstractNN(\n",
       "  (conv1): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (conv2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "  (fc1): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=18432, out_features=64, bias=True)\n",
       "  )\n",
       "  (fc2): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SimpleModel(\n",
       "  (conv_block1): Sequential(\n",
       "    (0): Conv2d(1, 16, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (1): ReLU()\n",
       "    (2): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1))\n",
       "    (3): ReLU()\n",
       "  )\n",
       "  (classifier): Sequential(\n",
       "    (0): Flatten(start_dim=1, end_dim=-1)\n",
       "    (1): Linear(in_features=18432, out_features=64, bias=True)\n",
       "    (2): ReLU()\n",
       "    (3): Dropout(p=0.1, inplace=False)\n",
       "    (4): Linear(in_features=64, out_features=10, bias=True)\n",
       "  )\n",
       "  (softMax): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7945708c7250>"
      ]
     },
     "execution_count": 157,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "img, label = dataset_train[1]\n",
    "plt.imshow(img[0], cmap='gray')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([28, 28])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([786, 28, 28])"
      ]
     },
     "execution_count": 158,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "max= torch.max(img)\n",
    "min = torch.min(img)\n",
    "scale = max-min\n",
    "from abstract import abstractTensor as AT\n",
    "\n",
    "x=AT(img.squeeze(0),alpha =0.001*scale*torch.ones(28*28))\n",
    "x=x.abstract_tensor()\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lenx:786\n",
      "x.shape=torch.Size([786, 1, 28, 28])\n",
      "lenx:786\n",
      "x.shape=torch.Size([786, 16, 26, 26])\n",
      "lenx:786\n",
      "torch.FloatTensor\n",
      "lenx:786\n",
      "torch.FloatTensor\n",
      "lenx:786\n",
      "y_min       =  tensor([36.8440,  0.0859,  7.1720, -0.0000, -0.0000, -0.0000,  7.5760, -0.0000,\n",
      "        -0.0000, -0.0000])\n",
      "y_max       =  tensor([38.7622,  1.7100,  9.2058,  0.0000,  0.0000,  0.0000,  9.4372,  0.0000,\n",
      "         0.0000,  0.0000])\n",
      "center Ztp  =  tensor([37.8031,  0.8980,  8.1889, -0.0000, -0.0000, -0.0000,  8.5066, -0.0000,\n",
      "        -0.0000, -0.0000])\n",
      "y_true      =  tensor([[37.8073,  0.9050,  8.2148,  0.0000,  0.0000,  0.0000,  8.5005,  0.0000,\n",
      "          0.0000,  0.0000]])\n",
      "y_max-x_min =  tensor([1.9182, 1.6241, 2.0339, 0.0000, 0.0000, 0.0000, 1.8613, 0.0000, 0.0000,\n",
      "        0.0000])\n",
      "Trash symbol=  tensor([0.4463, 0.5058, 0.4977, 0.0000, 0.0000, 0.0000, 0.4380, 0.0000, 0.0000,\n",
      "        0.0000])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    " \n",
    "   result,x_min,x_max,x_true=model.abstract_forward(x.unsqueeze(1),add_symbol=False)\n",
    "\n",
    "\n",
    "print(f\"y_min       =  {x_min}\")\n",
    "print(f\"y_max       =  {x_max}\")\n",
    "print(f\"center Ztp  =  {result[0]}\")\n",
    "print(f\"y_true      =  {x_true[:]}\")\n",
    "print(f\"y_max-x_min =  {x_max-x_min}\")\n",
    "print(f\"Trash symbol=  {result[-1]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Voici le premier affichage de relevance calcul par AbstractTorch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x79457084ba90>"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "result = result[1:785,1]\n",
    "result = result.reshape(28,28)\n",
    "plt.imshow(torch.abs(result.cpu()).numpy(), cmap='viridis')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
